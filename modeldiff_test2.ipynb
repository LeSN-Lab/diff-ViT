{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jieungkim/.conda/envs/ptq4vit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from config import Config\n",
    "from models import *\n",
    "from generate_data import generate_data\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser(description='FQ-ViT')\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    choices=[\n",
    "                        'deit_tiny', 'deit_small', 'deit_base', 'vit_base',\n",
    "                        'vit_large', 'swin_tiny', 'swin_small', 'swin_base'\n",
    "                    ],\n",
    "                    default='deit_tiny',\n",
    "                    help='model')\n",
    "parser.add_argument('--data', metavar='DIR',\n",
    "                    default='/home/shared/DATA/imagenet',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--quant', default=True, action='store_true')\n",
    "parser.add_argument('--ptf', default=True)\n",
    "parser.add_argument('--lis', default=True)\n",
    "parser.add_argument('--quant-method',\n",
    "                    default='minmax',\n",
    "                    choices=['minmax', 'ema', 'omse', 'percentile'])\n",
    "parser.add_argument('--mixed', default=False, action='store_true')\n",
    "# TODO: 100 --> 32\n",
    "parser.add_argument('--calib-batchsize',\n",
    "                    default=50,\n",
    "                    type=int,\n",
    "                    help='batchsize of calibration set')\n",
    "parser.add_argument(\"--mode\", default=0,\n",
    "                        type=int, \n",
    "                        help=\"mode of calibration data, 0: PSAQ-ViT, 1: Gaussian noise, 2: Real data\")\n",
    "# TODO: 10 --> 1\n",
    "parser.add_argument('--calib-iter', default=10, type=int)\n",
    "# TODO: 100 --> 200\n",
    "parser.add_argument('--val-batchsize',\n",
    "                    default=50,\n",
    "                    type=int,\n",
    "                    help='batchsize of validation set')\n",
    "parser.add_argument('--num-workers',\n",
    "                    default=16,\n",
    "                    type=int,\n",
    "                    help='number of data loading workers (default: 16)')\n",
    "parser.add_argument('--device', default='cuda', type=str, help='device')\n",
    "parser.add_argument('--print-freq',\n",
    "                    default=100,\n",
    "                    type=int,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--seed', default=0, type=int, help='seed')\n",
    "\n",
    "\n",
    "def str2model(name):\n",
    "    d = {\n",
    "        'deit_tiny': deit_tiny_patch16_224,\n",
    "        'deit_small': deit_small_patch16_224,\n",
    "        'deit_base': deit_base_patch16_224,\n",
    "        'vit_base': vit_base_patch16_224,\n",
    "        'vit_large': vit_large_patch16_224,\n",
    "        'swin_tiny': swin_tiny_patch4_window7_224,\n",
    "        'swin_small': swin_small_patch4_window7_224,\n",
    "        'swin_base': swin_base_patch4_window7_224,\n",
    "    }\n",
    "    print('Model: %s' % d[name].__name__)\n",
    "    return d[name]\n",
    "\n",
    "\n",
    "def seed(seed=0):\n",
    "    import os\n",
    "    import random\n",
    "    import sys\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    sys.setrecursionlimit(100000)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def build_transform(input_size=224,\n",
    "                    interpolation='bicubic',\n",
    "                    mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225),\n",
    "                    crop_pct=0.875):\n",
    "\n",
    "    def _pil_interp(method):\n",
    "        if method == 'bicubic':\n",
    "            return Image.BICUBIC\n",
    "        elif method == 'lanczos':\n",
    "            return Image.LANCZOS\n",
    "        elif method == 'hamming':\n",
    "            return Image.HAMMING\n",
    "        else:\n",
    "            return Image.BILINEAR\n",
    "\n",
    "    resize_im = input_size > 32\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        size = int(math.floor(input_size / crop_pct))\n",
    "        ip = _pil_interp(interpolation)\n",
    "        t.append(\n",
    "            transforms.Resize(\n",
    "                size,\n",
    "                interpolation=ip),  # to maintain same ratio w.r.t. 224 images\n",
    "        )\n",
    "        t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args, val_loader, model, criterion, device, bit_config=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    val_start_time = end = time.time()\n",
    "    for i, (data, target) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if i == 0:\n",
    "            plot_flag = False\n",
    "        else:\n",
    "            plot_flag = False\n",
    "        with torch.no_grad():\n",
    "            output, FLOPs, distance = model(data, bit_config, plot_flag)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data.item(), data.size(0))\n",
    "        top1.update(prec1.data.item(), data.size(0))\n",
    "        top5.update(prec5.data.item(), data.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      i,\n",
    "                      len(val_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      loss=losses,\n",
    "                      top1=top1,\n",
    "                      top5=top5,\n",
    "                  ))\n",
    "    val_end_time = time.time()\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "          format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_make(model_name, ptf, lis, quant_method, device):\n",
    "    device = torch.device(device)\n",
    "    cfg = Config(ptf, lis, quant_method)\n",
    "    model = str2model(model_name)(pretrained=True, cfg=cfg)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "    \n",
    "def calibrate_model(mode = 0, args = None, model = None, train_loader = None, device = None):\n",
    "    if mode == 2:\n",
    "        print(\"Generating data...\")\n",
    "        calibrate_data = generate_data(args)\n",
    "        print(\"Calibrating with generated data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 1: Gaussian noise\n",
    "    elif args.mode == 1:\n",
    "        calibrate_data = torch.randn((args.calib_batchsize, 3, 224, 224)).to(device)\n",
    "        print(\"Calibrating with Gaussian noise...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 2: Real data (Standard)\n",
    "    elif args.mode == 0:\n",
    "        # Get calibration set.\n",
    "        image_list = []\n",
    "        # output_list = []\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            if i == args.calib_iter:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            # target = target.to(device)\n",
    "            image_list.append(data)\n",
    "            # output_list.append(target)\n",
    "\n",
    "        print(\"Calibrating with real data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            # TODO:\n",
    "            # for i, image in enumerate(image_list):\n",
    "            #     if i == len(image_list) - 1:\n",
    "            #         # This is used for OMSE method to\n",
    "            #         # calculate minimum quantization error\n",
    "            #         model.model_open_last_calibrate()\n",
    "            #     output, FLOPs, global_distance = model(image, plot=False)\n",
    "            # model.model_quant(flag='off')\n",
    "            model.model_open_last_calibrate()\n",
    "            output, FLOPs, global_distance = model(image_list[0], plot=False)\n",
    "\n",
    "    model.model_close_calibrate()\n",
    "    model.model_quant()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/shared/DATA/imagenet/val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m traindir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m valdir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvaldir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     36\u001b[0m     val_dataset,\n\u001b[1;32m     37\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mval_batchsize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# switch to evaluate mode\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# define loss function (criterion)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ptq4vit/lib/python3.9/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/.conda/envs/ptq4vit/lib/python3.9/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/.conda/envs/ptq4vit/lib/python3.9/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ptq4vit/lib/python3.9/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/shared/DATA/imagenet/val'"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "seed(args.seed)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "cfg = Config(args.ptf, args.lis, args.quant_method)\n",
    "# model = str2model(args.model)(pretrained=True, cfg=cfg)\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Note: Different models have different strategies of data preprocessing.\n",
    "model_type = args.model.split('_')[0]\n",
    "if model_type == 'deit':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.875\n",
    "elif model_type == 'vit':\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "    crop_pct = 0.9\n",
    "elif model_type == 'swin':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.9\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "val_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "# Data\n",
    "traindir = os.path.join(args.data, 'train')\n",
    "valdir = os.path.join(args.data, 'val')\n",
    "\n",
    "val_dataset = datasets.ImageFolder(valdir, val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.val_batchsize,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# switch to evaluate mode\n",
    "# model.eval()\n",
    "\n",
    "# define loss function (criterion)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss(yhat, y):\n",
    "\treturn -((yhat[:,0]-y[:,0])**2 + 0.1*((yhat[:,1:]-y[:,1:])**2).mean(1)).mean()\n",
    "\n",
    "class AttackPGD(nn.Module):\n",
    "    def __init__(self, basic_net, epsilon, step_size, num_steps, bit_config):\n",
    "        super(AttackPGD, self).__init__()\n",
    "        self.basic_net = basic_net\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.bit_config = bit_config\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        self.basic_net.zero_grad()\n",
    "        x = inputs.clone().detach()\n",
    "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
    "        for i in range(self.num_steps):\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            # with torch.enable_grad():\n",
    "            outputs, Flops, distance = self.basic_net(x, self.bit_config, False)\n",
    "            loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "            # loss = myloss(outputs, targets)\n",
    "            loss.backward()\n",
    "            # grad = torch.autograd.grad(loss, [x], create_graph=False)[0]\n",
    "            grad = x.grad.clone()\n",
    "            x = x + self.step_size*torch.sign(grad)\n",
    "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
    "            x = torch.clamp(x, inputs.min().item(), inputs.max().item())\n",
    "            # x = torch.clamp(x, 0, 1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.basic_net.eval()\n",
    "                adv_output, Flops, distance= self.basic_net(x, self.bit_config, False)\n",
    "            \n",
    "        return adv_output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_inputs(n, rand=False, input_shape = (3, 224, 224)):\n",
    "    if rand:\n",
    "        batch_input_size = (n, input_shape[0], input_shape[1], input_shape[2])\n",
    "        images = np.random.normal(size = batch_input_size).astype(np.float32)\n",
    "    else:\n",
    "        model_type = args.model.split('_')[0]\n",
    "        if model_type == 'deit':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.875\n",
    "        elif model_type == 'vit':\n",
    "            mean = (0.5, 0.5, 0.5)\n",
    "            std = (0.5, 0.5, 0.5)\n",
    "            crop_pct = 0.9\n",
    "        elif model_type == 'swin':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.9\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "        # Data\n",
    "        traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=n,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        images, labels = next(iter(train_loader))\n",
    "    return images.cuda(), labels.cuda()\n",
    "        \n",
    "        \n",
    "def get_dataset(n, input_shape = (3, 224, 224)):\n",
    "    \n",
    "    \n",
    "    model_type = args.model.split('_')[0]\n",
    "    if model_type == 'deit':\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        crop_pct = 0.875\n",
    "    elif model_type == 'vit':\n",
    "        mean = (0.5, 0.5, 0.5)\n",
    "        std = (0.5, 0.5, 0.5)\n",
    "        crop_pct = 0.9\n",
    "    elif model_type == 'swin':\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        crop_pct = 0.9\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "    # Data\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=n,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return train_loader\n",
    "\n",
    "def gen_adv_inputs(model, inputs, labels, bit_config):\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # bit_config = [8]*50\n",
    "    with torch.no_grad():\n",
    "        clean_output, FLOPs, distance = model(inputs, plot=False)\n",
    "    # output_shape = clean_output.shape\n",
    "    # batch_size = output_shape[0]\n",
    "    # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "    # output_mean = clean_output.mean(axis = 0)\n",
    "    # target_outputs = output_mean - clean_output\n",
    "    \n",
    "    # y = target_outputs * 1000 \n",
    "    \n",
    "    # adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "    adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "    torch.cuda.empty_cache()\n",
    "    return adv_inputs.detach()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/shared/DATA/imagenet/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      6\u001b[0m attack_net \u001b[38;5;241m=\u001b[39m AttackPGD(int8_model, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, num_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, bit_config\u001b[38;5;241m=\u001b[39mbit_config)\n\u001b[0;32m----> 8\u001b[0m seed_images, seed_labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_seed_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m adv_inputs \u001b[38;5;241m=\u001b[39m gen_adv_inputs(not_quantized_model, seed_images, seed_labels, bit_config\u001b[38;5;241m=\u001b[39mbit_config)\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mget_seed_inputs\u001b[0;34m(n, rand, input_shape)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m traindir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraindir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     29\u001b[0m     train_dataset,\n\u001b[1;32m     30\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n",
      "File \u001b[0;32m~/.conda/envs/ptq4vit/lib/python3.9/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/.conda/envs/ptq4vit/lib/python3.9/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/.conda/envs/ptq4vit/lib/python3.9/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ptq4vit/lib/python3.9/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/shared/DATA/imagenet/train'"
     ]
    }
   ],
   "source": [
    "int8_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "int4_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "not_quantized_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "\n",
    "bit_config = [8]*50\n",
    "attack_net = AttackPGD(int8_model, epsilon=0.1, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "seed_images, seed_labels = get_seed_inputs(50, rand=False)\n",
    "adv_inputs = gen_adv_inputs(not_quantized_model, seed_images, seed_labels, bit_config=bit_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating with real data...\n",
      "Calibrating with real data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (qact_input): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): QConv2d(\n",
       "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
       "      (quantizer): UniformQuantizer()\n",
       "    )\n",
       "    (qact_before_norm): Identity()\n",
       "    (norm): Identity()\n",
       "    (qact): QAct(\n",
       "      (quantizer): UniformQuantizer()\n",
       "    )\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (qact_embed): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (qact_pos): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (qact1): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): QLinear(\n",
       "          in_features=192, out_features=576, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact0): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact2): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (proj): QLinear(\n",
       "          in_features=192, out_features=192, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact3): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact_attn1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (log_int_softmax): QIntSoftmax(\n",
       "          (quantizer): Log2Quantizer()\n",
       "        )\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (qact2): QAct(\n",
       "        (quantizer): UniformQuantizer()\n",
       "      )\n",
       "      (norm2): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (qact0): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (fc1): QLinear(\n",
       "          in_features=192, out_features=768, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "        (qact1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (fc2): QLinear(\n",
       "          in_features=768, out_features=192, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact2): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (qact4): QAct(\n",
       "        (quantizer): UniformQuantizer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (qact2): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (pre_logits): Identity()\n",
       "  (head): QLinear(\n",
       "    in_features=192, out_features=1000, bias=True\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (act_out): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# int8_model = calibrate_model(args.mode, args, int8_model, train_loader, device)\n",
    "# int4_model = calibrate_model(args.mode, args, int4_model, train_loader, device)\n",
    "\n",
    "\n",
    "int8_model.eval()\n",
    "int4_model.eval()\n",
    "not_quantized_model.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_activations(act):\n",
    "    # 입력 텐서를 2D로 재구성합니다. 첫 번째 차원은 유지하고 나머지는 평탄화합니다.\n",
    "    act = act.view(act.size(0), -1)\n",
    "\n",
    "    # 각 샘플(행)에 대해 L2 norm을 계산합니다.\n",
    "    act_norm = torch.norm(act, p=2, dim=1, keepdim=True)\n",
    "\n",
    "    # 0으로 나누는 것을 방지하기 위해 작은 값을 더합니다.\n",
    "    act_norm = act_norm + 1e-8\n",
    "\n",
    "    # 각 샘플을 해당 norm으로 나누어 정규화합니다.\n",
    "    act = act / act_norm\n",
    "\n",
    "    return act\n",
    "def get_activations(images, model, normalize_act=False):\n",
    "    # 모델을 평가 모드로 설정합니다.\n",
    "\n",
    "    # 중간 활성화를 저장할 리스트를 초기화합니다.\n",
    "    activations = []\n",
    "\n",
    "    # 그래디언트 계산을 비활성화합니다.\n",
    "    with torch.no_grad():\n",
    "        # 각 레이어에 대한 후크(hook) 함수를 정의합니다.\n",
    "        def hook(module, input, output):\n",
    "            activations.append(output)\n",
    "\n",
    "        # 모든 레이어에 후크를 등록합니다.\n",
    "        hooks = []\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, (nn.Conv2d, nn.Linear)):  # 원하는 레이어 유형을 선택하세요\n",
    "                hooks.append(layer.register_forward_hook(hook))\n",
    "\n",
    "        # 모델을 통해 이미지를 전달합니다.\n",
    "        images = images.cuda()\n",
    "        _ = model(images)\n",
    "\n",
    "        # 등록된 후크를 제거합니다.\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "    # 필요한 경우 활성화를 정규화합니다.\n",
    "    if normalize_act:\n",
    "        activations = [normalize_activations(act) for act in activations]\n",
    "\n",
    "    return activations\n",
    "#torch model의 layers의 수를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 65\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcka_result.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     61\u001b[0m         np\u001b[38;5;241m.\u001b[39msavetxt(f, heatmap)\n\u001b[0;32m---> 65\u001b[0m \u001b[43mcompute_cka_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnot_quantized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_act\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcka_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcka_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mcompute_cka_internal\u001b[0;34m(model, use_batch, use_train_mode, normalize_act, cka_batch, cka_batch_iter, cka_iter)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_cka_internal\u001b[39m(model, use_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                          use_train_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                          normalize_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m                          cka_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      8\u001b[0m                          cka_batch_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      9\u001b[0m                          cka_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 12\u001b[0m     sample_cka_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m(cka_batch)\n\u001b[1;32m     14\u001b[0m     sample_cka_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(sample_cka_dataset))\n\u001b[1;32m     16\u001b[0m     sample_images, _ \u001b[38;5;241m=\u001b[39m sample_cka_dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#torch model의 layers의 수를 확인한다.\n",
    "from efficient_CKA import *\n",
    "\n",
    "def compute_cka_internal(model, use_batch = True,\n",
    "                         use_train_mode = False,\n",
    "                         normalize_act = False,\n",
    "                         cka_batch = 50,\n",
    "                         cka_batch_iter = 10,\n",
    "                         cka_iter = 10):\n",
    "    model.eval()\n",
    "\n",
    "    sample_cka_dataset = get_dataset(cka_batch)\n",
    "\n",
    "    sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "    sample_images, _ = sample_cka_dataset\n",
    "    # n_layers = len(list(not_quantized_model.children()))\n",
    "    # n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "    sample_activations = get_activations(sample_images, model)\n",
    "    n_layers = len(sample_activations)\n",
    "\n",
    "    cka = MinibatchCKA(n_layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 사용 예시:\n",
    "    # model = YourModel()  # PyTorch 모델을 정의하세요\n",
    "    # images = torch.randn(10, 3, 224, 224)  # 예시 입력 이미지\n",
    "    # activations = get_activations(images, model, normalize_act=True)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    if use_batch:\n",
    "        for index in range(cka_iter):\n",
    "            #cka_batch만큼, shuffle해서, 데이터셋을 가져온다.\n",
    "            cka_dataset = get_dataset(cka_batch)\n",
    "            current_iter = 0\n",
    "            for images, _ in cka_dataset:\n",
    "                model_get_activation = get_activations(images, model, normalize_act)\n",
    "\n",
    "                cka.update_state(model_get_activation)\n",
    "                print(\"현재 반복:\", index, \"/\", current_iter)\n",
    "                if current_iter > cka_batch_iter:\n",
    "                    break\n",
    "                current_iter += 1\n",
    "    else:\n",
    "        cka_dataset = get_dataset(cka_batch)\n",
    "        all_images = []\n",
    "        for images, _ in cka_dataset:\n",
    "            all_images.append(images)\n",
    "        cka.update_state(get_activations(all_images, model, normalize_act))\n",
    "    heatmap = cka.result().cpu().numpy()\n",
    "    with open('cka_result.txt', 'wb') as f:\n",
    "        np.savetxt(f, heatmap)\n",
    "        \n",
    "                \n",
    "            \n",
    "compute_cka_internal(not_quantized_model, use_batch = True, normalize_act = False, cka_batch = 50, cka_iter = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.1, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [8]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = not_quantized_attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int8_model(adv_inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(name, model_outputs):\n",
    "    def hook(module, input, output):\n",
    "        model_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "int8_outputs = {}\n",
    "int4_outputs = {}\n",
    "not_quantized_outputs = {}\n",
    "def add_hooks(model, model_outputs):\n",
    "    # Input quantization\n",
    "    model.qact_input.register_forward_hook(hook_fn(\"qact_input\", model_outputs))\n",
    "    \n",
    "    # Patch Embedding\n",
    "    model.patch_embed.register_forward_hook(hook_fn(\"patch_embed\", model_outputs))\n",
    "    model.patch_embed.qact.register_forward_hook(hook_fn(\"patch_embed_qact\", model_outputs))\n",
    "    \n",
    "    # Position Embedding\n",
    "    model.pos_drop.register_forward_hook(hook_fn(\"pos_drop\", model_outputs))\n",
    "    model.qact_embed.register_forward_hook(hook_fn(\"qact_embed\", model_outputs))\n",
    "    model.qact_pos.register_forward_hook(hook_fn(\"qact_pos\", model_outputs))\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        block.norm1.register_forward_hook(hook_fn(f\"block_{i}_norm1\", model_outputs))\n",
    "        block.attn.qkv.register_forward_hook(hook_fn(f\"block_{i}_attn_qkv\", model_outputs))\n",
    "        block.attn.proj.register_forward_hook(hook_fn(f\"block_{i}_attn_proj\", model_outputs))\n",
    "        block.attn.qact3.register_forward_hook(hook_fn(f\"block_{i}_attn_qact3\", model_outputs))\n",
    "        block.qact2.register_forward_hook(hook_fn(f\"block_{i}_qact2\", model_outputs))\n",
    "        block.norm2.register_forward_hook(hook_fn(f\"block_{i}_norm2\", model_outputs))\n",
    "        block.mlp.fc1.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc1\", model_outputs))\n",
    "        block.mlp.fc2.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc2\", model_outputs))\n",
    "        block.mlp.qact2.register_forward_hook(hook_fn(f\"block_{i}_mlp_qact2\", model_outputs))\n",
    "        block.qact4.register_forward_hook(hook_fn(f\"block_{i}_qact4\", model_outputs))\n",
    "    \n",
    "    # Final Norm Layer\n",
    "    model.norm.register_forward_hook(hook_fn(\"final_norm\", model_outputs))\n",
    "    model.qact2.register_forward_hook(hook_fn(\"final_qact2\", model_outputs))\n",
    "    \n",
    "    # Classifier Head\n",
    "    model.head.register_forward_hook(hook_fn(\"head\", model_outputs))\n",
    "    model.act_out.register_forward_hook(hook_fn(\"act_out\", model_outputs))\n",
    "    \n",
    "add_hooks(int8_model, int8_outputs)\n",
    "add_hooks(int4_model, int4_outputs)\n",
    "add_hooks(not_quantized_model, not_quantized_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv(model, normal_inputs, adv_inputs, outputs, bit_config = None):\n",
    "    if bit_config is not None:\n",
    "        model(normal_inputs, bit_config=bit_config, plot=False)    \n",
    "    else:\n",
    "        model(normal_inputs, plot=False)\n",
    "    normal_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    \n",
    "    # 적대적 입력에 대한 출력 저장\n",
    "    if bit_config is not None:\n",
    "        model(adv_inputs, bit_config=bit_config, plot=False)\n",
    "    else:\n",
    "        model(adv_inputs, plot=False)\n",
    "    adv_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    # print(normal_outputs.keys())\n",
    "    # print(adv_outputs.keys())\n",
    "\n",
    "    model_ddv_dict = {}\n",
    "    #dictionary int8_outputs을 모두 출력한다.\n",
    "    for key in normal_outputs.keys():\n",
    "    \n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        specific_layers_output_pairs = zip(normal_outputs[key], adv_outputs[key])\n",
    "    \n",
    "        ddv = []\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya.detach().cpu().numpy().flatten()\n",
    "            yb = yb.detach().cpu().numpy().flatten()\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb)\n",
    "            ddv.append(cos_similarity)\n",
    "        ddv = np.array(ddv)\n",
    "        norm = np.linalg.norm(ddv)\n",
    "        if norm != 0:\n",
    "            ddv = ddv/ norm\n",
    "        model_ddv_dict[key] = ddv\n",
    "        # print(key, \"레이어에서\", ddv.shape)\n",
    "    return model_ddv_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n",
    "not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        if not (key in target_ddv.keys()):\n",
    "            continue\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "            # norm = np.linalg.norm(cos_similarity)\n",
    "            # if norm != 0:\n",
    "                # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            # print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qact_input, \"레이어에서\", 0.00\n",
      "patch_embed_qact, \"레이어에서\", 0.00\n",
      "patch_embed, \"레이어에서\", 0.00\n",
      "qact_embed, \"레이어에서\", 0.00\n",
      "qact_pos, \"레이어에서\", 0.00\n",
      "pos_drop, \"레이어에서\", 0.00\n",
      "block_0_norm1, \"레이어에서\", 0.00\n",
      "block_0_attn_proj, \"레이어에서\", 0.00\n",
      "block_0_attn_qact3, \"레이어에서\", 0.00\n",
      "block_0_qact2, \"레이어에서\", 0.00\n",
      "block_0_norm2, \"레이어에서\", 0.00\n",
      "block_0_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_0_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_0_qact4, \"레이어에서\", 0.00\n",
      "block_1_norm1, \"레이어에서\", 0.00\n",
      "block_1_attn_proj, \"레이어에서\", 0.00\n",
      "block_1_attn_qact3, \"레이어에서\", 0.00\n",
      "block_1_qact2, \"레이어에서\", 0.00\n",
      "block_1_norm2, \"레이어에서\", 0.00\n",
      "block_1_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_1_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_1_qact4, \"레이어에서\", 0.00\n",
      "block_2_norm1, \"레이어에서\", 0.00\n",
      "block_2_attn_proj, \"레이어에서\", 0.00\n",
      "block_2_attn_qact3, \"레이어에서\", 0.00\n",
      "block_2_qact2, \"레이어에서\", 0.00\n",
      "block_2_norm2, \"레이어에서\", 0.00\n",
      "block_2_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_2_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_2_qact4, \"레이어에서\", 0.00\n",
      "block_3_norm1, \"레이어에서\", 0.00\n",
      "block_3_attn_proj, \"레이어에서\", 0.00\n",
      "block_3_attn_qact3, \"레이어에서\", 0.00\n",
      "block_3_qact2, \"레이어에서\", 0.00\n",
      "block_3_norm2, \"레이어에서\", 0.00\n",
      "block_3_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_3_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_3_qact4, \"레이어에서\", 0.00\n",
      "block_4_norm1, \"레이어에서\", 0.00\n",
      "block_4_attn_proj, \"레이어에서\", 0.00\n",
      "block_4_attn_qact3, \"레이어에서\", 0.00\n",
      "block_4_qact2, \"레이어에서\", 0.00\n",
      "block_4_norm2, \"레이어에서\", 0.00\n",
      "block_4_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_4_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_4_qact4, \"레이어에서\", 0.00\n",
      "block_5_norm1, \"레이어에서\", 0.00\n",
      "block_5_attn_proj, \"레이어에서\", 0.00\n",
      "block_5_attn_qact3, \"레이어에서\", 0.00\n",
      "block_5_qact2, \"레이어에서\", 0.00\n",
      "block_5_norm2, \"레이어에서\", 0.00\n",
      "block_5_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_5_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_5_qact4, \"레이어에서\", 0.00\n",
      "block_6_norm1, \"레이어에서\", 0.00\n",
      "block_6_attn_proj, \"레이어에서\", 0.00\n",
      "block_6_attn_qact3, \"레이어에서\", 0.00\n",
      "block_6_qact2, \"레이어에서\", 0.00\n",
      "block_6_norm2, \"레이어에서\", 0.00\n",
      "block_6_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_6_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_6_qact4, \"레이어에서\", 0.00\n",
      "block_7_norm1, \"레이어에서\", 0.00\n",
      "block_7_attn_proj, \"레이어에서\", 0.00\n",
      "block_7_attn_qact3, \"레이어에서\", 0.00\n",
      "block_7_qact2, \"레이어에서\", 0.00\n",
      "block_7_norm2, \"레이어에서\", 0.00\n",
      "block_7_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_7_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_7_qact4, \"레이어에서\", 0.00\n",
      "block_8_norm1, \"레이어에서\", 0.00\n",
      "block_8_attn_proj, \"레이어에서\", 0.00\n",
      "block_8_attn_qact3, \"레이어에서\", 0.00\n",
      "block_8_qact2, \"레이어에서\", 0.00\n",
      "block_8_norm2, \"레이어에서\", 0.00\n",
      "block_8_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_8_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_8_qact4, \"레이어에서\", 0.00\n",
      "block_9_norm1, \"레이어에서\", 0.00\n",
      "block_9_attn_proj, \"레이어에서\", 0.00\n",
      "block_9_attn_qact3, \"레이어에서\", 0.00\n",
      "block_9_qact2, \"레이어에서\", 0.00\n",
      "block_9_norm2, \"레이어에서\", 0.00\n",
      "block_9_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_9_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_9_qact4, \"레이어에서\", 0.00\n",
      "block_10_norm1, \"레이어에서\", 0.00\n",
      "block_10_attn_proj, \"레이어에서\", 0.00\n",
      "block_10_attn_qact3, \"레이어에서\", 0.00\n",
      "block_10_qact2, \"레이어에서\", 0.00\n",
      "block_10_norm2, \"레이어에서\", 0.00\n",
      "block_10_mlp_fc2, \"레이어에서\", 0.00\n",
      "block_10_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_10_qact4, \"레이어에서\", 0.00\n",
      "block_11_norm1, \"레이어에서\", 0.00\n",
      "block_11_attn_proj, \"레이어에서\", 0.00\n",
      "block_11_attn_qact3, \"레이어에서\", 0.00\n",
      "block_11_qact2, \"레이어에서\", 0.00\n",
      "block_11_norm2, \"레이어에서\", 0.00\n",
      "block_11_mlp_fc2, \"레이어에서\", -0.00\n",
      "block_11_mlp_qact2, \"레이어에서\", 0.00\n",
      "block_11_qact4, \"레이어에서\", 0.00\n",
      "final_norm, \"레이어에서\", 0.00\n",
      "final_qact2, \"레이어에서\", 0.00\n",
      "head, \"레이어에서\", 0.00\n",
      "act_out, \"레이어에서\", 0.00\n"
     ]
    }
   ],
   "source": [
    "#int8div와 int4div의 value를 각각 조회해 array의 차이를 출력한다.\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)  # 두 벡터의 내적\n",
    "    norm_vec1 = np.linalg.norm(vec1)  # 첫 번째 벡터의 크기 (norm)\n",
    "    norm_vec2 = np.linalg.norm(vec2)  # 두 번째 벡터의 크기 (norm)\n",
    "    \n",
    "    return dot_product / (norm_vec1 * norm_vec2) \n",
    "\n",
    "\n",
    "for key in int8_ddv.keys():\n",
    "    if  (key in not_quantized_ddv.keys()):\n",
    "        int8_similarity = cosine_similarity(int8_ddv[key], not_quantized_ddv[key])\n",
    "        int4_similarity = cosine_similarity(int4_ddv[key], not_quantized_ddv[key])\n",
    "        int8_similarity = (int8_similarity + 1) /2 * 100\n",
    "        int4_similarity = (int4_similarity + 1) /2 * 100\n",
    "        print(f'{key}, \"레이어에서\", {(int8_similarity - int4_similarity):.2f}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 절대적 변화율 계산\n",
    "        # print(np.abs(diff).mean() * 100)\n",
    "        #difference를 통해 mse를 구한다.\n",
    "        # mse = np.square(difference).mean()\n",
    "        # print(mse)\n",
    "        \n",
    "# print(\"int4_ddv - not_quantized_ddv\")\n",
    "#         print(np.array(int4_ddv[key] - not_quantized_ddv[key]).mean()/2 * 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newerFQ2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
