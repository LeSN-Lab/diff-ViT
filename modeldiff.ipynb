{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jieungkim/.conda/envs/newerFQ2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from config import Config\n",
    "from models import *\n",
    "from generate_data import generate_data\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser(description='FQ-ViT')\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    choices=[\n",
    "                        'deit_tiny', 'deit_small', 'deit_base', 'vit_base',\n",
    "                        'vit_large', 'swin_tiny', 'swin_small', 'swin_base'\n",
    "                    ],\n",
    "                    default='deit_tiny',\n",
    "                    help='model')\n",
    "parser.add_argument('--data', metavar='DIR',\n",
    "                    default='/home/jieungkim/quantctr/imagenet',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--quant', default=True, action='store_true')\n",
    "parser.add_argument('--ptf', default=True)\n",
    "parser.add_argument('--lis', default=True)\n",
    "parser.add_argument('--quant-method',\n",
    "                    default='minmax',\n",
    "                    choices=['minmax', 'ema', 'omse', 'percentile'])\n",
    "parser.add_argument('--mixed', default=False, action='store_true')\n",
    "# TODO: 100 --> 32\n",
    "parser.add_argument('--calib-batchsize',\n",
    "                    default=50,\n",
    "                    type=int,\n",
    "                    help='batchsize of calibration set')\n",
    "parser.add_argument(\"--mode\", default=0,\n",
    "                        type=int, \n",
    "                        help=\"mode of calibration data, 0: PSAQ-ViT, 1: Gaussian noise, 2: Real data\")\n",
    "# TODO: 10 --> 1\n",
    "parser.add_argument('--calib-iter', default=10, type=int)\n",
    "# TODO: 100 --> 200\n",
    "parser.add_argument('--val-batchsize',\n",
    "                    default=50,\n",
    "                    type=int,\n",
    "                    help='batchsize of validation set')\n",
    "parser.add_argument('--num-workers',\n",
    "                    default=16,\n",
    "                    type=int,\n",
    "                    help='number of data loading workers (default: 16)')\n",
    "parser.add_argument('--device', default='cuda', type=str, help='device')\n",
    "parser.add_argument('--print-freq',\n",
    "                    default=100,\n",
    "                    type=int,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--seed', default=0, type=int, help='seed')\n",
    "\n",
    "\n",
    "def str2model(name):\n",
    "    d = {\n",
    "        'deit_tiny': deit_tiny_patch16_224,\n",
    "        'deit_small': deit_small_patch16_224,\n",
    "        'deit_base': deit_base_patch16_224,\n",
    "        'vit_base': vit_base_patch16_224,\n",
    "        'vit_large': vit_large_patch16_224,\n",
    "        'swin_tiny': swin_tiny_patch4_window7_224,\n",
    "        'swin_small': swin_small_patch4_window7_224,\n",
    "        'swin_base': swin_base_patch4_window7_224,\n",
    "    }\n",
    "    print('Model: %s' % d[name].__name__)\n",
    "    return d[name]\n",
    "\n",
    "\n",
    "def seed(seed=0):\n",
    "    import os\n",
    "    import random\n",
    "    import sys\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    sys.setrecursionlimit(100000)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def build_transform(input_size=224,\n",
    "                    interpolation='bicubic',\n",
    "                    mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225),\n",
    "                    crop_pct=0.875):\n",
    "\n",
    "    def _pil_interp(method):\n",
    "        if method == 'bicubic':\n",
    "            return Image.BICUBIC\n",
    "        elif method == 'lanczos':\n",
    "            return Image.LANCZOS\n",
    "        elif method == 'hamming':\n",
    "            return Image.HAMMING\n",
    "        else:\n",
    "            return Image.BILINEAR\n",
    "\n",
    "    resize_im = input_size > 32\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        size = int(math.floor(input_size / crop_pct))\n",
    "        ip = _pil_interp(interpolation)\n",
    "        t.append(\n",
    "            transforms.Resize(\n",
    "                size,\n",
    "                interpolation=ip),  # to maintain same ratio w.r.t. 224 images\n",
    "        )\n",
    "        t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args, val_loader, model, criterion, device, bit_config=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    val_start_time = end = time.time()\n",
    "    for i, (data, target) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if i == 0:\n",
    "            plot_flag = False\n",
    "        else:\n",
    "            plot_flag = False\n",
    "        with torch.no_grad():\n",
    "            output, FLOPs, distance = model(data, bit_config, plot_flag)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data.item(), data.size(0))\n",
    "        top1.update(prec1.data.item(), data.size(0))\n",
    "        top5.update(prec5.data.item(), data.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      i,\n",
    "                      len(val_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      loss=losses,\n",
    "                      top1=top1,\n",
    "                      top5=top5,\n",
    "                  ))\n",
    "    val_end_time = time.time()\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "          format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_make(model_name, ptf, lis, quant_method, device):\n",
    "    device = torch.device(device)\n",
    "    cfg = Config(ptf, lis, quant_method)\n",
    "    model = str2model(model_name)(pretrained=True, cfg=cfg)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "    \n",
    "def calibrate_model(mode = 0, args = None, model = None, train_loader = None, device = None):\n",
    "    if mode == 2:\n",
    "        print(\"Generating data...\")\n",
    "        calibrate_data = generate_data(args)\n",
    "        print(\"Calibrating with generated data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 1: Gaussian noise\n",
    "    elif args.mode == 1:\n",
    "        calibrate_data = torch.randn((args.calib_batchsize, 3, 224, 224)).to(device)\n",
    "        print(\"Calibrating with Gaussian noise...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 2: Real data (Standard)\n",
    "    elif args.mode == 0:\n",
    "        # Get calibration set.\n",
    "        image_list = []\n",
    "        # output_list = []\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            if i == args.calib_iter:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            # target = target.to(device)\n",
    "            image_list.append(data)\n",
    "            # output_list.append(target)\n",
    "\n",
    "        print(\"Calibrating with real data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            # TODO:\n",
    "            # for i, image in enumerate(image_list):\n",
    "            #     if i == len(image_list) - 1:\n",
    "            #         # This is used for OMSE method to\n",
    "            #         # calculate minimum quantization error\n",
    "            #         model.model_open_last_calibrate()\n",
    "            #     output, FLOPs, global_distance = model(image, plot=False)\n",
    "            # model.model_quant(flag='off')\n",
    "            model.model_open_last_calibrate()\n",
    "            output, FLOPs, global_distance = model(image_list[0], plot=False)\n",
    "\n",
    "    model.model_close_calibrate()\n",
    "    model.model_quant()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "seed(args.seed)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "cfg = Config(args.ptf, args.lis, args.quant_method)\n",
    "# model = str2model(args.model)(pretrained=True, cfg=cfg)\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Note: Different models have different strategies of data preprocessing.\n",
    "model_type = args.model.split('_')[0]\n",
    "if model_type == 'deit':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.875\n",
    "elif model_type == 'vit':\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "    crop_pct = 0.9\n",
    "elif model_type == 'swin':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.9\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "val_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "# Data\n",
    "traindir = os.path.join(args.data, 'train')\n",
    "valdir = os.path.join(args.data, 'val')\n",
    "\n",
    "val_dataset = datasets.ImageFolder(valdir, val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.val_batchsize,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# switch to evaluate mode\n",
    "# model.eval()\n",
    "\n",
    "# define loss function (criterion)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackPGD(nn.Module):\n",
    "    def __init__(self, basic_net, epsilon, step_size, num_steps, bit_config):\n",
    "        super(AttackPGD, self).__init__()\n",
    "        self.basic_net = basic_net\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.bit_config = bit_config\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        self.basic_net.zero_grad()\n",
    "        x = inputs.clone().detach()\n",
    "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
    "        for i in range(self.num_steps):\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            # with torch.enable_grad():\n",
    "            outputs, Flops, distance = self.basic_net(x, self.bit_config, False)\n",
    "            loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "                # loss = myloss(outputs, targets)\n",
    "            loss.backward()\n",
    "            # grad = torch.autograd.grad(loss, [x], create_graph=False)[0]\n",
    "            grad = x.grad.clone()\n",
    "            x = x + self.step_size*torch.sign(grad)\n",
    "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
    "            # x = torch.clamp(x, inputs.min().item(), inputs.max().item())\n",
    "            x = torch.clamp(x, 0, 1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.basic_net.eval()\n",
    "                adv_output, Flops, distance= self.basic_net(x, self.bit_config, False)\n",
    "            \n",
    "        return adv_output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_inputs(n, rand=False, input_shape = (3, 224, 224)):\n",
    "    if rand:\n",
    "        batch_input_size = (n, input_shape[0], input_shape[1], input_shape[2])\n",
    "        images = np.random.normal(size = batch_input_size).astype(np.float32)\n",
    "    else:\n",
    "        model_type = args.model.split('_')[0]\n",
    "        if model_type == 'deit':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.875\n",
    "        elif model_type == 'vit':\n",
    "            mean = (0.5, 0.5, 0.5)\n",
    "            std = (0.5, 0.5, 0.5)\n",
    "            crop_pct = 0.9\n",
    "        elif model_type == 'swin':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.9\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "        # Data\n",
    "        traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=n,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        images, labels = next(iter(train_loader))\n",
    "    return images.cuda(), labels.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def gen_adv_inputs(model, inputs, labels):\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    bit_config = [8]*50\n",
    "    with torch.no_grad():\n",
    "        clean_output, FLOPs, distance = model(inputs, bit_config, plot=False)\n",
    "    output_shape = clean_output.shape\n",
    "    batch_size = output_shape[0]\n",
    "    num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "    output_mean = clean_output.mean(axis = 0)\n",
    "    target_outputs = output_mean - clean_output\n",
    "    \n",
    "    y = target_outputs * 1000 \n",
    "    \n",
    "    adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "    torch.cuda.empty_cache()\n",
    "    return adv_inputs.detach()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n"
     ]
    }
   ],
   "source": [
    "int8_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "int4_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "\n",
    "bit_config = [8]*50\n",
    "attack_net = AttackPGD(int8_model, epsilon=0.1, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "seed_images, seed_labels = get_seed_inputs(20, rand=False)\n",
    "bit_config = [4]*50\n",
    "adv_inputs = gen_adv_inputs(int8_model, seed_images, seed_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating with real data...\n",
      "Calibrating with real data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (qact_input): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): QConv2d(\n",
       "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
       "      (quantizer): UniformQuantizer()\n",
       "    )\n",
       "    (qact_before_norm): Identity()\n",
       "    (norm): Identity()\n",
       "    (qact): QAct(\n",
       "      (quantizer): UniformQuantizer()\n",
       "    )\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (qact_embed): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (qact_pos): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (qact1): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): QLinear(\n",
       "          in_features=192, out_features=576, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact0): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact2): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (proj): QLinear(\n",
       "          in_features=192, out_features=192, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact3): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact_attn1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (log_int_softmax): QIntSoftmax(\n",
       "          (quantizer): Log2Quantizer()\n",
       "        )\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (qact2): QAct(\n",
       "        (quantizer): UniformQuantizer()\n",
       "      )\n",
       "      (norm2): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (qact0): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (fc1): QLinear(\n",
       "          in_features=192, out_features=768, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "        (qact1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (fc2): QLinear(\n",
       "          in_features=768, out_features=192, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact2): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (qact4): QAct(\n",
       "        (quantizer): UniformQuantizer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (qact2): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (pre_logits): Identity()\n",
       "  (head): QLinear(\n",
       "    in_features=192, out_features=1000, bias=True\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (act_out): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "int8_model = calibrate_model(args.mode, args, int8_model, train_loader, device)\n",
    "int4_model = calibrate_model(args.mode, args, int4_model, train_loader, device)\n",
    "\n",
    "\n",
    "int8_model.eval()\n",
    "int4_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in int4_model.named_modules():\n",
    "#     print(f\"Layer name: {name}\")\n",
    "#     print(f\"Layer type: {type(module)}\")\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(name, model_outputs):\n",
    "    def hook(module, input, output):\n",
    "        model_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "int8_outputs = {}\n",
    "int4_outputs = {}\n",
    "\n",
    "def add_hooks(model, model_outputs):\n",
    "    # Input quantization\n",
    "    model.qact_input.register_forward_hook(hook_fn(\"qact_input\", model_outputs))\n",
    "    \n",
    "    # Patch Embedding\n",
    "    model.patch_embed.register_forward_hook(hook_fn(\"patch_embed\", model_outputs))\n",
    "    model.patch_embed.qact.register_forward_hook(hook_fn(\"patch_embed_qact\", model_outputs))\n",
    "    \n",
    "    # Position Embedding\n",
    "    model.pos_drop.register_forward_hook(hook_fn(\"pos_drop\", model_outputs))\n",
    "    model.qact_embed.register_forward_hook(hook_fn(\"qact_embed\", model_outputs))\n",
    "    model.qact_pos.register_forward_hook(hook_fn(\"qact_pos\", model_outputs))\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        block.norm1.register_forward_hook(hook_fn(f\"block_{i}_norm1\", model_outputs))\n",
    "        block.attn.qkv.register_forward_hook(hook_fn(f\"block_{i}_attn_qkv\", model_outputs))\n",
    "        block.attn.proj.register_forward_hook(hook_fn(f\"block_{i}_attn_proj\", model_outputs))\n",
    "        block.attn.qact3.register_forward_hook(hook_fn(f\"block_{i}_attn_qact3\", model_outputs))\n",
    "        block.qact2.register_forward_hook(hook_fn(f\"block_{i}_qact2\", model_outputs))\n",
    "        block.norm2.register_forward_hook(hook_fn(f\"block_{i}_norm2\", model_outputs))\n",
    "        block.mlp.fc1.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc1\", model_outputs))\n",
    "        block.mlp.fc2.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc2\", model_outputs))\n",
    "        block.mlp.qact2.register_forward_hook(hook_fn(f\"block_{i}_mlp_qact2\", model_outputs))\n",
    "        block.qact4.register_forward_hook(hook_fn(f\"block_{i}_qact4\", model_outputs))\n",
    "    \n",
    "    # Final Norm Layer\n",
    "    model.norm.register_forward_hook(hook_fn(\"final_norm\", model_outputs))\n",
    "    model.qact2.register_forward_hook(hook_fn(\"final_qact2\", model_outputs))\n",
    "    \n",
    "    # Classifier Head\n",
    "    model.head.register_forward_hook(hook_fn(\"head\", model_outputs))\n",
    "    model.act_out.register_forward_hook(hook_fn(\"act_out\", model_outputs))\n",
    "    \n",
    "add_hooks(int8_model, int8_outputs)\n",
    "add_hooks(int4_model, int4_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv(model, normal_inputs, adv_inputs, outputs, bit_config):\n",
    "    \n",
    "    model(normal_inputs, bit_config=bit_config, plot=False)    \n",
    "    normal_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    \n",
    "    # 적대적 입력에 대한 출력 저장\n",
    "    model(adv_inputs, bit_config=bit_config, plot=False)\n",
    "    adv_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    # print(normal_outputs.keys())\n",
    "    # print(adv_outputs.keys())\n",
    "\n",
    "    model_ddv_dict = {}\n",
    "    #dictionary int8_outputs을 모두 출력한다.\n",
    "    for key in normal_outputs.keys():\n",
    "    \n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        specific_layers_output_pairs = zip(normal_outputs[key], adv_outputs[key])\n",
    "    \n",
    "        ddv = []\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya.detach().cpu().numpy().flatten()\n",
    "            yb = yb.detach().cpu().numpy().flatten()\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb)\n",
    "            ddv.append(cos_similarity)\n",
    "        ddv = np.array(ddv)\n",
    "        norm = np.linalg.norm(ddv)\n",
    "        if norm != 0:\n",
    "            ddv = ddv/ norm\n",
    "        model_ddv_dict[key] = ddv\n",
    "        # print(key, \"레이어에서\", ddv.shape)\n",
    "    return model_ddv_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.75 GiB of which 13.88 MiB is free. Including non-PyTorch memory, this process has 7.73 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 548.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m int8_bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      2\u001b[0m int4_bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 3\u001b[0m int8_ddv \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ddv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint8_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8_bit_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m int4_ddv \u001b[38;5;241m=\u001b[39m compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_and_print_similarities\u001b[39m(source_ddv, target_ddv):\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mcompute_ddv\u001b[0;34m(model, normal_inputs, adv_inputs, outputs, bit_config)\u001b[0m\n\u001b[1;32m      4\u001b[0m normal_outputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 적대적 입력에 대한 출력 저장\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43madv_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbit_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m adv_outputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(normal_outputs.keys())\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(adv_outputs.keys())\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/newerFQ2/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/newerFQ2/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/quantctr/diff-ViT/models/vit_fquant.py:767\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x, bit_config, plot, hessian_statistic)\u001b[0m\n\u001b[1;32m    765\u001b[0m FLOPs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    766\u001b[0m global_distance \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 767\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFLOPs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessian_statistic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m B, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bit_config:\n",
      "File \u001b[0;32m~/quantctr/diff-ViT/models/vit_fquant.py:742\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x, FLOPs, global_distance, bit_config, global_plot, hessian_statistic)\u001b[0m\n\u001b[1;32m    740\u001b[0m         plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;66;03m# print(i)\u001b[39;00m\n\u001b[0;32m--> 742\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFLOPs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_bit_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessian_statistic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# if bit_config:\u001b[39;00m\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;66;03m#     scale = blk.attn.get_requant_scale()\u001b[39;00m\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;66;03m#     print(torch.floor(torch.div(torch.log(scale),torch.log(torch.Tensor([2]).cuda()))))\u001b[39;00m\n\u001b[1;32m    748\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mqact4\u001b[38;5;241m.\u001b[39mquantizer,\n\u001b[1;32m    749\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqact2\u001b[38;5;241m.\u001b[39mquantizer)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/newerFQ2/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/newerFQ2/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/quantctr/diff-ViT/models/vit_fquant.py:418\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, last_quantizer, FLOPs, global_distance, local_bit_config, plot, quant, hessian_statistic)\u001b[0m\n\u001b[1;32m    412\u001b[0m     atten_bit_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# x = self.qact2(x + self.drop_path(\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m#     self.attn(\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m#         self.qact1(self.norm1(x, last_quantizer,\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m#                               self.qact1.quantizer)), FLOPs, global_distance, atten_bit_config, plot, quant)))\u001b[39;00m\n\u001b[1;32m    417\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqact2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqact0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_scale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFLOPs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matten_bit_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessian_statistic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessian_statistic\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    421\u001b[0m activation\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# x_old = copy.deepcopy(x)\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# x = self.attn(\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m#         self.qact1(self.norm1(x, last_quantizer,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m#             self.norm2(x, self.qact2.quantizer,\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m#                        self.qact3.quantizer)))))\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/newerFQ2/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/newerFQ2/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/quantctr/diff-ViT/models/vit_fquant.py:310\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, FLOPs, global_distance, atten_bit_config, plot, quant, smoothquant, hessian_statistic)\u001b[0m\n\u001b[1;32m    308\u001b[0m activation\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_int_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqact_attn1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# attn = attn.softmax(dim=-1)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n",
      "File \u001b[0;32m~/.conda/envs/newerFQ2/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/newerFQ2/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/quantctr/diff-ViT/models/ptq/layers.py:369\u001b[0m, in \u001b[0;36mQIntSoftmax.forward\u001b[0;34m(self, x, scale)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, scale):\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_i_softmax \u001b[38;5;129;01mand\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m         exp_int, exp_int_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m         softmax_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(exp_int_sum \u001b[38;5;241m/\u001b[39m exp_int)\n\u001b[1;32m    371\u001b[0m         rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_round(softmax_out)\n",
      "File \u001b[0;32m~/quantctr/diff-ViT/models/ptq/layers.py:363\u001b[0m, in \u001b[0;36mQIntSoftmax.int_softmax\u001b[0;34m(x, scaling_factor)\u001b[0m\n\u001b[1;32m    361\u001b[0m x_int_max, _ \u001b[38;5;241m=\u001b[39m x_int\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    362\u001b[0m x_int \u001b[38;5;241m=\u001b[39m x_int \u001b[38;5;241m-\u001b[39m x_int_max\n\u001b[0;32m--> 363\u001b[0m exp_int, exp_scaling_factor \u001b[38;5;241m=\u001b[39m \u001b[43mint_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m exp_int_sum \u001b[38;5;241m=\u001b[39m exp_int\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp_int, exp_int_sum\n",
      "File \u001b[0;32m~/quantctr/diff-ViT/models/ptq/layers.py:356\u001b[0m, in \u001b[0;36mQIntSoftmax.int_softmax.<locals>.int_exp\u001b[0;34m(x_int, scaling_factor)\u001b[0m\n\u001b[1;32m    354\u001b[0m r \u001b[38;5;241m=\u001b[39m x_int \u001b[38;5;241m-\u001b[39m x0_int \u001b[38;5;241m*\u001b[39m q\n\u001b[1;32m    355\u001b[0m exp_int, exp_scaling_factor \u001b[38;5;241m=\u001b[39m int_polynomial(r, scaling_factor)\n\u001b[0;32m--> 356\u001b[0m exp_int \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(torch\u001b[38;5;241m.\u001b[39mfloor(\u001b[43mexp_int\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    357\u001b[0m scaling_factor \u001b[38;5;241m=\u001b[39m exp_scaling_factor \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mn\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp_int, scaling_factor\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.75 GiB of which 13.88 MiB is free. Including non-PyTorch memory, this process has 7.73 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 548.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n",
    "\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb)\n",
    "            \n",
    "        \n",
    "            norm = np.linalg.norm(cos_similarity)\n",
    "            if norm != 0:\n",
    "                cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "\n",
    "calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newerFQ2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
