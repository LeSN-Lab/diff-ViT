{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jieungkim/.conda/envs/ptq4vit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from config import Config\n",
    "from models import *\n",
    "from generate_data import generate_data\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "parser = argparse.ArgumentParser(description='FQ-ViT')\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    choices=[\n",
    "                        'deit_tiny', 'deit_small', 'deit_base', 'vit_base',\n",
    "                        'vit_large', 'swin_tiny', 'swin_small', 'swin_base'\n",
    "                    ],\n",
    "                    default='deit_tiny',\n",
    "                    help='model')\n",
    "parser.add_argument('--data', metavar='DIR',\n",
    "                    default='/home/jieungkim/quantctr/imagenet',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--quant', default=True, action='store_true')\n",
    "parser.add_argument('--ptf', default=False)\n",
    "parser.add_argument('--lis', default=False)\n",
    "parser.add_argument('--quant-method',\n",
    "                    default='minmax',\n",
    "                    choices=['minmax', 'ema', 'omse', 'percentile'])\n",
    "parser.add_argument('--mixed', default=False, action='store_true')\n",
    "# TODO: 100 --> 32\n",
    "parser.add_argument('--calib-batchsize',\n",
    "                    default=5,\n",
    "                    type=int,\n",
    "                    help='batchsize of calibration set')\n",
    "parser.add_argument(\"--mode\", default=0,\n",
    "                        type=int, \n",
    "                        help=\"mode of calibration data, 0: PSAQ-ViT, 1: Gaussian noise, 2: Real data\")\n",
    "# TODO: 10 --> 1\n",
    "parser.add_argument('--calib-iter', default=10, type=int)\n",
    "# TODO: 100 --> 200\n",
    "parser.add_argument('--val-batchsize',\n",
    "                    default=5,\n",
    "                    type=int,\n",
    "                    help='batchsize of validation set')\n",
    "parser.add_argument('--num-workers',\n",
    "                    default=16,\n",
    "                    type=int,\n",
    "                    help='number of data loading workers (default: 16)')\n",
    "parser.add_argument('--device', default='cuda', type=str, help='device')\n",
    "parser.add_argument('--print-freq',\n",
    "                    default=100,\n",
    "                    type=int,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--seed', default=0, type=int, help='seed')\n",
    "\n",
    "\n",
    "def str2model(name):\n",
    "    d = {\n",
    "        'deit_tiny': deit_tiny_patch16_224,\n",
    "        'deit_small': deit_small_patch16_224,\n",
    "        'deit_base': deit_base_patch16_224,\n",
    "        'vit_base': vit_base_patch16_224,\n",
    "        'vit_large': vit_large_patch16_224,\n",
    "        'swin_tiny': swin_tiny_patch4_window7_224,\n",
    "        'swin_small': swin_small_patch4_window7_224,\n",
    "        'swin_base': swin_base_patch4_window7_224,\n",
    "    }\n",
    "    print('Model: %s' % d[name].__name__)\n",
    "    return d[name]\n",
    "\n",
    "\n",
    "def seed(seed=0):\n",
    "    import os\n",
    "    import random\n",
    "    import sys\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    sys.setrecursionlimit(100000)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def build_transform(input_size=224,\n",
    "                    interpolation='bicubic',\n",
    "                    mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225),\n",
    "                    crop_pct=0.875):\n",
    "\n",
    "    def _pil_interp(method):\n",
    "        if method == 'bicubic':\n",
    "            return Image.BICUBIC\n",
    "        elif method == 'lanczos':\n",
    "            return Image.LANCZOS\n",
    "        elif method == 'hamming':\n",
    "            return Image.HAMMING\n",
    "        else:\n",
    "            return Image.BILINEAR\n",
    "\n",
    "    resize_im = input_size > 32\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        size = int(math.floor(input_size / crop_pct))\n",
    "        ip = _pil_interp(interpolation)\n",
    "        t.append(\n",
    "            transforms.Resize(\n",
    "                size,\n",
    "                interpolation=ip),  # to maintain same ratio w.r.t. 224 images\n",
    "        )\n",
    "        t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args, val_loader, model, criterion, device, bit_config=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    val_start_time = end = time.time()\n",
    "    for i, (data, target) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if i == 0:\n",
    "            plot_flag = False\n",
    "        else:\n",
    "            plot_flag = False\n",
    "        with torch.no_grad():\n",
    "            output, FLOPs, distance = model(data, bit_config, plot_flag)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data.item(), data.size(0))\n",
    "        top1.update(prec1.data.item(), data.size(0))\n",
    "        top5.update(prec5.data.item(), data.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      i,\n",
    "                      len(val_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      loss=losses,\n",
    "                      top1=top1,\n",
    "                      top5=top5,\n",
    "                  ))\n",
    "    val_end_time = time.time()\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "          format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_make(model_name, ptf, lis, quant_method, device):\n",
    "    device = torch.device(device)\n",
    "    cfg = Config(ptf, lis, quant_method)\n",
    "    model = str2model(model_name)(pretrained=True, cfg=cfg)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "    \n",
    "def calibrate_model(mode = 0, args = None, model = None, train_loader = None, device = None):\n",
    "    if mode == 2:\n",
    "        print(\"Generating data...\")\n",
    "        calibrate_data = generate_data(args)\n",
    "        print(\"Calibrating with generated data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 1: Gaussian noise\n",
    "    elif args.mode == 1:\n",
    "        calibrate_data = torch.randn((args.calib_batchsize, 3, 224, 224)).to(device)\n",
    "        print(\"Calibrating with Gaussian noise...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 2: Real data (Standard)\n",
    "    elif args.mode == 0:\n",
    "        # Get calibration set.\n",
    "        image_list = []\n",
    "        # output_list = []\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            if i == args.calib_iter:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            # target = target.to(device)\n",
    "            image_list.append(data)\n",
    "            # output_list.append(target)\n",
    "\n",
    "        print(\"Calibrating with real data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            # TODO:\n",
    "            # for i, image in enumerate(image_list):\n",
    "            #     if i == len(image_list) - 1:\n",
    "            #         # This is used for OMSE method to\n",
    "            #         # calculate minimum quantization error\n",
    "            #         model.model_open_last_calibrate()\n",
    "            #     output, FLOPs, global_distance = model(image, plot=False)\n",
    "            # model.model_quant(flag='off')\n",
    "            model.model_open_last_calibrate()\n",
    "            output, FLOPs, global_distance = model(image_list[0], plot=False)\n",
    "\n",
    "    model.model_close_calibrate()\n",
    "    model.model_quant()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "seed(args.seed)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "cfg = Config(args.ptf, args.lis, args.quant_method)\n",
    "# model = str2model(args.model)(pretrained=True, cfg=cfg)\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Note: Different models have different strategies of data preprocessing.\n",
    "model_type = args.model.split('_')[0]\n",
    "if model_type == 'deit':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.875\n",
    "elif model_type == 'vit':\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "    crop_pct = 0.9\n",
    "elif model_type == 'swin':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.9\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "val_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "# Data\n",
    "traindir = os.path.join(args.data, 'train')\n",
    "valdir = os.path.join(args.data, 'val')\n",
    "\n",
    "val_dataset = datasets.ImageFolder(valdir, val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.val_batchsize,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# switch to evaluate mode\n",
    "# model.eval()\n",
    "\n",
    "# define loss function (criterion)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss(yhat, y):\n",
    "\treturn -((yhat[:,0]-y[:,0])**2 + 0.1*((yhat[:,1:]-y[:,1:])**2).mean(1)).mean()\n",
    "\n",
    "class AttackPGD(nn.Module):\n",
    "    def __init__(self, basic_net, epsilon, step_size, num_steps, bit_config):\n",
    "        super(AttackPGD, self).__init__()\n",
    "        self.basic_net = basic_net\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.bit_config = bit_config\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        self.basic_net.zero_grad()\n",
    "        x = inputs.clone().detach()\n",
    "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
    "        for i in range(self.num_steps):\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            # with torch.enable_grad():\n",
    "            outputs, Flops, distance = self.basic_net(x, self.bit_config, False)\n",
    "            loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "            # loss = myloss(outputs, targets)\n",
    "            loss.backward()\n",
    "            # grad = torch.autograd.grad(loss, [x], create_graph=False)[0]\n",
    "            grad = x.grad.clone()\n",
    "            x = x + self.step_size*torch.sign(grad)\n",
    "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
    "            x = torch.clamp(x, inputs.min().item(), inputs.max().item())\n",
    "            # x = torch.clamp(x, 0, 1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.basic_net.eval()\n",
    "                adv_output, Flops, distance= self.basic_net(x, self.bit_config, False)\n",
    "            \n",
    "        return adv_output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_inputs(n, rand=False, input_shape = (3, 224, 224)):\n",
    "    if rand:\n",
    "        batch_input_size = (n, input_shape[0], input_shape[1], input_shape[2])\n",
    "        images = np.random.normal(size = batch_input_size).astype(np.float32)\n",
    "    else:\n",
    "        model_type = args.model.split('_')[0]\n",
    "        if model_type == 'deit':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.875\n",
    "        elif model_type == 'vit':\n",
    "            mean = (0.5, 0.5, 0.5)\n",
    "            std = (0.5, 0.5, 0.5)\n",
    "            crop_pct = 0.9\n",
    "        elif model_type == 'swin':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.9\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "        # Data\n",
    "        traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=n,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        images, labels = next(iter(train_loader))\n",
    "    return images.cuda(), labels.cuda()\n",
    "        \n",
    "def get_dataset(n, input_shape = (3, 224, 224)):\n",
    "    \n",
    "    \n",
    "    model_type = args.model.split('_')[0]\n",
    "    if model_type == 'deit':\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        crop_pct = 0.875\n",
    "    elif model_type == 'vit':\n",
    "        mean = (0.5, 0.5, 0.5)\n",
    "        std = (0.5, 0.5, 0.5)\n",
    "        crop_pct = 0.9\n",
    "    elif model_type == 'swin':\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        crop_pct = 0.9\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "    # Data\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=n,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return train_loader\n",
    "        \n",
    "\n",
    "def gen_adv_inputs(model, inputs, labels, bit_config, attack_net):\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # bit_config = [8]*50\n",
    "    with torch.no_grad():\n",
    "        clean_output, FLOPs, distance = model(inputs, bit_config, plot=False)\n",
    "    # output_shape = clean_output.shape\n",
    "    # batch_size = output_shape[0]\n",
    "    # num_classes = output_shape[1]\n",
    "    \n",
    "    \"\"\"\n",
    "    다양성 최대화:\n",
    "    ModelDiff 논문의 4.2절에서는 생성된 입력의 다양성(diversity)을 강조합니다.\n",
    "    target_outputs = output_mean - clean_output를 사용함으로써, 각 입력이 평균 출력과 다르게 되도록 유도하고 있습니다.\n",
    "    이는 생성된 입력들이 서로 다른 특성을 가지도록 하는 데 도움이 됩니다.\n",
    "    \"\"\"\n",
    "    output_mean = clean_output.mean(axis = 0)\n",
    "    target_outputs = output_mean - clean_output\n",
    "    \"\"\"\n",
    "    결정 경계 탐색:\n",
    "    y = target_outputs * 1000에서 큰 스케일 팩터(1000)를 사용하는 것은,\n",
    "    모델의 결정 경계를 더 잘 탐색하기 위한 것으로 보입니다.\n",
    "    이는 논문의 Figure 3에서 설명하는 \"decision boundary\" 개념과 연관됩니다.\n",
    "    \"\"\"\n",
    "    y = target_outputs * 1000 \n",
    "    \n",
    "    adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "    # adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "    torch.cuda.empty_cache()\n",
    "    return adv_inputs.detach()\n",
    "\n",
    "def metrics_output_diversity(model, bit_config, inputs, use_torch=False):\n",
    "    # 논문의 4.2절에서 설명한 출력 다양성 메트릭 계산\n",
    "    outputs = model(inputs, bit_config, False)[0].detach().to('cpu').numpy()\n",
    "#         output_dists = []\n",
    "#         for i in range(0, len(outputs) - 1):\n",
    "#             for j in range(i + 1, len(outputs)):\n",
    "#                 output_dist = spatial.distance.euclidean(outputs[i], outputs[j])\n",
    "#                 output_dists.append(output_dist)\n",
    "#         diversity = sum(output_dists) / len(output_dists)\n",
    "    # cdist 함수는 두 집합 모든 쌍 사이의 거리를 유클리드 거리를 이용해서 계산함.\n",
    "    #outputs_dists는 모든 출력 쌍 사이의 거리를 담은 행렬.\n",
    "    output_dists = spatial.distance.cdist(list(outputs), list(outputs), metric='euclidean')\n",
    "    #계산된 모든 거리의 평균을 구함.\n",
    "    diversity = np.mean(output_dists)\n",
    "    return diversity\n",
    "\n",
    "def gen_profiling_inputs_in_blackbox(model1, model1_bit_config, model2, model2_bit_config, seed_inputs, use_torch=False, epsilon=0.2):\n",
    "    #논문의 4.2절에서 설명한 테스트 입력 생성 알고리즘 구현\n",
    "    input_shape = seed_inputs[0].shape\n",
    "    n_inputs = seed_inputs.shape[0]\n",
    "    max_iterations = 1000 #최대 반복 횟수 설정\n",
    "    # max_steps = 10 \n",
    "    \n",
    "    \n",
    "    ndims = np.prod(input_shape) #입력의 총 차원 수 계산\n",
    "#         mutate_positions = torch.randperm(ndims)\n",
    "\n",
    "        # Move seed_inputs to GPU if not already\n",
    "    if not seed_inputs.is_cuda:\n",
    "        seed_inputs = seed_inputs.cuda()\n",
    "        \n",
    "    # 초기 모델의 출력 계산\n",
    "    with torch.no_grad():\n",
    "        initial_outputs1 = model1(seed_inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        initial_outputs2 = model2(seed_inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "    \n",
    "    def evaluate_inputs(inputs):\n",
    "        #논문의 Equantion 1에서 설명한 score 함수 구현\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).cuda()\n",
    "        elif inputs.device.type != 'cuda':\n",
    "            inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "            outputs2 = model2(inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        \n",
    "        metrics1 = metrics_output_diversity(model1, model1_bit_config, inputs) #diversity 계산\n",
    "        metrics2 = metrics_output_diversity(model2, model2_bit_config, inputs) # diversity 계산\n",
    "\n",
    "\n",
    "        # divergence 계산 (초기 출력과의 거리)\n",
    "        output_dist1 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs1),\n",
    "            list(initial_outputs1),\n",
    "            metric='euclidean').diagonal())\n",
    "        output_dist2 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs2),\n",
    "            list(initial_outputs2),\n",
    "            metric='euclidean').diagonal())\n",
    "        print(f'  output distance: {output_dist1},{output_dist2}')\n",
    "        print(f'  metrics: {metrics1},{metrics2}')\n",
    "        # if mutated_metrics <= metrics:\n",
    "        #     break\n",
    "        \n",
    "        #score 계산 : divergence와 diversity의 곱\n",
    "        return output_dist1 * output_dist2 * metrics1 * metrics2\n",
    "    \n",
    "    inputs = seed_inputs\n",
    "    score = evaluate_inputs(inputs)\n",
    "    print(f'score={score}')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        #Alogrithm 1: Search-based input generation \n",
    "        # comparator._compute_distance(inputs)\n",
    "        print(f'mutation {i}-th iteration')\n",
    "        # mutation_idx = random.randint(0, len(inputs))\n",
    "        # mutation = np.random.random_sample(size=input_shape).astype(np.float32)\n",
    "        \n",
    "        #무작위 위치 선택하여 mutation 생성\n",
    "        mutation_pos = np.random.randint(0, ndims)\n",
    "        mutation = np.zeros(ndims).astype(np.float32)\n",
    "        mutation[mutation_pos] = epsilon\n",
    "        mutation = np.reshape(mutation, input_shape) #이 코드는 1차원 mutation 벡터를 원래 입력 데이터의 shape으로 재구성합니다.\n",
    "        \n",
    "        \n",
    "        \n",
    "        mutation_batch = torch.zeros_like(inputs)\n",
    "        mutation_idx = np.random.randint(0, n_inputs)\n",
    "        mutation_batch[mutation_idx] = torch.from_numpy(mutation).cuda()\n",
    "        \n",
    "        # print(f'{inputs.shape} {mutation_perturbation.shape}')\n",
    "        # for j in range(max_steps):\n",
    "            # mutated_inputs = np.clip(inputs + mutation, 0, 1)\n",
    "            # print(f'{list(inputs)[0].shape}')\n",
    "        mutate_right_inputs = inputs + mutation_batch\n",
    "        mutate_right_score = evaluate_inputs(mutate_right_inputs)\n",
    "        mutate_left_inputs = inputs - mutation_batch\n",
    "        mutate_left_score = evaluate_inputs(mutate_left_inputs)\n",
    "        \n",
    "        if mutate_right_score <= score and mutate_left_score <= score:\n",
    "            continue\n",
    "        if mutate_right_score > mutate_left_score:\n",
    "            print(f'mutate right: {score}->{mutate_right_score}')\n",
    "            inputs = mutate_right_inputs\n",
    "            score = mutate_right_score\n",
    "        else:\n",
    "            print(f'mutate left: {score}->{mutate_left_score}')\n",
    "            inputs = mutate_left_inputs\n",
    "            score = mutate_left_score\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def gen_profiling_inputs_in_whitebox(model1, model1_bit_config, model2, model2_bit_config, seed_inputs, seed_labels, use_torch=False, epsilon=0.2, whitebox_attack_net=None):\n",
    "    #논문의 4.2절에서 설명한 테스트 입력 생성 알고리즘 구현\n",
    "    input_shape = seed_inputs[0].shape\n",
    "    n_inputs = seed_inputs.shape[0]\n",
    "    max_iterations = 20 #최대 반복 횟수 설정\n",
    "    # max_steps = 10 \n",
    "    \n",
    "    \n",
    "    ndims = np.prod(input_shape) #입력의 총 차원 수 계산\n",
    "#         mutate_positions = torch.randperm(ndims)\n",
    "\n",
    "        # Move seed_inputs to GPU if not already\n",
    "    if not seed_inputs.is_cuda:\n",
    "        seed_inputs = seed_inputs.cuda()\n",
    "        \n",
    "    # 초기 모델의 출력 계산\n",
    "    with torch.no_grad():\n",
    "        initial_outputs1 = model1(seed_inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        initial_outputs2 = model2(seed_inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "    \n",
    "    def evaluate_inputs(inputs):\n",
    "        #논문의 Equantion 1에서 설명한 score 함수 구현\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).cuda()     \n",
    "        elif inputs.device.type != 'cuda':\n",
    "            inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "            outputs2 = model2(inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        \n",
    "        metrics1 = metrics_output_diversity(model1, model1_bit_config, inputs) #diversity 계산\n",
    "        metrics2 = metrics_output_diversity(model2, model2_bit_config, inputs) # diversity 계산\n",
    "\n",
    "\n",
    "        # divergence 계산 (초기 출력과의 거리)\n",
    "        output_dist1 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs1),\n",
    "            list(initial_outputs1),\n",
    "            metric='euclidean').diagonal())\n",
    "        output_dist2 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs2),\n",
    "            list(initial_outputs2),\n",
    "            metric='euclidean').diagonal())\n",
    "        print(f'  output distance: {output_dist1},{output_dist2}')\n",
    "        print(f'  metrics: {metrics1},{metrics2}')\n",
    "        # if mutated_metrics <= metrics:\n",
    "        #     break\n",
    "        \n",
    "        #score 계산 : divergence와 diversity의 곱\n",
    "        return output_dist1 * output_dist2 * metrics1 * metrics2\n",
    "    \n",
    "    inputs = seed_inputs\n",
    "    max_inputs = None\n",
    "    labels = seed_labels\n",
    "    score = evaluate_inputs(inputs)\n",
    "    print(f'score={score}')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        #Alogrithm 1: Search-based input generation \n",
    "        # comparator._compute_distance(inputs)\n",
    "        print(f'mutation {i}-th iteration')\n",
    "        # mutation_idx = random.randint(0, len(inputs))\n",
    "        # mutation = np.random.random_sample(size=input_shape).astype(np.float32)\n",
    "        \n",
    "        #무작위 위치 선택하여 mutation 생성\n",
    "        current_adv_inputs = gen_adv_inputs(model1, inputs, labels, model1_bit_config, attack_net=whitebox_attack_net)\n",
    "        \n",
    "        current_score = evaluate_inputs(current_adv_inputs)\n",
    "        \n",
    "        \n",
    "        if current_score > score:\n",
    "            print(f'current score update: {score}->{current_score}')\n",
    "            max_inputs = current_adv_inputs\n",
    "            score = current_score\n",
    "            \n",
    "    \n",
    "    return max_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deit_tiny_patch16_224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n"
     ]
    }
   ],
   "source": [
    "int8_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "int4_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "not_quantized_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "\n",
    "eight_bit_config = [8]*50\n",
    "not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.06, step_size=0.01, num_steps=50, bit_config=None)\n",
    "four_bit_config = [4]*50\n",
    "seed_images, seed_labels = get_seed_inputs(50, rand=False)\n",
    "adv_inputs = gen_adv_inputs(not_quantized_model, seed_images, seed_labels, bit_config=None, attack_net=not_quantized_attack_net)\n",
    "# mutation_inputs = gen_profiling_inputs_in_blackbox(not_quantized_model, None,  int4_model, four_bit_config, seed_images, epsilon=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutation_adv_inputs = gen_profiling_inputs_in_whitebox(not_quantized_model, None,  int4_model, four_bit_config, seed_images, seed_labels, epsilon=0.02, whitebox_attack_net=not_quantized_attack_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# int8_model = calibrate_model(args.mode, args, int8_model, train_loader, device)\n",
    "# int4_model = calibrate_model(args.mode, args, int4_model, train_loader, device)\n",
    "\n",
    "\n",
    "int8_model.eval()\n",
    "int4_model.eval()\n",
    "not_quantized_model.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cka 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_activations(act):\n",
    "    # 입력 텐서를 2D로 재구성합니다. 첫 번째 차원은 유지하고 나머지는 평탄화합니다.\n",
    "    act = act.view(act.size(0), -1)\n",
    "\n",
    "    # 각 샘플(행)에 대해 L2 norm을 계산합니다.\n",
    "    act_norm = torch.norm(act, p=2, dim=1, keepdim=True)\n",
    "\n",
    "    # 0으로 나누는 것을 방지하기 위해 작은 값을 더합니다.\n",
    "    act_norm = act_norm + 1e-8\n",
    "\n",
    "    # 각 샘플을 해당 norm으로 나누어 정규화합니다.\n",
    "    act = act / act_norm\n",
    "\n",
    "    return act\n",
    "def get_activations(images, model, normalize_act=False):\n",
    "    # 모델을 평가 모드로 설정합니다.\n",
    "\n",
    "    # 중간 활성화를 저장할 리스트를 초기화합니다.\n",
    "    activations = []\n",
    "\n",
    "    # 그래디언트 계산을 비활성화합니다.\n",
    "    with torch.no_grad():\n",
    "        # 각 레이어에 대한 후크(hook) 함수를 정의합니다.\n",
    "        def hook(module, input, output):\n",
    "            activations.append(output)\n",
    "\n",
    "        # 모든 레이어에 후크를 등록합니다.\n",
    "        hooks = []\n",
    "        for layer in model.modules():\n",
    "            # print(layer.__class__.__name__.lower(), \" -> type: \", type(layer))\n",
    "            # if isinstance(layer, (nn.Conv2d, nn.Linear)):  # 원하는 레이어 유형을 선택하세요\n",
    "            #QConv2d, QLinear, QAct, QIntSoftmax\n",
    "            if type(layer) in [QConv2d, QLinear]:\n",
    "                hooks.append(layer.register_forward_hook(hook))\n",
    "\n",
    "        # 모델을 통해 이미지를 전달합니다.\n",
    "        images = images.cuda()\n",
    "        _ = model(images)\n",
    "\n",
    "        # 등록된 후크를 제거합니다.\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "    # 필요한 경우 활성화를 정규화합니다.\n",
    "    if normalize_act:\n",
    "        activations = [normalize_activations(act) for act in activations]\n",
    "\n",
    "    return activations\n",
    "#torch model의 layers의 수를 확인한다.\n",
    "from efficient_CKA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers [4, 21, 29, 46, 51, 61, 69, 86, 91, 101, 109, 126, 131, 141, 149, 166, 171, 181, 189, 206, 211, 221, 229, 246, 251, 261, 269, 286, 291, 301, 309, 326, 331, 341, 349, 366, 371, 381, 389, 406, 411, 421, 429, 446, 451, 461, 469, 486, 491, 502]\n",
      "[4, 21, 29, 46, 51, 61, 69, 86, 91, 101, 109, 126, 131, 141, 149, 166, 171, 181, 189, 206, 211, 221, 229, 246, 251, 261, 269, 286, 291, 301, 309, 326, 331, 341, 349, 366, 371, 381, 389, 406, 411, 421, 429, 446, 451, 461, 469, 486, 491, 502]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "best_model = not_quantized_model\n",
    "# 저장된 모델 불러오기\n",
    "# best_model = torch.load(os.path.join(base_dir, 'model.pth'))\n",
    "\n",
    "# 'add' 연산이 포함된 레이어의 인덱스 찾기\n",
    "layers = []\n",
    "best_model.eval()\n",
    "# print(\"model.modules.length\", len([module for module in best_model.modules()])) // 506\n",
    "with torch.no_grad():\n",
    "    for i, module in enumerate(best_model.modules()):\n",
    "        # print(module.__class__.__name__.lower())\n",
    "        # if isinstance(module, torch.nn.Module) and 'mlp' in module.__class__.__name__.lower():\n",
    "        if type(module) in [QConv2d, QLinear]:\n",
    "            layers.append(i)\n",
    "print(\"layers\", layers)\n",
    "cka_batch = 50\n",
    "\n",
    "\n",
    "sample_cka_dataset = get_dataset(cka_batch)\n",
    "\n",
    "sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "sample_images, _ = sample_cka_dataset\n",
    "# n_layers = len(list(not_quantized_model.children()))\n",
    "# n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "# sample_activations = get_activations(sample_images, best_model)\n",
    "\n",
    "# 모델을 평가 모드로 설정합니다.\n",
    "\n",
    "# 중간 활성화를 저장할 리스트를 초기화합니다.\n",
    "activations = []\n",
    "hooked_activation_index_list = []\n",
    "# 그래디언트 계산을 비활성화합니다.\n",
    "with torch.no_grad():\n",
    "    # 각 레이어에 대한 후크(hook) 함수를 정의합니다.\n",
    "    def hook(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    # 모든 레이어에 후크를 등록합니다.\n",
    "    hooks = []\n",
    "    for index, layer in enumerate(best_model.modules()):\n",
    "        # print(layer.__class__.__name__.lower(), \" -> type: \", type(layer))\n",
    "        # if isinstance(layer, (nn.Conv2d, nn.Linear)):  # 원하는 레이어 유형을 선택하세요\n",
    "        #QConv2d, QLinear, QAct, QIntSoftmax\n",
    "        if type(layer) in [QConv2d, QLinear]:\n",
    "            hooks.append(layer.register_forward_hook(hook))\n",
    "            hooked_activation_index_list.append(index)\n",
    "\n",
    "    # 모델을 통해 이미지를 전달합니다.\n",
    "    images = sample_images.cuda()\n",
    "    _ = best_model(images)\n",
    "\n",
    "    # 등록된 후크를 제거합니다.\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "# 필요한 경우 활성화를 정규화합니다.\n",
    "if True:\n",
    "    activations = [normalize_activations(act) for act in activations]\n",
    "\n",
    "sample_activations = activations\n",
    "\n",
    "\n",
    "n_layers = len(sample_activations)\n",
    "print(hooked_activation_index_list)\n",
    "print(n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def compute_cka_internal(model, use_batch = True,\n",
    "                         use_train_mode = False,\n",
    "                         normalize_act = False,\n",
    "                         cka_batch = 50,\n",
    "                         cka_batch_iter = 10,\n",
    "                         cka_iter = 10):\n",
    "    model.eval()\n",
    "\n",
    "    sample_cka_dataset = get_dataset(cka_batch)\n",
    "\n",
    "    sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "    sample_images, _ = sample_cka_dataset\n",
    "    # n_layers = len(list(not_quantized_model.children()))\n",
    "    # n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "    sample_activations = get_activations(sample_images, model)\n",
    "    n_layers = len(sample_activations)\n",
    "\n",
    "    cka = MinibatchCKA(n_layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 사용 예시:\n",
    "    # model = YourModel()  # PyTorch 모델을 정의하세요\n",
    "    # images = torch.randn(10, 3, 224, 224)  # 예시 입력 이미지\n",
    "    # activations = get_activations(images, model, normalize_act=True)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    if use_batch:\n",
    "        for index in range(cka_iter):\n",
    "            #cka_batch만큼, shuffle해서, 데이터셋을 가져온다.\n",
    "            cka_dataset = get_dataset(cka_batch)\n",
    "            current_iter = 0\n",
    "            for images, _ in cka_dataset:\n",
    "                model_get_activation = get_activations(images, model, normalize_act) #각 모델의 레이어별 활성화를 가져온다.\n",
    "\n",
    "                cka.update_state(model_get_activation) #레이어 마다의 activation을 다 가져옴. 예를 들어 24 * 50 * feature^2. \n",
    "                \n",
    "                if current_iter > cka_batch_iter:\n",
    "                    break\n",
    "                current_iter += 1\n",
    "            print(\"현재 반복:\", index)\n",
    "    else:\n",
    "        cka_dataset = get_dataset(cka_batch)\n",
    "        all_images = []\n",
    "        for images, _ in cka_dataset:\n",
    "            all_images.append(images)\n",
    "        cka.update_state(get_activations(all_images, model, normalize_act))\n",
    "    heatmap = cka.result().cpu().numpy()\n",
    "    with open('cka_result.pkl', 'wb') as f:\n",
    "        #pickle로 heatmap을 저장한다.\n",
    "        pickle.dump(heatmap, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "sample_cka_dataset = get_dataset(50)\n",
    "\n",
    "sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "sample_images, _ = sample_cka_dataset\n",
    "# n_layers = len(list(not_quantized_model.children()))\n",
    "# n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "sample_activations = get_activations(sample_images, not_quantized_model)\n",
    "n_layers = len(sample_activations)\n",
    "\n",
    "print(n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 반복: 0\n",
      "현재 반복: 1\n",
      "현재 반복: 2\n",
      "현재 반복: 3\n",
      "현재 반복: 4\n",
      "현재 반복: 5\n",
      "현재 반복: 6\n",
      "현재 반복: 7\n",
      "현재 반복: 8\n",
      "현재 반복: 9\n"
     ]
    }
   ],
   "source": [
    "compute_cka_internal(not_quantized_model, use_batch = True, normalize_act = False, cka_batch = 50, cka_iter = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKA Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers [4, 21, 29, 46, 51, 61, 69, 86, 91, 101, 109, 126, 131, 141, 149, 166, 171, 181, 189, 206, 211, 221, 229, 246, 251, 261, 269, 286, 291, 301, 309, 326, 331, 341, 349, 366, 371, 381, 389, 406, 411, 421, 429, 446, 451, 461, 469, 486, 491, 502]\n",
      "x | y : 1 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from plot import *\n",
    "\n",
    "\n",
    "base_dir = '/home/jieungkim/quantctr/diff-ViT'\n",
    "\n",
    "\n",
    "\n",
    "# GPU 설정\n",
    "\n",
    "\n",
    "# CKA 결과 파일 경로 설정\n",
    "cka_dir = os.path.join(base_dir, 'cka_result.pkl')\n",
    "best_model = not_quantized_model\n",
    "# 저장된 모델 불러오기\n",
    "# best_model = torch.load(os.path.join(base_dir, 'model.pth'))\n",
    "\n",
    "# 'add' 연산이 포함된 레이어의 인덱스 찾기\n",
    "layers = []\n",
    "best_model.eval()\n",
    "# print(\"model.modules.length\", len([module for module in best_model.modules()])) // 506\n",
    "with torch.no_grad():\n",
    "    for i, module in enumerate(best_model.modules()):\n",
    "        # print(module.__class__.__name__.lower())\n",
    "        # if isinstance(module, torch.nn.Module) and 'mlp' in module.__class__.__name__.lower():\n",
    "        if type(module) in [QConv2d, QLinear]:\n",
    "            layers.append(i)\n",
    "print(\"layers\", layers)\n",
    "\n",
    "# CKA 결과 불러오기\n",
    "with open(cka_dir, 'rb') as f:\n",
    "    cka = pickle.load(f)\n",
    "\n",
    "# 특정 레이어에 대한 CKA 결과 추출\n",
    "# cka1 = cka[layers][:, layers]\n",
    "# print(cka.shape, cka1.shape)\n",
    "\n",
    "# 추출된 CKA 결과 저장\n",
    "out_dir = os.path.join(base_dir, 'cka_within_unquantized_model.pkl')\n",
    "# with open(out_dir, 'wb') as f:\n",
    "#     pickle.dump(cka1, f)\n",
    "\n",
    "# 전체 레이어에 대한 CKA 결과 플롯 생성\n",
    "plot_dir = os.path.join(base_dir, 'layer')\n",
    "plot_ckalist_resume([cka], plot_dir)\n",
    "\n",
    "# 특정 블록에 대한 CKA 결과 플롯 생성\n",
    "# plot_dir = os.path.join(base_dir, 'block')\n",
    "# plot_ckalist_resume([cka1], plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=None)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     eight_bit_config = [8]*50\n",
    "#     four_bit_config = [4] * 50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     # adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int4_model(adv_inputs, four_bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(name, model_outputs):\n",
    "    def hook(module, input, output):\n",
    "        model_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "int8_outputs = {}\n",
    "int4_outputs = {}\n",
    "not_quantized_outputs = {}\n",
    "def add_hooks(model, model_outputs):\n",
    "    # Input quantization\n",
    "    model.qact_input.register_forward_hook(hook_fn(\"qact_input\", model_outputs))\n",
    "    \n",
    "    # Patch Embedding\n",
    "    model.patch_embed.register_forward_hook(hook_fn(\"patch_embed\", model_outputs))\n",
    "    model.patch_embed.qact.register_forward_hook(hook_fn(\"patch_embed_qact\", model_outputs))\n",
    "    \n",
    "    # Position Embedding\n",
    "    model.pos_drop.register_forward_hook(hook_fn(\"pos_drop\", model_outputs))\n",
    "    model.qact_embed.register_forward_hook(hook_fn(\"qact_embed\", model_outputs))\n",
    "    model.qact_pos.register_forward_hook(hook_fn(\"qact_pos\", model_outputs))\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        block.norm1.register_forward_hook(hook_fn(f\"block_{i}_norm1\", model_outputs))\n",
    "        block.attn.qkv.register_forward_hook(hook_fn(f\"block_{i}_attn_qkv\", model_outputs))\n",
    "        block.attn.proj.register_forward_hook(hook_fn(f\"block_{i}_attn_proj\", model_outputs))\n",
    "        block.attn.qact3.register_forward_hook(hook_fn(f\"block_{i}_attn_qact3\", model_outputs))\n",
    "        block.qact2.register_forward_hook(hook_fn(f\"block_{i}_qact2\", model_outputs))\n",
    "        block.norm2.register_forward_hook(hook_fn(f\"block_{i}_norm2\", model_outputs))\n",
    "        block.mlp.fc1.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc1\", model_outputs))\n",
    "        block.mlp.fc2.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc2\", model_outputs))\n",
    "        block.mlp.qact2.register_forward_hook(hook_fn(f\"block_{i}_mlp_qact2\", model_outputs))\n",
    "        block.qact4.register_forward_hook(hook_fn(f\"block_{i}_qact4\", model_outputs))\n",
    "    \n",
    "    # Final Norm Layer\n",
    "    model.norm.register_forward_hook(hook_fn(\"final_norm\", model_outputs))\n",
    "    model.qact2.register_forward_hook(hook_fn(\"final_qact2\", model_outputs))\n",
    "    \n",
    "    # Classifier Head\n",
    "    model.head.register_forward_hook(hook_fn(\"head\", model_outputs))\n",
    "    model.act_out.register_forward_hook(hook_fn(\"act_out\", model_outputs))\n",
    "    \n",
    "add_hooks(int8_model, int8_outputs)\n",
    "add_hooks(int4_model, int4_outputs)\n",
    "add_hooks(not_quantized_model, not_quantized_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv(model, normal_inputs, adv_inputs, outputs, bit_config = None):\n",
    "    if bit_config is not None:\n",
    "        model(normal_inputs, bit_config=bit_config, plot=False)    \n",
    "    else:\n",
    "        model(normal_inputs, plot=False)\n",
    "    normal_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    \n",
    "    # 적대적 입력에 대한 출력 저장\n",
    "    if bit_config is not None:\n",
    "        model(adv_inputs, bit_config=bit_config, plot=False)\n",
    "    else:\n",
    "        model(adv_inputs, plot=False)\n",
    "    adv_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    # print(normal_outputs.keys())\n",
    "    # print(adv_outputs.keys())\n",
    "\n",
    "    model_ddv_dict = {}\n",
    "    #dictionary int8_outputs을 모두 출력한다.\n",
    "    for key in normal_outputs.keys():\n",
    "    \n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        specific_layers_output_pairs = zip(normal_outputs[key], adv_outputs[key])\n",
    "    \n",
    "        ddv = []\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya.detach().cpu().numpy().flatten()\n",
    "            yb = yb.detach().cpu().numpy().flatten()\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb)\n",
    "            ddv.append(cos_similarity)\n",
    "        ddv = np.array(ddv)\n",
    "        norm = np.linalg.norm(ddv)\n",
    "        if norm != 0:\n",
    "            ddv = ddv/ norm\n",
    "        model_ddv_dict[key] = ddv\n",
    "        # print(key, \"레이어에서\", ddv.shape)\n",
    "    return model_ddv_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.66 GiB of which 5.69 MiB is free. Process 224057 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 6.32 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 859.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m int8_bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      2\u001b[0m int4_bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 3\u001b[0m int8_ddv \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ddv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint8_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8_bit_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m int4_ddv \u001b[38;5;241m=\u001b[39m compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n\u001b[1;32m      5\u001b[0m not_quantized_ddv \u001b[38;5;241m=\u001b[39m compute_ddv(not_quantized_model, seed_images, adv_inputs, not_quantized_outputs)\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mcompute_ddv\u001b[0;34m(model, normal_inputs, adv_inputs, outputs, bit_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     model(adv_inputs, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m adv_outputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(normal_outputs.keys())\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(adv_outputs.keys())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_ddv_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     model(adv_inputs, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m adv_outputs \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(normal_outputs.keys())\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(adv_outputs.keys())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_ddv_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.66 GiB of which 5.69 MiB is free. Process 224057 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 6.32 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 859.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n",
    "not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        if not (key in target_ddv.keys()):\n",
    "            continue\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "            # norm = np.linalg.norm(cos_similarity)\n",
    "            # if norm != 0:\n",
    "                # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            # print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, mutation_adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, mutation_adv_inputs, int4_outputs, int4_bit_config)\n",
    "not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, mutation_adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        if not (key in target_ddv.keys()):\n",
    "            continue\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "            # norm = np.linalg.norm(cos_similarity)\n",
    "            # if norm != 0:\n",
    "                # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            # print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qact_input, \"레이어에서\", 100.00, 100.00\n",
      "patch_embed_qact, \"레이어에서\", 100.00, 100.00\n",
      "patch_embed, \"레이어에서\", 100.00, 100.00\n",
      "qact_embed, \"레이어에서\", 100.00, 100.00\n",
      "qact_pos, \"레이어에서\", 100.00, 100.00\n",
      "pos_drop, \"레이어에서\", 100.00, 100.00\n",
      "block_0_norm1, \"레이어에서\", 100.00, 100.00\n",
      "block_0_attn_proj, \"레이어에서\", 100.00, 100.00\n",
      "block_0_attn_qact3, \"레이어에서\", 100.00, 100.00\n",
      "block_0_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_norm2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_mlp_fc2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_mlp_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_qact4, \"레이어에서\", 100.00, 100.00\n",
      "block_1_norm1, \"레이어에서\", 100.00, 100.00\n",
      "block_1_attn_proj, \"레이어에서\", 100.00, 100.00\n",
      "block_1_attn_qact3, \"레이어에서\", 100.00, 100.00\n",
      "block_1_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_1_norm2, \"레이어에서\", 100.00, 100.00\n",
      "block_1_mlp_fc2, \"레이어에서\", 99.99, 99.99\n",
      "block_1_mlp_qact2, \"레이어에서\", 99.99, 99.99\n",
      "block_1_qact4, \"레이어에서\", 100.00, 100.00\n",
      "block_2_norm1, \"레이어에서\", 100.00, 99.99\n",
      "block_2_attn_proj, \"레이어에서\", 99.99, 99.99\n",
      "block_2_attn_qact3, \"레이어에서\", 99.99, 99.99\n",
      "block_2_qact2, \"레이어에서\", 100.00, 99.99\n",
      "block_2_norm2, \"레이어에서\", 100.00, 99.98\n",
      "block_2_mlp_fc2, \"레이어에서\", 99.99, 99.96\n",
      "block_2_mlp_qact2, \"레이어에서\", 99.99, 99.96\n",
      "block_2_qact4, \"레이어에서\", 100.00, 99.99\n",
      "block_3_norm1, \"레이어에서\", 99.99, 99.98\n",
      "block_3_attn_proj, \"레이어에서\", 99.98, 99.97\n",
      "block_3_attn_qact3, \"레이어에서\", 99.98, 99.97\n",
      "block_3_qact2, \"레이어에서\", 99.99, 99.98\n",
      "block_3_norm2, \"레이어에서\", 99.99, 99.97\n",
      "block_3_mlp_fc2, \"레이어에서\", 99.98, 99.96\n",
      "block_3_mlp_qact2, \"레이어에서\", 99.98, 99.96\n",
      "block_3_qact4, \"레이어에서\", 99.99, 99.98\n",
      "block_4_norm1, \"레이어에서\", 99.99, 99.97\n",
      "block_4_attn_proj, \"레이어에서\", 99.92, 99.89\n",
      "block_4_attn_qact3, \"레이어에서\", 99.92, 99.89\n",
      "block_4_qact2, \"레이어에서\", 99.97, 99.95\n",
      "block_4_norm2, \"레이어에서\", 99.96, 99.93\n",
      "block_4_mlp_fc2, \"레이어에서\", 99.93, 99.88\n",
      "block_4_mlp_qact2, \"레이어에서\", 99.93, 99.88\n",
      "block_4_qact4, \"레이어에서\", 99.97, 99.93\n",
      "block_5_norm1, \"레이어에서\", 99.97, 99.92\n",
      "block_5_attn_proj, \"레이어에서\", 99.89, 99.81\n",
      "block_5_attn_qact3, \"레이어에서\", 99.89, 99.81\n",
      "block_5_qact2, \"레이어에서\", 99.96, 99.92\n",
      "block_5_norm2, \"레이어에서\", 99.96, 99.92\n",
      "block_5_mlp_fc2, \"레이어에서\", 99.81, 99.68\n",
      "block_5_mlp_qact2, \"레이어에서\", 99.81, 99.68\n",
      "block_5_qact4, \"레이어에서\", 99.95, 99.90\n",
      "block_6_norm1, \"레이어에서\", 99.95, 99.89\n",
      "block_6_attn_proj, \"레이어에서\", 99.87, 99.77\n",
      "block_6_attn_qact3, \"레이어에서\", 99.87, 99.77\n",
      "block_6_qact2, \"레이어에서\", 99.93, 99.87\n",
      "block_6_norm2, \"레이어에서\", 99.94, 99.88\n",
      "block_6_mlp_fc2, \"레이어에서\", 99.78, 99.51\n",
      "block_6_mlp_qact2, \"레이어에서\", 99.78, 99.51\n",
      "block_6_qact4, \"레이어에서\", 99.91, 99.82\n",
      "block_7_norm1, \"레이어에서\", 99.91, 99.78\n",
      "block_7_attn_proj, \"레이어에서\", 99.83, 99.65\n",
      "block_7_attn_qact3, \"레이어에서\", 99.83, 99.65\n",
      "block_7_qact2, \"레이어에서\", 99.90, 99.80\n",
      "block_7_norm2, \"레이어에서\", 99.92, 99.82\n",
      "block_7_mlp_fc2, \"레이어에서\", 99.67, 99.26\n",
      "block_7_mlp_qact2, \"레이어에서\", 99.67, 99.26\n",
      "block_7_qact4, \"레이어에서\", 99.84, 99.69\n",
      "block_8_norm1, \"레이어에서\", 99.82, 99.60\n",
      "block_8_attn_proj, \"레이어에서\", 99.63, 99.16\n",
      "block_8_attn_qact3, \"레이어에서\", 99.63, 99.16\n",
      "block_8_qact2, \"레이어에서\", 99.84, 99.70\n",
      "block_8_norm2, \"레이어에서\", 99.90, 99.79\n",
      "block_8_mlp_fc2, \"레이어에서\", 99.48, 98.93\n",
      "block_8_mlp_qact2, \"레이어에서\", 99.48, 98.94\n",
      "block_8_qact4, \"레이어에서\", 99.78, 99.60\n",
      "block_9_norm1, \"레이어에서\", 99.71, 99.42\n",
      "block_9_attn_proj, \"레이어에서\", 99.51, 98.77\n",
      "block_9_attn_qact3, \"레이어에서\", 99.51, 98.77\n",
      "block_9_qact2, \"레이어에서\", 99.81, 99.65\n",
      "block_9_norm2, \"레이어에서\", 99.85, 99.71\n",
      "block_9_mlp_fc2, \"레이어에서\", 99.65, 99.33\n",
      "block_9_mlp_qact2, \"레이어에서\", 99.65, 99.33\n",
      "block_9_qact4, \"레이어에서\", 99.79, 99.62\n",
      "block_10_norm1, \"레이어에서\", 99.69, 99.36\n",
      "block_10_attn_proj, \"레이어에서\", 99.16, 98.15\n",
      "block_10_attn_qact3, \"레이어에서\", 99.16, 98.15\n",
      "block_10_qact2, \"레이어에서\", 99.82, 99.68\n",
      "block_10_norm2, \"레이어에서\", 99.85, 99.71\n",
      "block_10_mlp_fc2, \"레이어에서\", 99.40, 98.95\n",
      "block_10_mlp_qact2, \"레이어에서\", 99.41, 98.95\n",
      "block_10_qact4, \"레이어에서\", 99.79, 99.65\n",
      "block_11_norm1, \"레이어에서\", 99.70, 99.43\n",
      "block_11_attn_proj, \"레이어에서\", 96.04, 93.47\n",
      "block_11_attn_qact3, \"레이어에서\", 96.06, 93.47\n",
      "block_11_qact2, \"레이어에서\", 99.72, 99.57\n",
      "block_11_norm2, \"레이어에서\", 99.83, 99.68\n",
      "block_11_mlp_fc2, \"레이어에서\", 99.91, 99.83\n",
      "block_11_mlp_qact2, \"레이어에서\", 99.91, 99.83\n",
      "block_11_qact4, \"레이어에서\", 99.69, 99.51\n",
      "final_norm, \"레이어에서\", 99.75, 99.51\n",
      "final_qact2, \"레이어에서\", 45.06, 35.89\n",
      "head, \"레이어에서\", 51.45, 43.24\n",
      "act_out, \"레이어에서\", 51.43, 43.23\n"
     ]
    }
   ],
   "source": [
    "#int8div와 int4div의 value를 각각 조회해 array의 차이를 출력한다.\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)  # 두 벡터의 내적\n",
    "    norm_vec1 = np.linalg.norm(vec1)  # 첫 번째 벡터의 크기 (norm)\n",
    "    norm_vec2 = np.linalg.norm(vec2)  # 두 번째 벡터의 크기 (norm)\n",
    "    \n",
    "    return dot_product / (norm_vec1 * norm_vec2) \n",
    "\n",
    "\n",
    "for key in int8_ddv.keys():\n",
    "    if  (key in not_quantized_ddv.keys()):\n",
    "        int8_similarity = cosine_similarity(int8_ddv[key], not_quantized_ddv[key])\n",
    "        int4_similarity = cosine_similarity(int4_ddv[key], not_quantized_ddv[key])\n",
    "        int8_similarity = (int8_similarity + 1) /2 * 100\n",
    "        int4_similarity = (int4_similarity + 1) /2 * 100\n",
    "        print(f'{key}, \"레이어에서\", {int8_similarity:.2f}, {int4_similarity:.2f}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 절대적 변화율 계산\n",
    "        # print(np.abs(diff).mean() * 100)\n",
    "        #difference를 통해 mse를 구한다.\n",
    "        # mse = np.square(difference).mean()\n",
    "        # print(mse)\n",
    "        \n",
    "# print(\"int4_ddv - not_quantized_ddv\")\n",
    "#         print(np.array(int4_ddv[key] - not_quantized_ddv[key]).mean()/2 * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [8]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = int8_model(inputs, bit_config, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int8_model(inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [4]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int4_model(adv_inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newerFQ2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
