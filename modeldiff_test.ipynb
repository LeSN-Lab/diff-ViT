{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jieungkim/.conda/envs/ptq4vit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from config import Config\n",
    "from models import *\n",
    "from generate_data import generate_data\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "parser = argparse.ArgumentParser(description='FQ-ViT')\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    choices=[\n",
    "                        'deit_tiny', 'deit_small', 'deit_base', 'vit_base',\n",
    "                        'vit_large', 'swin_tiny', 'swin_small', 'swin_base'\n",
    "                    ],\n",
    "                    default='deit_tiny',\n",
    "                    help='model')\n",
    "parser.add_argument('--data', metavar='DIR',\n",
    "                    default='/home/jieungkim/quantctr/imagenet',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--quant', default=True, action='store_true')\n",
    "parser.add_argument('--ptf', default=False)\n",
    "parser.add_argument('--lis', default=False)\n",
    "parser.add_argument('--quant-method',\n",
    "                    default='minmax',\n",
    "                    choices=['minmax', 'ema', 'omse', 'percentile'])\n",
    "parser.add_argument('--mixed', default=False, action='store_true')\n",
    "# TODO: 100 --> 32\n",
    "parser.add_argument('--calib-batchsize',\n",
    "                    default=5,\n",
    "                    type=int,\n",
    "                    help='batchsize of calibration set')\n",
    "parser.add_argument(\"--mode\", default=0,\n",
    "                        type=int, \n",
    "                        help=\"mode of calibration data, 0: PSAQ-ViT, 1: Gaussian noise, 2: Real data\")\n",
    "# TODO: 10 --> 1\n",
    "parser.add_argument('--calib-iter', default=10, type=int)\n",
    "# TODO: 100 --> 200\n",
    "parser.add_argument('--val-batchsize',\n",
    "                    default=5,\n",
    "                    type=int,\n",
    "                    help='batchsize of validation set')\n",
    "parser.add_argument('--num-workers',\n",
    "                    default=16,\n",
    "                    type=int,\n",
    "                    help='number of data loading workers (default: 16)')\n",
    "parser.add_argument('--device', default='cuda', type=str, help='device')\n",
    "parser.add_argument('--print-freq',\n",
    "                    default=100,\n",
    "                    type=int,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--seed', default=0, type=int, help='seed')\n",
    "\n",
    "\n",
    "def str2model(name):\n",
    "    d = {\n",
    "        'deit_tiny': deit_tiny_patch16_224,\n",
    "        'deit_small': deit_small_patch16_224,\n",
    "        'deit_base': deit_base_patch16_224,\n",
    "        'vit_base': vit_base_patch16_224,\n",
    "        'vit_large': vit_large_patch16_224,\n",
    "        'swin_tiny': swin_tiny_patch4_window7_224,\n",
    "        'swin_small': swin_small_patch4_window7_224,\n",
    "        'swin_base': swin_base_patch4_window7_224,\n",
    "    }\n",
    "    print('Model: %s' % d[name].__name__)\n",
    "    return d[name]\n",
    "\n",
    "\n",
    "def seed(seed=0):\n",
    "    import os\n",
    "    import random\n",
    "    import sys\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    sys.setrecursionlimit(100000)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def build_transform(input_size=224,\n",
    "                    interpolation='bicubic',\n",
    "                    mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225),\n",
    "                    crop_pct=0.875):\n",
    "\n",
    "    def _pil_interp(method):\n",
    "        if method == 'bicubic':\n",
    "            return Image.BICUBIC\n",
    "        elif method == 'lanczos':\n",
    "            return Image.LANCZOS\n",
    "        elif method == 'hamming':\n",
    "            return Image.HAMMING\n",
    "        else:\n",
    "            return Image.BILINEAR\n",
    "\n",
    "    resize_im = input_size > 32\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        size = int(math.floor(input_size / crop_pct))\n",
    "        ip = _pil_interp(interpolation)\n",
    "        t.append(\n",
    "            transforms.Resize(\n",
    "                size,\n",
    "                interpolation=ip),  # to maintain same ratio w.r.t. 224 images\n",
    "        )\n",
    "        t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args, val_loader, model, criterion, device, bit_config=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    val_start_time = end = time.time()\n",
    "    for i, (data, target) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if i == 0:\n",
    "            plot_flag = False\n",
    "        else:\n",
    "            plot_flag = False\n",
    "        with torch.no_grad():\n",
    "            output, FLOPs, distance = model(data, bit_config, plot_flag)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data.item(), data.size(0))\n",
    "        top1.update(prec1.data.item(), data.size(0))\n",
    "        top5.update(prec5.data.item(), data.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      i,\n",
    "                      len(val_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      loss=losses,\n",
    "                      top1=top1,\n",
    "                      top5=top5,\n",
    "                  ))\n",
    "    val_end_time = time.time()\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "          format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_make(model_name, ptf, lis, quant_method, device):\n",
    "    device = torch.device(device)\n",
    "    cfg = Config(ptf, lis, quant_method)\n",
    "    model = str2model(model_name)(pretrained=True, cfg=cfg)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "    \n",
    "def calibrate_model(mode = 0, args = None, model = None, train_loader = None, device = None):\n",
    "    if mode == 2:\n",
    "        print(\"Generating data...\")\n",
    "        calibrate_data = generate_data(args)\n",
    "        print(\"Calibrating with generated data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 1: Gaussian noise\n",
    "    elif args.mode == 1:\n",
    "        calibrate_data = torch.randn((args.calib_batchsize, 3, 224, 224)).to(device)\n",
    "        print(\"Calibrating with Gaussian noise...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 2: Real data (Standard)\n",
    "    elif args.mode == 0:\n",
    "        # Get calibration set.\n",
    "        image_list = []\n",
    "        # output_list = []\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            if i == args.calib_iter:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            # target = target.to(device)\n",
    "            image_list.append(data)\n",
    "            # output_list.append(target)\n",
    "\n",
    "        print(\"Calibrating with real data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            # TODO:\n",
    "            # for i, image in enumerate(image_list):\n",
    "            #     if i == len(image_list) - 1:\n",
    "            #         # This is used for OMSE method to\n",
    "            #         # calculate minimum quantization error\n",
    "            #         model.model_open_last_calibrate()\n",
    "            #     output, FLOPs, global_distance = model(image, plot=False)\n",
    "            # model.model_quant(flag='off')\n",
    "            model.model_open_last_calibrate()\n",
    "            output, FLOPs, global_distance = model(image_list[0], plot=False)\n",
    "\n",
    "    model.model_close_calibrate()\n",
    "    model.model_quant()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "seed(args.seed)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "cfg = Config(args.ptf, args.lis, args.quant_method)\n",
    "# model = str2model(args.model)(pretrained=True, cfg=cfg)\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Note: Different models have different strategies of data preprocessing.\n",
    "model_type = args.model.split('_')[0]\n",
    "if model_type == 'deit':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.875\n",
    "elif model_type == 'vit':\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "    crop_pct = 0.9\n",
    "elif model_type == 'swin':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.9\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "val_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "# Data\n",
    "traindir = os.path.join(args.data, 'train')\n",
    "valdir = os.path.join(args.data, 'val')\n",
    "\n",
    "val_dataset = datasets.ImageFolder(valdir, val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.val_batchsize,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# switch to evaluate mode\n",
    "# model.eval()\n",
    "\n",
    "# define loss function (criterion)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss(yhat, y):\n",
    "\treturn -((yhat[:,0]-y[:,0])**2 + 0.1*((yhat[:,1:]-y[:,1:])**2).mean(1)).mean()\n",
    "\n",
    "class AttackPGD(nn.Module):\n",
    "    def __init__(self, basic_net, epsilon, step_size, num_steps, bit_config):\n",
    "        super(AttackPGD, self).__init__()\n",
    "        self.basic_net = basic_net\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.bit_config = bit_config\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        self.basic_net.zero_grad()\n",
    "        x = inputs.clone().detach()\n",
    "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
    "        for i in range(self.num_steps):\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            # with torch.enable_grad():\n",
    "            outputs, Flops, distance = self.basic_net(x, self.bit_config, False)\n",
    "            loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "            # loss = myloss(outputs, targets)\n",
    "            loss.backward()\n",
    "            # grad = torch.autograd.grad(loss, [x], create_graph=False)[0]\n",
    "            grad = x.grad.clone()\n",
    "            x = x + self.step_size*torch.sign(grad)\n",
    "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
    "            x = torch.clamp(x, inputs.min().item(), inputs.max().item())\n",
    "            # x = torch.clamp(x, 0, 1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.basic_net.eval()\n",
    "                adv_output, Flops, distance= self.basic_net(x, self.bit_config, False)\n",
    "            \n",
    "        return adv_output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_inputs(n, rand=False, input_shape = (3, 224, 224)):\n",
    "    if rand:\n",
    "        batch_input_size = (n, input_shape[0], input_shape[1], input_shape[2])\n",
    "        images = np.random.normal(size = batch_input_size).astype(np.float32)\n",
    "    else:\n",
    "        model_type = args.model.split('_')[0]\n",
    "        if model_type == 'deit':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.875\n",
    "        elif model_type == 'vit':\n",
    "            mean = (0.5, 0.5, 0.5)\n",
    "            std = (0.5, 0.5, 0.5)\n",
    "            crop_pct = 0.9\n",
    "        elif model_type == 'swin':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.9\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "        # Data\n",
    "        traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=n,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        images, labels = next(iter(train_loader))\n",
    "    return images.cuda(), labels.cuda()\n",
    "        \n",
    "def get_dataset(n, input_shape = (3, 224, 224)):\n",
    "    \n",
    "    \n",
    "    model_type = args.model.split('_')[0]\n",
    "    if model_type == 'deit':\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        crop_pct = 0.875\n",
    "    elif model_type == 'vit':\n",
    "        mean = (0.5, 0.5, 0.5)\n",
    "        std = (0.5, 0.5, 0.5)\n",
    "        crop_pct = 0.9\n",
    "    elif model_type == 'swin':\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        crop_pct = 0.9\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "    # Data\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=n,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return train_loader\n",
    "        \n",
    "\n",
    "def gen_adv_inputs(model, inputs, labels, attack_net):\n",
    "    \"\"\"\n",
    "    model에 대해 입력을 받아서 adversarial example을 생성하는 함수입니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # bit_config = [8]*50\n",
    "    # with torch.no_grad():\n",
    "    #     clean_output, FLOPs, distance = model(inputs, bit_config, plot=False)\n",
    "    # output_shape = clean_output.shape\n",
    "    # batch_size = output_shape[0]\n",
    "    # num_classes = output_shape[1]\n",
    "    \n",
    "    \"\"\"\n",
    "    다양성 최대화:\n",
    "    ModelDiff 논문의 4.2절에서는 생성된 입력의 다양성(diversity)을 강조합니다.\n",
    "    target_outputs = output_mean - clean_output를 사용함으로써, 각 입력이 평균 출력과 다르게 되도록 유도하고 있습니다.\n",
    "    이는 생성된 입력들이 서로 다른 특성을 가지도록 하는 데 도움이 됩니다.\n",
    "    \"\"\"\n",
    "    # output_mean = clean_output.mean(axis = 0)\n",
    "    # target_outputs = output_mean - clean_output\n",
    "    \"\"\"\n",
    "    결정 경계 탐색:\n",
    "    y = target_outputs * 1000에서 큰 스케일 팩터(1000)를 사용하는 것은,\n",
    "    모델의 결정 경계를 더 잘 탐색하기 위한 것으로 보입니다.\n",
    "    이는 논문의 Figure 3에서 설명하는 \"decision boundary\" 개념과 연관됩니다.\n",
    "    \"\"\"\n",
    "    # y = target_outputs * 1000 \n",
    "    \n",
    "    adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "    # adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "    torch.cuda.empty_cache()\n",
    "    return adv_inputs.detach()\n",
    "\n",
    "def metrics_output_diversity(model, bit_config, inputs, use_torch=False):\n",
    "    # 논문의 4.2절에서 설명한 출력 다양성 메트릭 계산\n",
    "    outputs = model(inputs, bit_config, False)[0].detach().to('cpu').numpy()\n",
    "#         output_dists = []\n",
    "#         for i in range(0, len(outputs) - 1):\n",
    "#             for j in range(i + 1, len(outputs)):\n",
    "#                 output_dist = spatial.distance.euclidean(outputs[i], outputs[j])\n",
    "#                 output_dists.append(output_dist)\n",
    "#         diversity = sum(output_dists) / len(output_dists)\n",
    "    # cdist 함수는 두 집합 모든 쌍 사이의 거리를 유클리드 거리를 이용해서 계산함.\n",
    "    #outputs_dists는 모든 출력 쌍 사이의 거리를 담은 행렬.\n",
    "    output_dists = spatial.distance.cdist(list(outputs), list(outputs), metric='euclidean')\n",
    "    #계산된 모든 거리의 평균을 구함.\n",
    "    diversity = np.mean(output_dists)\n",
    "    return diversity\n",
    "\n",
    "def gen_profiling_inputs_in_blackbox(model1, model1_bit_config, model2, model2_bit_config, seed_inputs, use_torch=False, epsilon=0.2):\n",
    "    #논문의 4.2절에서 설명한 테스트 입력 생성 알고리즘 구현\n",
    "    input_shape = seed_inputs[0].shape\n",
    "    n_inputs = seed_inputs.shape[0]\n",
    "    max_iterations = 1000 #최대 반복 횟수 설정\n",
    "    # max_steps = 10 \n",
    "    \n",
    "    \n",
    "    ndims = np.prod(input_shape) #입력의 총 차원 수 계산\n",
    "#         mutate_positions = torch.randperm(ndims)\n",
    "\n",
    "        # Move seed_inputs to GPU if not already\n",
    "    if not seed_inputs.is_cuda:\n",
    "        seed_inputs = seed_inputs.cuda()\n",
    "        \n",
    "    # 초기 모델의 출력 계산\n",
    "    with torch.no_grad():\n",
    "        initial_outputs1 = model1(seed_inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        initial_outputs2 = model2(seed_inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "    \n",
    "    def evaluate_inputs(inputs):\n",
    "        #논문의 Equantion 1에서 설명한 score 함수 구현\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).cuda()\n",
    "        elif inputs.device.type != 'cuda':\n",
    "            inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "            outputs2 = model2(inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        \n",
    "        metrics1 = metrics_output_diversity(model1, model1_bit_config, inputs) #diversity 계산\n",
    "        metrics2 = metrics_output_diversity(model2, model2_bit_config, inputs) # diversity 계산\n",
    "\n",
    "\n",
    "        # divergence 계산 (초기 출력과의 거리)\n",
    "        output_dist1 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs1),\n",
    "            list(initial_outputs1),\n",
    "            metric='euclidean').diagonal())\n",
    "        output_dist2 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs2),\n",
    "            list(initial_outputs2),\n",
    "            metric='euclidean').diagonal())\n",
    "        print(f'  output distance: {output_dist1},{output_dist2}')\n",
    "        print(f'  metrics: {metrics1},{metrics2}')\n",
    "        # if mutated_metrics <= metrics:\n",
    "        #     break\n",
    "        \n",
    "        #score 계산 : divergence와 diversity의 곱\n",
    "        return output_dist1 * output_dist2 * metrics1 * metrics2\n",
    "    \n",
    "    inputs = seed_inputs\n",
    "    score = evaluate_inputs(inputs)\n",
    "    print(f'score={score}')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        #Alogrithm 1: Search-based input generation \n",
    "        # comparator._compute_distance(inputs)\n",
    "        print(f'mutation {i}-th iteration')\n",
    "        # mutation_idx = random.randint(0, len(inputs))\n",
    "        # mutation = np.random.random_sample(size=input_shape).astype(np.float32)\n",
    "        \n",
    "        #무작위 위치 선택하여 mutation 생성\n",
    "        mutation_pos = np.random.randint(0, ndims)\n",
    "        mutation = np.zeros(ndims).astype(np.float32)\n",
    "        mutation[mutation_pos] = epsilon\n",
    "        mutation = np.reshape(mutation, input_shape) #이 코드는 1차원 mutation 벡터를 원래 입력 데이터의 shape으로 재구성합니다.\n",
    "        \n",
    "        \n",
    "        \n",
    "        mutation_batch = torch.zeros_like(inputs)\n",
    "        mutation_idx = np.random.randint(0, n_inputs)\n",
    "        mutation_batch[mutation_idx] = torch.from_numpy(mutation).cuda()\n",
    "        \n",
    "        # print(f'{inputs.shape} {mutation_perturbation.shape}')\n",
    "        # for j in range(max_steps):\n",
    "            # mutated_inputs = np.clip(inputs + mutation, 0, 1)\n",
    "            # print(f'{list(inputs)[0].shape}')\n",
    "        mutate_right_inputs = inputs + mutation_batch\n",
    "        mutate_right_score = evaluate_inputs(mutate_right_inputs)\n",
    "        mutate_left_inputs = inputs - mutation_batch\n",
    "        mutate_left_score = evaluate_inputs(mutate_left_inputs)\n",
    "        \n",
    "        if mutate_right_score <= score and mutate_left_score <= score:\n",
    "            continue\n",
    "        if mutate_right_score > mutate_left_score:\n",
    "            print(f'mutate right: {score}->{mutate_right_score}')\n",
    "            inputs = mutate_right_inputs\n",
    "            score = mutate_right_score\n",
    "        else:\n",
    "            print(f'mutate left: {score}->{mutate_left_score}')\n",
    "            inputs = mutate_left_inputs\n",
    "            score = mutate_left_score\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def gen_profiling_inputs_in_whitebox(model1, model1_bit_config, model2, model2_bit_config, seed_inputs, seed_labels, use_torch=False, epsilon=0.2, whitebox_attack_net=None):\n",
    "    #논문의 4.2절에서 설명한 테스트 입력 생성 알고리즘 구현\n",
    "    input_shape = seed_inputs[0].shape\n",
    "    n_inputs = seed_inputs.shape[0]\n",
    "    max_iterations = 20 #최대 반복 횟수 설정\n",
    "    # max_steps = 10 \n",
    "    \n",
    "    \n",
    "    ndims = np.prod(input_shape) #입력의 총 차원 수 계산\n",
    "#         mutate_positions = torch.randperm(ndims)\n",
    "\n",
    "        # Move seed_inputs to GPU if not already\n",
    "    if not seed_inputs.is_cuda:\n",
    "        seed_inputs = seed_inputs.cuda()\n",
    "        \n",
    "    # 초기 모델의 출력 계산\n",
    "    with torch.no_grad():\n",
    "        initial_outputs1 = model1(seed_inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        initial_outputs2 = model2(seed_inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "    \n",
    "    def evaluate_inputs(inputs):\n",
    "        #논문의 Equantion 1에서 설명한 score 함수 구현\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).cuda()     \n",
    "        elif inputs.device.type != 'cuda':\n",
    "            inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "            outputs2 = model2(inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        \n",
    "        metrics1 = metrics_output_diversity(model1, model1_bit_config, inputs) #diversity 계산\n",
    "        metrics2 = metrics_output_diversity(model2, model2_bit_config, inputs) # diversity 계산\n",
    "\n",
    "\n",
    "        # divergence 계산 (초기 출력과의 거리)\n",
    "        output_dist1 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs1),\n",
    "            list(initial_outputs1),\n",
    "            metric='euclidean').diagonal())\n",
    "        output_dist2 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs2),\n",
    "            list(initial_outputs2),\n",
    "            metric='euclidean').diagonal())\n",
    "        print(f'  output distance: {output_dist1},{output_dist2}')\n",
    "        print(f'  metrics: {metrics1},{metrics2}')\n",
    "        # if mutated_metrics <= metrics:\n",
    "        #     break\n",
    "        \n",
    "        #score 계산 : divergence와 diversity의 곱\n",
    "        return output_dist1 * output_dist2 * metrics1 * metrics2\n",
    "    \n",
    "    inputs = seed_inputs\n",
    "    max_inputs = None\n",
    "    labels = seed_labels\n",
    "    score = evaluate_inputs(inputs)\n",
    "    print(f'score={score}')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        #Alogrithm 1: Search-based input generation \n",
    "        # comparator._compute_distance(inputs)\n",
    "        print(f'mutation {i}-th iteration')\n",
    "        # mutation_idx = random.randint(0, len(inputs))\n",
    "        # mutation = np.random.random_sample(size=input_shape).astype(np.float32)\n",
    "        \n",
    "        #무작위 위치 선택하여 mutation 생성\n",
    "        current_adv_inputs = gen_adv_inputs(model1, inputs, labels, model1_bit_config, attack_net=whitebox_attack_net)\n",
    "        \n",
    "        current_score = evaluate_inputs(current_adv_inputs)\n",
    "        \n",
    "        \n",
    "        if current_score > score:\n",
    "            print(f'current score update: {score}->{current_score}')\n",
    "            max_inputs = current_adv_inputs\n",
    "            score = current_score\n",
    "            \n",
    "    \n",
    "    return max_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n"
     ]
    }
   ],
   "source": [
    "int8_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "int4_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "not_quantized_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "\n",
    "eight_bit_config = [8]*50\n",
    "not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.06, step_size=0.01, num_steps=50, bit_config=None)\n",
    "four_bit_config = [4]*50\n",
    "seed_images, seed_labels = get_seed_inputs(50, rand=False)\n",
    "adv_inputs = gen_adv_inputs(not_quantized_model, seed_images, seed_labels, attack_net=not_quantized_attack_net)\n",
    "# mutation_inputs = gen_profiling_inputs_in_blackbox(not_quantized_model, None,  int4_model, four_bit_config, seed_images, epsilon=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutation_adv_inputs = gen_profiling_inputs_in_whitebox(not_quantized_model, None,  int4_model, four_bit_config, seed_images, seed_labels, epsilon=0.02, whitebox_attack_net=not_quantized_attack_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating with real data...\n",
      "Calibrating with real data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "int8_model = calibrate_model(args.mode, args, int8_model, train_loader, device)\n",
    "int4_model = calibrate_model(args.mode, args, int4_model, train_loader, device)\n",
    "\n",
    "\n",
    "int8_model.eval()\n",
    "int4_model.eval()\n",
    "not_quantized_model.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cka 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_activations(act):\n",
    "    # 입력 텐서를 2D로 재구성합니다. 첫 번째 차원은 유지하고 나머지는 평탄화합니다.\n",
    "    act = act.view(act.size(0), -1)\n",
    "\n",
    "    # 각 샘플(행)에 대해 L2 norm을 계산합니다.\n",
    "    act_norm = torch.norm(act, p=2, dim=1, keepdim=True)\n",
    "\n",
    "    # 0으로 나누는 것을 방지하기 위해 작은 값을 더합니다.\n",
    "    act_norm = act_norm + 1e-8\n",
    "\n",
    "    # 각 샘플을 해당 norm으로 나누어 정규화합니다.\n",
    "    act = act / act_norm\n",
    "\n",
    "    return act\n",
    "#torch model의 layers의 수를 확인한다.\n",
    "from efficient_CKA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(images, model, bit_config, normalize_act=False):\n",
    "    model = model.to(device)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def get_module_path(module):\n",
    "        return f\"{module.__class__.__module__}.{module.__class__.__name__}\"\n",
    "\n",
    "    activations = []\n",
    "    layer_info = []\n",
    "    from models.vit_fquant import Attention, Mlp\n",
    "    def hook_return(index):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(module, Attention):\n",
    "                activations.append(module.qkv_output)\n",
    "                layer_info.append({\n",
    "                'relative_index': len(activations) - 1,\n",
    "                'absolute_index': index,\n",
    "                'name': module.__class__.__name__,\n",
    "                'layer_type': type(module),\n",
    "                'path': get_module_path(module)\n",
    "\n",
    "                })\n",
    "            elif isinstance(module, Mlp):\n",
    "                activations.append(module.fc1_output)\n",
    "                layer_info.append({\n",
    "                'relative_index': len(activations) - 1,\n",
    "                'absolute_index': index,\n",
    "                'name': module.__class__.__name__,\n",
    "                'layer_type': type(module),\n",
    "                'path': get_module_path(module)\n",
    "\n",
    "                })\n",
    "            else:\n",
    "                activations.append(output)\n",
    "                layer_info.append({\n",
    "                    'relative_index': len(activations) - 1,\n",
    "                    'absolute_index': index,\n",
    "                    'name': module.__class__.__name__,\n",
    "                    'layer_type': type(module),\n",
    "                    'path': get_module_path(module)\n",
    "\n",
    "                })\n",
    "            \n",
    "\n",
    "        return hook\n",
    "\n",
    "    hooks = []\n",
    "\n",
    "    if bit_config is None:\n",
    "        for index, layer in enumerate(model.modules()):\n",
    "            if type(layer) in [QConv2d, QLinear, Attention, Mlp]:\n",
    "                hooks.append(layer.register_forward_hook(hook_return(index)))\n",
    "    else:\n",
    "        for index, layer in enumerate(model.modules()):\n",
    "            if type(layer) in [QConv2d, QLinear]:\n",
    "                hooks.append(layer.register_forward_hook(hook_return(index)))\n",
    "    # 모델을 통해 이미지를 전달합니다.\n",
    "    images = images.cuda()\n",
    "    _ = model(images, bit_config = bit_config, plot=False)\n",
    "\n",
    "    # 등록된 후크를 제거합니다.\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # layer_info와 activations를 절대 인덱스를 기준으로 정렬\n",
    "    sorted_indices = sorted(range(len(layer_info)), key=lambda k: layer_info[k]['absolute_index'])\n",
    "    layer_info = [layer_info[i] for i in sorted_indices]\n",
    "    activations = [activations[i] for i in sorted_indices]\n",
    "\n",
    "    # 상대 인덱스 재할당\n",
    "    for i, info in enumerate(layer_info):\n",
    "        info['relative_index'] = i\n",
    "\n",
    "\n",
    "    if normalize_act:\n",
    "        activations = [normalize_activations(act) for act in activations]\n",
    "    # for info in layer_info:\n",
    "    #     print(f\"Layer {info['relative_index']}(absolute: {info['absolute_index']}): {info['name']} (Type: {info['layer_type']}, Path: {info['path']})\")\n",
    "    return activations\n",
    "    # 정렬된 레이어 정보 출력\n",
    "    \n",
    "\n",
    "    # print(f\"\\nTotal number of activations: {len(activations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0(absolute: 4): QConv2d (Type: <class 'models.ptq.layers.QConv2d'>, Path: models.ptq.layers.QConv2d)\n",
      "Layer 1(absolute: 21): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 2(absolute: 29): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 3(absolute: 46): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 4(absolute: 51): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 5(absolute: 61): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 6(absolute: 69): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 7(absolute: 86): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 8(absolute: 91): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 9(absolute: 101): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 10(absolute: 109): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 11(absolute: 126): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 12(absolute: 131): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 13(absolute: 141): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 14(absolute: 149): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 15(absolute: 166): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 16(absolute: 171): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 17(absolute: 181): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 18(absolute: 189): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 19(absolute: 206): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 20(absolute: 211): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 21(absolute: 221): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 22(absolute: 229): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 23(absolute: 246): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 24(absolute: 251): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 25(absolute: 261): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 26(absolute: 269): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 27(absolute: 286): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 28(absolute: 291): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 29(absolute: 301): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 30(absolute: 309): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 31(absolute: 326): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 32(absolute: 331): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 33(absolute: 341): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 34(absolute: 349): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 35(absolute: 366): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 36(absolute: 371): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 37(absolute: 381): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 38(absolute: 389): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 39(absolute: 406): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 40(absolute: 411): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 41(absolute: 421): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 42(absolute: 429): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 43(absolute: 446): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 44(absolute: 451): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 45(absolute: 461): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 46(absolute: 469): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 47(absolute: 486): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 48(absolute: 491): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n",
      "Layer 49(absolute: 502): QLinear (Type: <class 'models.ptq.layers.QLinear'>, Path: models.ptq.layers.QLinear)\n"
     ]
    }
   ],
   "source": [
    "sample_cka_dataset = get_dataset(10)\n",
    "\n",
    "sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "sample_images, _ = sample_cka_dataset\n",
    "# n_layers = len(list(not_quantized_model.children()))\n",
    "# n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "sample_activations = get_activations(sample_images, int8_model, four_bit_config, normalize_act=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def compute_cka_internal(model, use_batch = True,\n",
    "                         use_train_mode = False,\n",
    "                         normalize_act = False,\n",
    "                         cka_batch = 50,\n",
    "                         cka_batch_iter = 10,\n",
    "                         cka_iter = 10,\n",
    "                         result_name = 'cka_result.pkl'\n",
    "                         ):\n",
    "    model.eval()\n",
    "\n",
    "    sample_cka_dataset = get_dataset(cka_batch)\n",
    "\n",
    "    sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "    sample_images, _ = sample_cka_dataset\n",
    "    # n_layers = len(list(not_quantized_model.children()))\n",
    "    # n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "    sample_activations = get_activations(sample_images, model, None, normalize_act)\n",
    "    n_layers = len(sample_activations)\n",
    "\n",
    "    cka = MinibatchCKA(n_layers)\n",
    "    \n",
    "\n",
    "    if use_batch:\n",
    "        for index in range(cka_iter):\n",
    "            #cka_batch만큼, shuffle해서, 데이터셋을 가져온다.\n",
    "            cka_dataset = get_dataset(cka_batch)\n",
    "            current_iter = 0\n",
    "            for images, _ in cka_dataset:\n",
    "                model_get_activation = get_activations(images, model, None, normalize_act) #각 모델의 레이어별 활성화를 가져온다.\n",
    "\n",
    "                cka.update_state(model_get_activation) #레이어 마다의 activation을 다 가져옴. 예를 들어 24 * 50 * feature^2. \n",
    "                \n",
    "                if current_iter > cka_batch_iter:\n",
    "                    break\n",
    "                current_iter += 1\n",
    "            print(\"현재 반복:\", index)\n",
    "    else:\n",
    "        cka_dataset = get_dataset(cka_batch)\n",
    "        all_images = []\n",
    "        for images, _ in cka_dataset:\n",
    "            all_images.append(images)\n",
    "        cka.update_state(get_activations(all_images, model, None, normalize_act))\n",
    "    heatmap = cka.result().cpu().numpy()\n",
    "    with open(result_name, 'wb') as f:\n",
    "        #pickle로 heatmap을 저장한다.\n",
    "        pickle.dump(heatmap, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# sample_cka_dataset = get_dataset(50)\n",
    "\n",
    "# sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "# sample_images, _ = sample_cka_dataset\n",
    "# # n_layers = len(list(not_quantized_model.children()))\n",
    "# # n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "# sample_activations = get_activations(sample_images, not_quantized_model)\n",
    "# n_layers = len(sample_activations)\n",
    "\n",
    "# print(n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 반복: 0\n",
      "현재 반복: 1\n",
      "현재 반복: 2\n",
      "현재 반복: 3\n",
      "현재 반복: 4\n",
      "현재 반복: 5\n",
      "현재 반복: 6\n",
      "현재 반복: 7\n",
      "현재 반복: 8\n",
      "현재 반복: 9\n",
      "현재 반복: 10\n",
      "현재 반복: 11\n",
      "현재 반복: 12\n",
      "현재 반복: 13\n",
      "현재 반복: 14\n",
      "현재 반복: 15\n",
      "현재 반복: 16\n",
      "현재 반복: 17\n",
      "현재 반복: 18\n",
      "현재 반복: 19\n"
     ]
    }
   ],
   "source": [
    "compute_cka_internal(int4_model, use_batch = True, normalize_act = False, cka_batch = 20, cka_iter = 20, result_name='cka_int4_result.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKA Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from plot import *\n",
    "\n",
    "def plot_cka_map(cka_file_name, plot_name):\n",
    "    base_dir = '/home/jieungkim/quantctr/diff-ViT'\n",
    "\n",
    "\n",
    "\n",
    "    # GPU 설정\n",
    "\n",
    "\n",
    "    # CKA 결과 파일 경로 설정\n",
    "    cka_dir = os.path.join(base_dir, cka_file_name)\n",
    "\n",
    "\n",
    "    # CKA 결과 불러오기\n",
    "    with open(cka_dir, 'rb') as f:\n",
    "        cka = pickle.load(f)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # 전체 레이어에 대한 CKA 결과 플롯 생성\n",
    "    plot_dir = os.path.join(base_dir, plot_name)\n",
    "    plot_ckalist_resume([cka], plot_dir)\n",
    "# plot_cka_map('cka_not_quantized_result.pkl', 'cka_not_quantized_result.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## not quantized, int8, int4간의 cka map 코사인 유사\n",
    "\n",
    "=> 같은 모델을 사용하기 때문에 각 레이어간 유사도는 유사할것이라고 생각할 수 있음. 그러한 모델의 특성간의 유사도가 그다지는 차이가 안나는게 맞다고 생각됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between Not Quantized and INT8: 0.99977183\n",
      "Cosine Similarity between Not Quantized and INT4: 0.9998938\n",
      "Cosine Similarity between INT8 and INT4: 0.99985826\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# 1. 필요한 라이브러리 임포트\n",
    "\n",
    "# 2. pkl 파일 로드\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "cka_not_quantized = load_pickle('cka_not_quantized_result.pkl')\n",
    "cka_int8 = load_pickle('cka_int8_result.pkl')\n",
    "cka_int4 = load_pickle('cka_int4_result.pkl')\n",
    "\n",
    "# 3. 코사인 유사도 계산 함수 정의\n",
    "def cosine_similarity(a, b):\n",
    "    return 1 - cosine(a.flatten(), b.flatten())\n",
    "\n",
    "# 4. 각 쌍의 히트맵에 대해 코사인 유사도 계산\n",
    "similarity_not_quantized_int8 = cosine_similarity(cka_not_quantized, cka_int8)\n",
    "similarity_not_quantized_int4 = cosine_similarity(cka_not_quantized, cka_int4)\n",
    "similarity_int8_int4 = cosine_similarity(cka_int8, cka_int4)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Cosine Similarity between Not Quantized and INT8:\", similarity_not_quantized_int8)\n",
    "print(\"Cosine Similarity between Not Quantized and INT4:\", similarity_not_quantized_int4)\n",
    "print(\"Cosine Similarity between INT8 and INT4:\", similarity_int8_int4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddv+cka를 활용한 점수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDV_CKA import *\n",
    "def compute_cka_with_adversarial(model1, model2, use_batch = True,\n",
    "                         normalize_act = False,\n",
    "                         cka_batch = 50,\n",
    "                         cka_batch_iter = 10,\n",
    "                         cka_iter = 10,\n",
    "                         result_name = 'cka_result.pkl',\n",
    "                         model1_bit_config = None,\n",
    "                         model2_bit_config = None,\n",
    "                         ):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    sample_cka_dataset = get_dataset(cka_batch)\n",
    "\n",
    "    sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "    sample_images, sample_labels = sample_cka_dataset\n",
    "    cka_attack_net1 = AttackPGD(model1, epsilon=0.06, step_size=0.01, num_steps=50, bit_config = model1_bit_config)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # cka_attack_net2 = AttackPGD(model2, epsilon=0.06, step_size=0.01, num_steps=50, bit_config = bit_config)\n",
    "    #@To Do: cka_attack_net2를 직접 사용해보기\n",
    "    cka_attack_net2 = cka_attack_net1 #모델1과 같은 공격 네트워크를 사용한다. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    sample_activations = get_activations(sample_images, model1, model1_bit_config, normalize_act)\n",
    "    n_layers = len(sample_activations)\n",
    "\n",
    "    cka = MinibatchAdvCKA(n_layers)\n",
    "    \n",
    "\n",
    "    if use_batch:\n",
    "        for index in range(cka_iter):\n",
    "            #cka_batch만큼, shuffle해서, 데이터셋을 가져온다.\n",
    "            cka_dataset = get_dataset(cka_batch)\n",
    "            current_iter = 0\n",
    "            for images, labels in cka_dataset:\n",
    "                adv_images = gen_adv_inputs(model1, images, labels, cka_attack_net1)\n",
    "                \n",
    "                model1_get_activation = get_activations(images, model1, model1_bit_config, normalize_act) #각 모델의 레이어별 활성화를 가져온다.\n",
    "                model1_get_adv_activation = get_activations(adv_images, model1, model1_bit_config, normalize_act)\n",
    "                \n",
    "                model2_get_activation = get_activations(images, model2, model2_bit_config, normalize_act)\n",
    "                model2_get_adv_activation = get_activations(adv_images, model2, model2_bit_config, normalize_act)\n",
    "                \n",
    "                cka.update_state(model1_activations=model1_get_activation,\n",
    "                                 model1_adv_activations=model1_get_adv_activation,\n",
    "                                 model2_activations=model2_get_activation,\n",
    "                                 model2_adv_activations=model2_get_adv_activation) #레이어 마다의 activation을 다 가져옴. 예를 들어 24 * 50 * feature^2. \n",
    "                \n",
    "                if current_iter > cka_batch_iter:\n",
    "                    break\n",
    "                current_iter += 1\n",
    "            print(\"현재 반복:\", index)\n",
    "    else:\n",
    "        cka_dataset = get_dataset(cka_batch)\n",
    "        all_images = []\n",
    "        all_labels = []\n",
    "        for images, labels in cka_dataset:\n",
    "            all_images.append(images)\n",
    "            all_labels.append(labels)\n",
    "            all_adv_images = gen_adv_inputs(model1, all_images, all_labels, cka_attack_net1)\n",
    "        cka.update_state(\n",
    "            model1_activations=get_activations(all_images, model1,  model1_bit_config, normalize_act),\n",
    "            model1_adv_activations=get_activations(all_adv_images, model1, model1_bit_config, normalize_act),\n",
    "            model2_activations=get_activations(all_images, model2, model2_bit_config, normalize_act),\n",
    "            model2_adv_activations=get_activations(all_adv_images, model2, model2_bit_config, normalize_act),\n",
    "            )\n",
    "    heatmap = cka.result().cpu().numpy()\n",
    "    with open(result_name, 'wb') as f:\n",
    "        #pickle로 heatmap을 저장한다.\n",
    "        pickle.dump(heatmap, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 반복: 0\n",
      "현재 반복: 1\n",
      "현재 반복: 2\n",
      "현재 반복: 3\n",
      "현재 반복: 4\n",
      "현재 반복: 5\n",
      "현재 반복: 6\n",
      "현재 반복: 7\n",
      "현재 반복: 8\n",
      "현재 반복: 9\n",
      "현재 반복: 10\n",
      "현재 반복: 11\n",
      "현재 반복: 12\n",
      "현재 반복: 13\n",
      "현재 반복: 14\n",
      "현재 반복: 15\n",
      "현재 반복: 16\n",
      "현재 반복: 17\n",
      "현재 반복: 18\n",
      "현재 반복: 19\n"
     ]
    }
   ],
   "source": [
    "compute_cka_with_adversarial(not_quantized_model,\n",
    "                             not_quantized_model, \n",
    "                             use_batch = True, \n",
    "                             normalize_act = False, \n",
    "                             cka_batch = 10, \n",
    "                             cka_iter = 20, \n",
    "                             result_name='cka_with_adversarial_not_quantized_not_quantized.pkl', \n",
    "                             model1_bit_config = None,\n",
    "                             model2_bit_config = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x | y : 1 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cka_map('cka_with_adversarial_int8_not_quantized.pkl', 'cka_with_adversarial_int8_not_quantized.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal values: [1.         1.0000002  0.99999994 1.0000001  1.0000001  0.99999994\n",
      " 1.         1.         1.0000001  0.99999994 1.         0.9999999\n",
      " 0.9999998  0.9999999  0.99999976 1.         0.9999998  1.0000001\n",
      " 1.0000001  1.0000001  1.0000002  1.         1.         1.\n",
      " 0.99999994 1.0000001  1.         1.         0.9999999  0.9999998\n",
      " 1.0000001  0.9999998  1.         1.0000001  0.9999999  1.0000001\n",
      " 0.9999999  0.99999994 1.         1.0000001  1.0000001  1.\n",
      " 0.9999999  1.         1.0000001  0.99999994 1.         0.99999994\n",
      " 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def load_and_plot_diagonal(pickle_file, save_name):\n",
    "    # pickle 파일 불러오기\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        heatmap = pickle.load(f)\n",
    "\n",
    "    # 대각 성분 추출\n",
    "    diagonal = np.diag(heatmap)\n",
    "\n",
    "    # 그래프 그리기\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(diagonal, marker='o')\n",
    "    plt.title('Diagonal Elements of CKA Matrix')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('CKA Value')\n",
    "    plt.ylim(0, 1)  # CKA 값의 범위는 0에서 1 사이입니다\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 그래프 저장\n",
    "    plt.savefig(f'{save_name}_diagonal.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return diagonal\n",
    "\n",
    "# 사용 예시\n",
    "\n",
    "pickle_file = 'cka_with_adversarial_int8_not_quantized.pkl'\n",
    "save_name = 'cka_with_adversarial_int8_not_quantized'\n",
    "diagonal_values = load_and_plot_diagonal(pickle_file, save_name)\n",
    "\n",
    "# 대각 값들 출력\n",
    "print(\"Diagonal values:\", diagonal_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=None)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     eight_bit_config = [8]*50\n",
    "#     four_bit_config = [4] * 50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     # adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int4_model(adv_inputs, four_bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(name, model_outputs):\n",
    "    def hook(module, input, output):\n",
    "        model_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "int8_outputs = {}\n",
    "int4_outputs = {}\n",
    "not_quantized_outputs = {}\n",
    "def add_hooks(model, model_outputs):\n",
    "    # Input quantization\n",
    "    model.qact_input.register_forward_hook(hook_fn(\"qact_input\", model_outputs))\n",
    "    \n",
    "    # Patch Embedding\n",
    "    model.patch_embed.register_forward_hook(hook_fn(\"patch_embed\", model_outputs))\n",
    "    model.patch_embed.qact.register_forward_hook(hook_fn(\"patch_embed_qact\", model_outputs))\n",
    "    \n",
    "    # Position Embedding\n",
    "    model.pos_drop.register_forward_hook(hook_fn(\"pos_drop\", model_outputs))\n",
    "    model.qact_embed.register_forward_hook(hook_fn(\"qact_embed\", model_outputs))\n",
    "    model.qact_pos.register_forward_hook(hook_fn(\"qact_pos\", model_outputs))\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        block.norm1.register_forward_hook(hook_fn(f\"block_{i}_norm1\", model_outputs))\n",
    "        block.attn.qkv.register_forward_hook(hook_fn(f\"block_{i}_attn_qkv\", model_outputs))\n",
    "        block.attn.proj.register_forward_hook(hook_fn(f\"block_{i}_attn_proj\", model_outputs))\n",
    "        block.attn.qact3.register_forward_hook(hook_fn(f\"block_{i}_attn_qact3\", model_outputs))\n",
    "        block.qact2.register_forward_hook(hook_fn(f\"block_{i}_qact2\", model_outputs))\n",
    "        block.norm2.register_forward_hook(hook_fn(f\"block_{i}_norm2\", model_outputs))\n",
    "        block.mlp.fc1.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc1\", model_outputs))\n",
    "        block.mlp.fc2.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc2\", model_outputs))\n",
    "        block.mlp.qact2.register_forward_hook(hook_fn(f\"block_{i}_mlp_qact2\", model_outputs))\n",
    "        block.qact4.register_forward_hook(hook_fn(f\"block_{i}_qact4\", model_outputs))\n",
    "    \n",
    "    # Final Norm Layer\n",
    "    model.norm.register_forward_hook(hook_fn(\"final_norm\", model_outputs))\n",
    "    model.qact2.register_forward_hook(hook_fn(\"final_qact2\", model_outputs))\n",
    "    \n",
    "    # Classifier Head\n",
    "    model.head.register_forward_hook(hook_fn(\"head\", model_outputs))\n",
    "    model.act_out.register_forward_hook(hook_fn(\"act_out\", model_outputs))\n",
    "    \n",
    "add_hooks(int8_model, int8_outputs)\n",
    "add_hooks(int4_model, int4_outputs)\n",
    "add_hooks(not_quantized_model, not_quantized_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv(model, normal_inputs, adv_inputs, outputs, bit_config = None):\n",
    "    if bit_config is not None:\n",
    "        model(normal_inputs, bit_config=bit_config, plot=False)    \n",
    "    else:\n",
    "        model(normal_inputs, plot=False)\n",
    "    normal_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    \n",
    "    # 적대적 입력에 대한 출력 저장\n",
    "    if bit_config is not None:\n",
    "        model(adv_inputs, bit_config=bit_config, plot=False)\n",
    "    else:\n",
    "        model(adv_inputs, plot=False)\n",
    "    adv_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    # print(normal_outputs.keys())\n",
    "    # print(adv_outputs.keys())\n",
    "\n",
    "    model_ddv_dict = {}\n",
    "    #dictionary int8_outputs을 모두 출력한다.\n",
    "    for key in normal_outputs.keys():\n",
    "    \n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        specific_layers_output_pairs = zip(normal_outputs[key], adv_outputs[key])\n",
    "    \n",
    "        ddv = []\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya.detach().cpu().numpy().flatten()\n",
    "            yb = yb.detach().cpu().numpy().flatten()\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb)\n",
    "            ddv.append(cos_similarity)\n",
    "        ddv = np.array(ddv)\n",
    "        norm = np.linalg.norm(ddv)\n",
    "        if norm != 0:\n",
    "            ddv = ddv/ norm\n",
    "        model_ddv_dict[key] = ddv\n",
    "        # print(key, \"레이어에서\", ddv.shape)\n",
    "    return model_ddv_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.66 GiB of which 5.69 MiB is free. Process 224057 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 6.32 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 859.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m int8_bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      2\u001b[0m int4_bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 3\u001b[0m int8_ddv \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ddv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint8_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8_bit_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m int4_ddv \u001b[38;5;241m=\u001b[39m compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n\u001b[1;32m      5\u001b[0m not_quantized_ddv \u001b[38;5;241m=\u001b[39m compute_ddv(not_quantized_model, seed_images, adv_inputs, not_quantized_outputs)\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mcompute_ddv\u001b[0;34m(model, normal_inputs, adv_inputs, outputs, bit_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     model(adv_inputs, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m adv_outputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(normal_outputs.keys())\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(adv_outputs.keys())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_ddv_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     model(adv_inputs, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m adv_outputs \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(normal_outputs.keys())\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(adv_outputs.keys())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_ddv_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.66 GiB of which 5.69 MiB is free. Process 224057 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 6.32 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 859.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n",
    "not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        if not (key in target_ddv.keys()):\n",
    "            continue\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "            # norm = np.linalg.norm(cos_similarity)\n",
    "            # if norm != 0:\n",
    "                # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            # print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, mutation_adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, mutation_adv_inputs, int4_outputs, int4_bit_config)\n",
    "not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, mutation_adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        if not (key in target_ddv.keys()):\n",
    "            continue\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "            # norm = np.linalg.norm(cos_similarity)\n",
    "            # if norm != 0:\n",
    "                # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            # print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qact_input, \"레이어에서\", 100.00, 100.00\n",
      "patch_embed_qact, \"레이어에서\", 100.00, 100.00\n",
      "patch_embed, \"레이어에서\", 100.00, 100.00\n",
      "qact_embed, \"레이어에서\", 100.00, 100.00\n",
      "qact_pos, \"레이어에서\", 100.00, 100.00\n",
      "pos_drop, \"레이어에서\", 100.00, 100.00\n",
      "block_0_norm1, \"레이어에서\", 100.00, 100.00\n",
      "block_0_attn_proj, \"레이어에서\", 100.00, 100.00\n",
      "block_0_attn_qact3, \"레이어에서\", 100.00, 100.00\n",
      "block_0_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_norm2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_mlp_fc2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_mlp_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_qact4, \"레이어에서\", 100.00, 100.00\n",
      "block_1_norm1, \"레이어에서\", 100.00, 100.00\n",
      "block_1_attn_proj, \"레이어에서\", 100.00, 100.00\n",
      "block_1_attn_qact3, \"레이어에서\", 100.00, 100.00\n",
      "block_1_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_1_norm2, \"레이어에서\", 100.00, 100.00\n",
      "block_1_mlp_fc2, \"레이어에서\", 99.99, 99.99\n",
      "block_1_mlp_qact2, \"레이어에서\", 99.99, 99.99\n",
      "block_1_qact4, \"레이어에서\", 100.00, 100.00\n",
      "block_2_norm1, \"레이어에서\", 100.00, 99.99\n",
      "block_2_attn_proj, \"레이어에서\", 99.99, 99.99\n",
      "block_2_attn_qact3, \"레이어에서\", 99.99, 99.99\n",
      "block_2_qact2, \"레이어에서\", 100.00, 99.99\n",
      "block_2_norm2, \"레이어에서\", 100.00, 99.98\n",
      "block_2_mlp_fc2, \"레이어에서\", 99.99, 99.96\n",
      "block_2_mlp_qact2, \"레이어에서\", 99.99, 99.96\n",
      "block_2_qact4, \"레이어에서\", 100.00, 99.99\n",
      "block_3_norm1, \"레이어에서\", 99.99, 99.98\n",
      "block_3_attn_proj, \"레이어에서\", 99.98, 99.97\n",
      "block_3_attn_qact3, \"레이어에서\", 99.98, 99.97\n",
      "block_3_qact2, \"레이어에서\", 99.99, 99.98\n",
      "block_3_norm2, \"레이어에서\", 99.99, 99.97\n",
      "block_3_mlp_fc2, \"레이어에서\", 99.98, 99.96\n",
      "block_3_mlp_qact2, \"레이어에서\", 99.98, 99.96\n",
      "block_3_qact4, \"레이어에서\", 99.99, 99.98\n",
      "block_4_norm1, \"레이어에서\", 99.99, 99.97\n",
      "block_4_attn_proj, \"레이어에서\", 99.92, 99.89\n",
      "block_4_attn_qact3, \"레이어에서\", 99.92, 99.89\n",
      "block_4_qact2, \"레이어에서\", 99.97, 99.95\n",
      "block_4_norm2, \"레이어에서\", 99.96, 99.93\n",
      "block_4_mlp_fc2, \"레이어에서\", 99.93, 99.88\n",
      "block_4_mlp_qact2, \"레이어에서\", 99.93, 99.88\n",
      "block_4_qact4, \"레이어에서\", 99.97, 99.93\n",
      "block_5_norm1, \"레이어에서\", 99.97, 99.92\n",
      "block_5_attn_proj, \"레이어에서\", 99.89, 99.81\n",
      "block_5_attn_qact3, \"레이어에서\", 99.89, 99.81\n",
      "block_5_qact2, \"레이어에서\", 99.96, 99.92\n",
      "block_5_norm2, \"레이어에서\", 99.96, 99.92\n",
      "block_5_mlp_fc2, \"레이어에서\", 99.81, 99.68\n",
      "block_5_mlp_qact2, \"레이어에서\", 99.81, 99.68\n",
      "block_5_qact4, \"레이어에서\", 99.95, 99.90\n",
      "block_6_norm1, \"레이어에서\", 99.95, 99.89\n",
      "block_6_attn_proj, \"레이어에서\", 99.87, 99.77\n",
      "block_6_attn_qact3, \"레이어에서\", 99.87, 99.77\n",
      "block_6_qact2, \"레이어에서\", 99.93, 99.87\n",
      "block_6_norm2, \"레이어에서\", 99.94, 99.88\n",
      "block_6_mlp_fc2, \"레이어에서\", 99.78, 99.51\n",
      "block_6_mlp_qact2, \"레이어에서\", 99.78, 99.51\n",
      "block_6_qact4, \"레이어에서\", 99.91, 99.82\n",
      "block_7_norm1, \"레이어에서\", 99.91, 99.78\n",
      "block_7_attn_proj, \"레이어에서\", 99.83, 99.65\n",
      "block_7_attn_qact3, \"레이어에서\", 99.83, 99.65\n",
      "block_7_qact2, \"레이어에서\", 99.90, 99.80\n",
      "block_7_norm2, \"레이어에서\", 99.92, 99.82\n",
      "block_7_mlp_fc2, \"레이어에서\", 99.67, 99.26\n",
      "block_7_mlp_qact2, \"레이어에서\", 99.67, 99.26\n",
      "block_7_qact4, \"레이어에서\", 99.84, 99.69\n",
      "block_8_norm1, \"레이어에서\", 99.82, 99.60\n",
      "block_8_attn_proj, \"레이어에서\", 99.63, 99.16\n",
      "block_8_attn_qact3, \"레이어에서\", 99.63, 99.16\n",
      "block_8_qact2, \"레이어에서\", 99.84, 99.70\n",
      "block_8_norm2, \"레이어에서\", 99.90, 99.79\n",
      "block_8_mlp_fc2, \"레이어에서\", 99.48, 98.93\n",
      "block_8_mlp_qact2, \"레이어에서\", 99.48, 98.94\n",
      "block_8_qact4, \"레이어에서\", 99.78, 99.60\n",
      "block_9_norm1, \"레이어에서\", 99.71, 99.42\n",
      "block_9_attn_proj, \"레이어에서\", 99.51, 98.77\n",
      "block_9_attn_qact3, \"레이어에서\", 99.51, 98.77\n",
      "block_9_qact2, \"레이어에서\", 99.81, 99.65\n",
      "block_9_norm2, \"레이어에서\", 99.85, 99.71\n",
      "block_9_mlp_fc2, \"레이어에서\", 99.65, 99.33\n",
      "block_9_mlp_qact2, \"레이어에서\", 99.65, 99.33\n",
      "block_9_qact4, \"레이어에서\", 99.79, 99.62\n",
      "block_10_norm1, \"레이어에서\", 99.69, 99.36\n",
      "block_10_attn_proj, \"레이어에서\", 99.16, 98.15\n",
      "block_10_attn_qact3, \"레이어에서\", 99.16, 98.15\n",
      "block_10_qact2, \"레이어에서\", 99.82, 99.68\n",
      "block_10_norm2, \"레이어에서\", 99.85, 99.71\n",
      "block_10_mlp_fc2, \"레이어에서\", 99.40, 98.95\n",
      "block_10_mlp_qact2, \"레이어에서\", 99.41, 98.95\n",
      "block_10_qact4, \"레이어에서\", 99.79, 99.65\n",
      "block_11_norm1, \"레이어에서\", 99.70, 99.43\n",
      "block_11_attn_proj, \"레이어에서\", 96.04, 93.47\n",
      "block_11_attn_qact3, \"레이어에서\", 96.06, 93.47\n",
      "block_11_qact2, \"레이어에서\", 99.72, 99.57\n",
      "block_11_norm2, \"레이어에서\", 99.83, 99.68\n",
      "block_11_mlp_fc2, \"레이어에서\", 99.91, 99.83\n",
      "block_11_mlp_qact2, \"레이어에서\", 99.91, 99.83\n",
      "block_11_qact4, \"레이어에서\", 99.69, 99.51\n",
      "final_norm, \"레이어에서\", 99.75, 99.51\n",
      "final_qact2, \"레이어에서\", 45.06, 35.89\n",
      "head, \"레이어에서\", 51.45, 43.24\n",
      "act_out, \"레이어에서\", 51.43, 43.23\n"
     ]
    }
   ],
   "source": [
    "#int8div와 int4div의 value를 각각 조회해 array의 차이를 출력한다.\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)  # 두 벡터의 내적\n",
    "    norm_vec1 = np.linalg.norm(vec1)  # 첫 번째 벡터의 크기 (norm)\n",
    "    norm_vec2 = np.linalg.norm(vec2)  # 두 번째 벡터의 크기 (norm)\n",
    "    \n",
    "    return dot_product / (norm_vec1 * norm_vec2) \n",
    "\n",
    "\n",
    "for key in int8_ddv.keys():\n",
    "    if  (key in not_quantized_ddv.keys()):\n",
    "        int8_similarity = cosine_similarity(int8_ddv[key], not_quantized_ddv[key])\n",
    "        int4_similarity = cosine_similarity(int4_ddv[key], not_quantized_ddv[key])\n",
    "        int8_similarity = (int8_similarity + 1) /2 * 100\n",
    "        int4_similarity = (int4_similarity + 1) /2 * 100\n",
    "        print(f'{key}, \"레이어에서\", {int8_similarity:.2f}, {int4_similarity:.2f}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 절대적 변화율 계산\n",
    "        # print(np.abs(diff).mean() * 100)\n",
    "        #difference를 통해 mse를 구한다.\n",
    "        # mse = np.square(difference).mean()\n",
    "        # print(mse)\n",
    "        \n",
    "# print(\"int4_ddv - not_quantized_ddv\")\n",
    "#         print(np.array(int4_ddv[key] - not_quantized_ddv[key]).mean()/2 * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [8]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = int8_model(inputs, bit_config, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int8_model(inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [4]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int4_model(adv_inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newerFQ2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
