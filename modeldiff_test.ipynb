{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AI2/anaconda3/envs/p2vit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from config import Config\n",
    "from models import *\n",
    "from generate_data import generate_data\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "parser = argparse.ArgumentParser(description='FQ-ViT')\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    choices=[\n",
    "                        'deit_tiny', 'deit_small', 'deit_base', 'vit_base',\n",
    "                        'vit_large', 'swin_tiny', 'swin_small', 'swin_base'\n",
    "                    ],\n",
    "                    default='deit_tiny',\n",
    "                    help='model')\n",
    "parser.add_argument('--data', metavar='DIR',\n",
    "                    default='/home/shared/DATA/imagenet',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--quant', default=True, action='store_true')\n",
    "parser.add_argument('--ptf', default=True)\n",
    "parser.add_argument('--lis', default=True)\n",
    "parser.add_argument('--quant-method',\n",
    "                    default='minmax',\n",
    "                    choices=['minmax', 'ema', 'omse', 'percentile'])\n",
    "parser.add_argument('--mixed', default=False, action='store_true')\n",
    "# TODO: 100 --> 32\n",
    "parser.add_argument('--calib-batchsize',\n",
    "                    default=50,\n",
    "                    type=int,\n",
    "                    help='batchsize of calibration set')\n",
    "parser.add_argument(\"--mode\", default=0,\n",
    "                        type=int, \n",
    "                        help=\"mode of calibration data, 0: PSAQ-ViT, 1: Gaussian noise, 2: Real data\")\n",
    "# TODO: 10 --> 1\n",
    "parser.add_argument('--calib-iter', default=10, type=int)\n",
    "# TODO: 100 --> 200\n",
    "parser.add_argument('--val-batchsize',\n",
    "                    default=50,\n",
    "                    type=int,\n",
    "                    help='batchsize of validation set')\n",
    "parser.add_argument('--num-workers',\n",
    "                    default=16,\n",
    "                    type=int,\n",
    "                    help='number of data loading workers (default: 16)')\n",
    "parser.add_argument('--device', default='cuda', type=str, help='device')\n",
    "parser.add_argument('--print-freq',\n",
    "                    default=100,\n",
    "                    type=int,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--seed', default=0, type=int, help='seed')\n",
    "\n",
    "\n",
    "def str2model(name):\n",
    "    d = {\n",
    "        'deit_tiny': deit_tiny_patch16_224,\n",
    "        'deit_small': deit_small_patch16_224,\n",
    "        'deit_base': deit_base_patch16_224,\n",
    "        'vit_base': vit_base_patch16_224,\n",
    "        'vit_large': vit_large_patch16_224,\n",
    "        'swin_tiny': swin_tiny_patch4_window7_224,\n",
    "        'swin_small': swin_small_patch4_window7_224,\n",
    "        'swin_base': swin_base_patch4_window7_224,\n",
    "    }\n",
    "    print('Model: %s' % d[name].__name__)\n",
    "    return d[name]\n",
    "\n",
    "\n",
    "def seed(seed=0):\n",
    "    import os\n",
    "    import random\n",
    "    import sys\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    sys.setrecursionlimit(100000)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def build_transform(input_size=224,\n",
    "                    interpolation='bicubic',\n",
    "                    mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225),\n",
    "                    crop_pct=0.875):\n",
    "\n",
    "    def _pil_interp(method):\n",
    "        if method == 'bicubic':\n",
    "            return Image.BICUBIC\n",
    "        elif method == 'lanczos':\n",
    "            return Image.LANCZOS\n",
    "        elif method == 'hamming':\n",
    "            return Image.HAMMING\n",
    "        else:\n",
    "            return Image.BILINEAR\n",
    "\n",
    "    resize_im = input_size > 32\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        size = int(math.floor(input_size / crop_pct))\n",
    "        ip = _pil_interp(interpolation)\n",
    "        t.append(\n",
    "            transforms.Resize(\n",
    "                size,\n",
    "                interpolation=ip),  # to maintain same ratio w.r.t. 224 images\n",
    "        )\n",
    "        t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args, val_loader, model, criterion, device, bit_config=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    val_start_time = end = time.time()\n",
    "    for i, (data, target) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if i == 0:\n",
    "            plot_flag = False\n",
    "        else:\n",
    "            plot_flag = False\n",
    "        with torch.no_grad():\n",
    "            output, FLOPs, distance = model(data, bit_config, plot_flag)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data.item(), data.size(0))\n",
    "        top1.update(prec1.data.item(), data.size(0))\n",
    "        top5.update(prec5.data.item(), data.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      i,\n",
    "                      len(val_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      loss=losses,\n",
    "                      top1=top1,\n",
    "                      top5=top5,\n",
    "                  ))\n",
    "    val_end_time = time.time()\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "          format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_make(model_name, ptf, lis, quant_method, device):\n",
    "    device = torch.device(device)\n",
    "    cfg = Config(ptf, lis, quant_method)\n",
    "    model = str2model(model_name)(pretrained=True, cfg=cfg)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "    \n",
    "def calibrate_model(mode = 0, args = None, model = None, train_loader = None, device = None):\n",
    "    if mode == 2:\n",
    "        print(\"Generating data...\")\n",
    "        calibrate_data = generate_data(args)\n",
    "        print(\"Calibrating with generated data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 1: Gaussian noise\n",
    "    elif args.mode == 1:\n",
    "        calibrate_data = torch.randn((args.calib_batchsize, 3, 224, 224)).to(device)\n",
    "        print(\"Calibrating with Gaussian noise...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 2: Real data (Standard)\n",
    "    elif args.mode == 0:\n",
    "        # Get calibration set.\n",
    "        image_list = []\n",
    "        # output_list = []\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            if i == args.calib_iter:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            # target = target.to(device)\n",
    "            image_list.append(data)\n",
    "            # output_list.append(target)\n",
    "\n",
    "        print(\"Calibrating with real data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            # TODO:\n",
    "            # for i, image in enumerate(image_list):\n",
    "            #     if i == len(image_list) - 1:\n",
    "            #         # This is used for OMSE method to\n",
    "            #         # calculate minimum quantization error\n",
    "            #         model.model_open_last_calibrate()\n",
    "            #     output, FLOPs, global_distance = model(image, plot=False)\n",
    "            # model.model_quant(flag='off')\n",
    "            model.model_open_last_calibrate()\n",
    "            output, FLOPs, global_distance = model(image_list[0], plot=False)\n",
    "\n",
    "    model.model_close_calibrate()\n",
    "    model.model_quant()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "seed(args.seed)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "cfg = Config(args.ptf, args.lis, args.quant_method)\n",
    "# model = str2model(args.model)(pretrained=True, cfg=cfg)\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Note: Different models have different strategies of data preprocessing.\n",
    "model_type = args.model.split('_')[0]\n",
    "if model_type == 'deit':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.875\n",
    "elif model_type == 'vit':\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "    crop_pct = 0.9\n",
    "elif model_type == 'swin':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.9\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "val_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "# Data\n",
    "traindir = os.path.join(args.data, 'train')\n",
    "valdir = os.path.join(args.data, 'val')\n",
    "\n",
    "val_dataset = datasets.ImageFolder(valdir, val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.val_batchsize,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# switch to evaluate mode\n",
    "# model.eval()\n",
    "\n",
    "# define loss function (criterion)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss(yhat, y):\n",
    "\treturn -((yhat[:,0]-y[:,0])**2 + 0.1*((yhat[:,1:]-y[:,1:])**2).mean(1)).mean()\n",
    "\n",
    "class AttackPGD(nn.Module):\n",
    "    def __init__(self, basic_net, epsilon, step_size, num_steps, bit_config):\n",
    "        super(AttackPGD, self).__init__()\n",
    "        self.basic_net = basic_net\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.bit_config = bit_config\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        self.basic_net.zero_grad()\n",
    "        x = inputs.clone().detach()\n",
    "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
    "        for i in range(self.num_steps):\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            # with torch.enable_grad():\n",
    "            outputs, Flops, distance = self.basic_net(x, self.bit_config, False)\n",
    "            loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "            # loss = myloss(outputs, targets)\n",
    "            loss.backward()\n",
    "            # grad = torch.autograd.grad(loss, [x], create_graph=False)[0]\n",
    "            grad = x.grad.clone()\n",
    "            x = x + self.step_size*torch.sign(grad)\n",
    "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
    "            x = torch.clamp(x, inputs.min().item(), inputs.max().item())\n",
    "            # x = torch.clamp(x, 0, 1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.basic_net.eval()\n",
    "                adv_output, Flops, distance= self.basic_net(x, self.bit_config, False)\n",
    "            \n",
    "        return adv_output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_inputs(n, rand=False, input_shape = (3, 224, 224)):\n",
    "    if rand:\n",
    "        batch_input_size = (n, input_shape[0], input_shape[1], input_shape[2])\n",
    "        images = np.random.normal(size = batch_input_size).astype(np.float32)\n",
    "    else:\n",
    "        model_type = args.model.split('_')[0]\n",
    "        if model_type == 'deit':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.875\n",
    "        elif model_type == 'vit':\n",
    "            mean = (0.5, 0.5, 0.5)\n",
    "            std = (0.5, 0.5, 0.5)\n",
    "            crop_pct = 0.9\n",
    "        elif model_type == 'swin':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.9\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "        # Data\n",
    "        traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=n,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        images, labels = next(iter(train_loader))\n",
    "    return images.cuda(), labels.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def gen_adv_inputs(model, inputs, labels, bit_config, attack_net):\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # bit_config = [8]*50\n",
    "    with torch.no_grad():\n",
    "        clean_output, FLOPs, distance = model(inputs, bit_config, plot=False)\n",
    "    # output_shape = clean_output.shape\n",
    "    # batch_size = output_shape[0]\n",
    "    # num_classes = output_shape[1]\n",
    "    \n",
    "    \"\"\"\n",
    "    다양성 최대화:\n",
    "    ModelDiff 논문의 4.2절에서는 생성된 입력의 다양성(diversity)을 강조합니다.\n",
    "    target_outputs = output_mean - clean_output를 사용함으로써, 각 입력이 평균 출력과 다르게 되도록 유도하고 있습니다.\n",
    "    이는 생성된 입력들이 서로 다른 특성을 가지도록 하는 데 도움이 됩니다.\n",
    "    \"\"\"\n",
    "    output_mean = clean_output.mean(axis = 0)\n",
    "    target_outputs = output_mean - clean_output\n",
    "    \"\"\"\n",
    "    결정 경계 탐색:\n",
    "    y = target_outputs * 1000에서 큰 스케일 팩터(1000)를 사용하는 것은,\n",
    "    모델의 결정 경계를 더 잘 탐색하기 위한 것으로 보입니다.\n",
    "    이는 논문의 Figure 3에서 설명하는 \"decision boundary\" 개념과 연관됩니다.\n",
    "    \"\"\"\n",
    "    y = target_outputs * 1000 \n",
    "    \n",
    "    adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "    # adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "    torch.cuda.empty_cache()\n",
    "    return adv_inputs.detach()\n",
    "\n",
    "def metrics_output_diversity(model, bit_config, inputs, use_torch=False):\n",
    "    # 논문의 4.2절에서 설명한 출력 다양성 메트릭 계산\n",
    "    outputs = model(inputs, bit_config, False)[0].detach().to('cpu').numpy()\n",
    "#         output_dists = []\n",
    "#         for i in range(0, len(outputs) - 1):\n",
    "#             for j in range(i + 1, len(outputs)):\n",
    "#                 output_dist = spatial.distance.euclidean(outputs[i], outputs[j])\n",
    "#                 output_dists.append(output_dist)\n",
    "#         diversity = sum(output_dists) / len(output_dists)\n",
    "    # cdist 함수는 두 집합 모든 쌍 사이의 거리를 유클리드 거리를 이용해서 계산함.\n",
    "    #outputs_dists는 모든 출력 쌍 사이의 거리를 담은 행렬.\n",
    "    output_dists = spatial.distance.cdist(list(outputs), list(outputs), metric='euclidean')\n",
    "    #계산된 모든 거리의 평균을 구함.\n",
    "    diversity = np.mean(output_dists)\n",
    "    return diversity\n",
    "\n",
    "def gen_profiling_inputs_in_blackbox(model1, model1_bit_config, model2, model2_bit_config, seed_inputs, use_torch=False, epsilon=0.2):\n",
    "    #논문의 4.2절에서 설명한 테스트 입력 생성 알고리즘 구현\n",
    "    input_shape = seed_inputs[0].shape\n",
    "    n_inputs = seed_inputs.shape[0]\n",
    "    max_iterations = 1000 #최대 반복 횟수 설정\n",
    "    # max_steps = 10 \n",
    "    \n",
    "    \n",
    "    ndims = np.prod(input_shape) #입력의 총 차원 수 계산\n",
    "#         mutate_positions = torch.randperm(ndims)\n",
    "\n",
    "        # Move seed_inputs to GPU if not already\n",
    "    if not seed_inputs.is_cuda:\n",
    "        seed_inputs = seed_inputs.cuda()\n",
    "        \n",
    "    # 초기 모델의 출력 계산\n",
    "    with torch.no_grad():\n",
    "        initial_outputs1 = model1(seed_inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        initial_outputs2 = model2(seed_inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "    \n",
    "    def evaluate_inputs(inputs):\n",
    "        #논문의 Equantion 1에서 설명한 score 함수 구현\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).cuda()\n",
    "        elif inputs.device.type != 'cuda':\n",
    "            inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "            outputs2 = model2(inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        \n",
    "        metrics1 = metrics_output_diversity(model1, model1_bit_config, inputs) #diversity 계산\n",
    "        metrics2 = metrics_output_diversity(model2, model2_bit_config, inputs) # diversity 계산\n",
    "\n",
    "\n",
    "        # divergence 계산 (초기 출력과의 거리)\n",
    "        output_dist1 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs1),\n",
    "            list(initial_outputs1),\n",
    "            metric='euclidean').diagonal())\n",
    "        output_dist2 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs2),\n",
    "            list(initial_outputs2),\n",
    "            metric='euclidean').diagonal())\n",
    "        print(f'  output distance: {output_dist1},{output_dist2}')\n",
    "        print(f'  metrics: {metrics1},{metrics2}')\n",
    "        # if mutated_metrics <= metrics:\n",
    "        #     break\n",
    "        \n",
    "        #score 계산 : divergence와 diversity의 곱\n",
    "        return output_dist1 * output_dist2 * metrics1 * metrics2\n",
    "    \n",
    "    inputs = seed_inputs\n",
    "    score = evaluate_inputs(inputs)\n",
    "    print(f'score={score}')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        #Alogrithm 1: Search-based input generation \n",
    "        # comparator._compute_distance(inputs)\n",
    "        print(f'mutation {i}-th iteration')\n",
    "        # mutation_idx = random.randint(0, len(inputs))\n",
    "        # mutation = np.random.random_sample(size=input_shape).astype(np.float32)\n",
    "        \n",
    "        #무작위 위치 선택하여 mutation 생성\n",
    "        mutation_pos = np.random.randint(0, ndims)\n",
    "        mutation = np.zeros(ndims).astype(np.float32)\n",
    "        mutation[mutation_pos] = epsilon\n",
    "        mutation = np.reshape(mutation, input_shape) #이 코드는 1차원 mutation 벡터를 원래 입력 데이터의 shape으로 재구성합니다.\n",
    "        \n",
    "        \n",
    "        \n",
    "        mutation_batch = torch.zeros_like(inputs)\n",
    "        mutation_idx = np.random.randint(0, n_inputs)\n",
    "        mutation_batch[mutation_idx] = torch.from_numpy(mutation).cuda()\n",
    "        \n",
    "        # print(f'{inputs.shape} {mutation_perturbation.shape}')\n",
    "        # for j in range(max_steps):\n",
    "            # mutated_inputs = np.clip(inputs + mutation, 0, 1)\n",
    "            # print(f'{list(inputs)[0].shape}')\n",
    "        mutate_right_inputs = inputs + mutation_batch\n",
    "        mutate_right_score = evaluate_inputs(mutate_right_inputs)\n",
    "        mutate_left_inputs = inputs - mutation_batch\n",
    "        mutate_left_score = evaluate_inputs(mutate_left_inputs)\n",
    "        \n",
    "        if mutate_right_score <= score and mutate_left_score <= score:\n",
    "            continue\n",
    "        if mutate_right_score > mutate_left_score:\n",
    "            print(f'mutate right: {score}->{mutate_right_score}')\n",
    "            inputs = mutate_right_inputs\n",
    "            score = mutate_right_score\n",
    "        else:\n",
    "            print(f'mutate left: {score}->{mutate_left_score}')\n",
    "            inputs = mutate_left_inputs\n",
    "            score = mutate_left_score\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def gen_profiling_inputs_in_whitebox(model1, model1_bit_config, model2, model2_bit_config, seed_inputs, seed_labels, use_torch=False, epsilon=0.2, attack_net=None):\n",
    "    #논문의 4.2절에서 설명한 테스트 입력 생성 알고리즘 구현\n",
    "    input_shape = seed_inputs[0].shape\n",
    "    n_inputs = seed_inputs.shape[0]\n",
    "    max_iterations = 20 #최대 반복 횟수 설정\n",
    "    # max_steps = 10 \n",
    "    \n",
    "    \n",
    "    ndims = np.prod(input_shape) #입력의 총 차원 수 계산\n",
    "#         mutate_positions = torch.randperm(ndims)\n",
    "\n",
    "        # Move seed_inputs to GPU if not already\n",
    "    if not seed_inputs.is_cuda:\n",
    "        seed_inputs = seed_inputs.cuda()\n",
    "        \n",
    "    # 초기 모델의 출력 계산\n",
    "    with torch.no_grad():\n",
    "        initial_outputs1 = model1(seed_inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        initial_outputs2 = model2(seed_inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "    \n",
    "    def evaluate_inputs(inputs):\n",
    "        #논문의 Equantion 1에서 설명한 score 함수 구현\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).cuda()     \n",
    "        elif inputs.device.type != 'cuda':\n",
    "            inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "            outputs2 = model2(inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        \n",
    "        metrics1 = metrics_output_diversity(model1, model1_bit_config, inputs) #diversity 계산\n",
    "        metrics2 = metrics_output_diversity(model2, model2_bit_config, inputs) # diversity 계산\n",
    "\n",
    "\n",
    "        # divergence 계산 (초기 출력과의 거리)\n",
    "        output_dist1 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs1),\n",
    "            list(initial_outputs1),\n",
    "            metric='euclidean').diagonal())\n",
    "        output_dist2 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs2),\n",
    "            list(initial_outputs2),\n",
    "            metric='euclidean').diagonal())\n",
    "        print(f'  output distance: {output_dist1},{output_dist2}')\n",
    "        print(f'  metrics: {metrics1},{metrics2}')\n",
    "        # if mutated_metrics <= metrics:\n",
    "        #     break\n",
    "        \n",
    "        #score 계산 : divergence와 diversity의 곱\n",
    "        return output_dist1 * output_dist2 * metrics1 * metrics2\n",
    "    \n",
    "    inputs = seed_inputs\n",
    "    max_inputs = None\n",
    "    labels = seed_labels\n",
    "    score = evaluate_inputs(inputs)\n",
    "    print(f'score={score}')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        #Alogrithm 1: Search-based input generation \n",
    "        # comparator._compute_distance(inputs)\n",
    "        print(f'mutation {i}-th iteration')\n",
    "        # mutation_idx = random.randint(0, len(inputs))\n",
    "        # mutation = np.random.random_sample(size=input_shape).astype(np.float32)\n",
    "        \n",
    "        #무작위 위치 선택하여 mutation 생성\n",
    "        current_adv_inputs = gen_adv_inputs(model1, inputs, labels, model1_bit_config, attack_net=attack_net)\n",
    "        \n",
    "        current_score = evaluate_inputs(current_adv_inputs)\n",
    "        \n",
    "        \n",
    "        if current_score > score:\n",
    "            print(f'current score update: {score}->{current_score}')\n",
    "            max_inputs = current_adv_inputs\n",
    "            score = current_score\n",
    "            \n",
    "    \n",
    "    return max_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n"
     ]
    }
   ],
   "source": [
    "int8_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "int4_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "not_quantized_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "\n",
    "eight_bit_config = [8]*50\n",
    "attack_net = AttackPGD(not_quantized_model, epsilon=0.06, step_size=0.01, num_steps=50, bit_config=None)\n",
    "four_bit_config = [4]*50\n",
    "seed_images, seed_labels = get_seed_inputs(50, rand=False)\n",
    "adv_inputs = gen_adv_inputs(not_quantized_model, seed_images, seed_labels, bit_config=None, attack_net=attack_net)\n",
    "# mutation_inputs = gen_profiling_inputs_in_blackbox(not_quantized_model, None,  int4_model, four_bit_config, seed_images, epsilon=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutation_adv_inputs = gen_profiling_inputs_in_whitebox(not_quantized_model, None,  int4_model, four_bit_config, seed_images, seed_labels, epsilon=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating with real data...\n",
      "Calibrating with real data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (qact_input): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): QConv2d(\n",
       "      3, 192, kernel_size=(16, 16), stride=(16, 16)\n",
       "      (quantizer): UniformQuantizer()\n",
       "    )\n",
       "    (qact_before_norm): Identity()\n",
       "    (norm): Identity()\n",
       "    (qact): QAct(\n",
       "      (quantizer): UniformQuantizer()\n",
       "    )\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (qact_embed): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (qact_pos): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (qact1): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): QLinear(\n",
       "          in_features=192, out_features=576, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact0): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact2): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (proj): QLinear(\n",
       "          in_features=192, out_features=192, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact3): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact_attn1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (log_int_softmax): QIntSoftmax(\n",
       "          (quantizer): Log2Quantizer()\n",
       "        )\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (qact2): QAct(\n",
       "        (quantizer): UniformQuantizer()\n",
       "      )\n",
       "      (norm2): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (qact0): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (fc1): QLinear(\n",
       "          in_features=192, out_features=768, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "        (qact1): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (fc2): QLinear(\n",
       "          in_features=768, out_features=192, bias=True\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (qact2): QAct(\n",
       "          (quantizer): UniformQuantizer()\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (qact4): QAct(\n",
       "        (quantizer): UniformQuantizer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): QIntLayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (qact2): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (pre_logits): Identity()\n",
       "  (head): QLinear(\n",
       "    in_features=192, out_features=1000, bias=True\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       "  (act_out): QAct(\n",
       "    (quantizer): UniformQuantizer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "int8_model = calibrate_model(args.mode, args, int8_model, train_loader, device)\n",
    "int4_model = calibrate_model(args.mode, args, int4_model, train_loader, device)\n",
    "\n",
    "\n",
    "int8_model.eval()\n",
    "int4_model.eval()\n",
    "not_quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=None)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [8]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     # adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = not_quantized_model(adv_inputs, None, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(name, model_outputs):\n",
    "    def hook(module, input, output):\n",
    "        model_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "int8_outputs = {}\n",
    "int4_outputs = {}\n",
    "not_quantized_outputs = {}\n",
    "def add_hooks(model, model_outputs):\n",
    "    # Input quantization\n",
    "    model.qact_input.register_forward_hook(hook_fn(\"qact_input\", model_outputs))\n",
    "    \n",
    "    # Patch Embedding\n",
    "    model.patch_embed.register_forward_hook(hook_fn(\"patch_embed\", model_outputs))\n",
    "    model.patch_embed.qact.register_forward_hook(hook_fn(\"patch_embed_qact\", model_outputs))\n",
    "    \n",
    "    # Position Embedding\n",
    "    model.pos_drop.register_forward_hook(hook_fn(\"pos_drop\", model_outputs))\n",
    "    model.qact_embed.register_forward_hook(hook_fn(\"qact_embed\", model_outputs))\n",
    "    model.qact_pos.register_forward_hook(hook_fn(\"qact_pos\", model_outputs))\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        block.norm1.register_forward_hook(hook_fn(f\"block_{i}_norm1\", model_outputs))\n",
    "        block.attn.qkv.register_forward_hook(hook_fn(f\"block_{i}_attn_qkv\", model_outputs))\n",
    "        block.attn.proj.register_forward_hook(hook_fn(f\"block_{i}_attn_proj\", model_outputs))\n",
    "        block.attn.qact3.register_forward_hook(hook_fn(f\"block_{i}_attn_qact3\", model_outputs))\n",
    "        block.qact2.register_forward_hook(hook_fn(f\"block_{i}_qact2\", model_outputs))\n",
    "        block.norm2.register_forward_hook(hook_fn(f\"block_{i}_norm2\", model_outputs))\n",
    "        block.mlp.fc1.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc1\", model_outputs))\n",
    "        block.mlp.fc2.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc2\", model_outputs))\n",
    "        block.mlp.qact2.register_forward_hook(hook_fn(f\"block_{i}_mlp_qact2\", model_outputs))\n",
    "        block.qact4.register_forward_hook(hook_fn(f\"block_{i}_qact4\", model_outputs))\n",
    "    \n",
    "    # Final Norm Layer\n",
    "    model.norm.register_forward_hook(hook_fn(\"final_norm\", model_outputs))\n",
    "    model.qact2.register_forward_hook(hook_fn(\"final_qact2\", model_outputs))\n",
    "    \n",
    "    # Classifier Head\n",
    "    model.head.register_forward_hook(hook_fn(\"head\", model_outputs))\n",
    "    model.act_out.register_forward_hook(hook_fn(\"act_out\", model_outputs))\n",
    "    \n",
    "add_hooks(int8_model, int8_outputs)\n",
    "add_hooks(int4_model, int4_outputs)\n",
    "add_hooks(not_quantized_model, not_quantized_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv(model, normal_inputs, adv_inputs, outputs, bit_config = None):\n",
    "    if bit_config is not None:\n",
    "        model(normal_inputs, bit_config=bit_config, plot=False)    \n",
    "    else:\n",
    "        model(normal_inputs, plot=False)\n",
    "    normal_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    \n",
    "    # 적대적 입력에 대한 출력 저장\n",
    "    if bit_config is not None:\n",
    "        model(adv_inputs, bit_config=bit_config, plot=False)\n",
    "    else:\n",
    "        model(adv_inputs, plot=False)\n",
    "    adv_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    # print(normal_outputs.keys())\n",
    "    # print(adv_outputs.keys())\n",
    "\n",
    "    model_ddv_dict = {}\n",
    "    #dictionary int8_outputs을 모두 출력한다.\n",
    "    for key in normal_outputs.keys():\n",
    "    \n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        specific_layers_output_pairs = zip(normal_outputs[key], adv_outputs[key])\n",
    "    \n",
    "        ddv = []\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya.detach().cpu().numpy().flatten()\n",
    "            yb = yb.detach().cpu().numpy().flatten()\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb)\n",
    "            ddv.append(cos_similarity)\n",
    "        ddv = np.array(ddv)\n",
    "        norm = np.linalg.norm(ddv)\n",
    "        if norm != 0:\n",
    "            ddv = ddv/ norm\n",
    "        model_ddv_dict[key] = ddv\n",
    "        # print(key, \"레이어에서\", ddv.shape)\n",
    "    return model_ddv_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n",
    "not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        if not (key in target_ddv.keys()):\n",
    "            continue\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "            # norm = np.linalg.norm(cos_similarity)\n",
    "            # if norm != 0:\n",
    "                # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            # print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int8_bit_config = [8]*50\n",
    "# int4_bit_config = [4]*50\n",
    "# int8_ddv = compute_ddv(int8_model, seed_images, mutation_adv_inputs , int8_outputs, int8_bit_config)\n",
    "# int4_ddv = compute_ddv(int4_model, seed_images, mutation_adv_inputs, int4_outputs, int4_bit_config)\n",
    "# not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, mutation_adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "# def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "#     for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "#         #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "#         if not (key in target_ddv.keys()):\n",
    "#             continue\n",
    "#         specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "#         # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "#         for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "#             #ya와 yb의 cosiene similarity를 계산\n",
    "#             # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "#             ya = ya / np.linalg.norm(ya)\n",
    "#             yb = yb / np.linalg.norm(yb)\n",
    "#             cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "#             # norm = np.linalg.norm(cos_similarity)\n",
    "#             # if norm != 0:\n",
    "#                 # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "#             print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# # calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "# calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "# calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# # print(int8_ddv.keys())\n",
    "# # print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qact_input, \"레이어에서\", 100.00, 100.00\n",
      "patch_embed_qact, \"레이어에서\", 100.00, 100.00\n",
      "patch_embed, \"레이어에서\", 100.00, 100.00\n",
      "qact_embed, \"레이어에서\", 100.00, 100.00\n",
      "qact_pos, \"레이어에서\", 100.00, 100.00\n",
      "pos_drop, \"레이어에서\", 100.00, 100.00\n",
      "block_0_norm1, \"레이어에서\", 100.00, 100.00\n",
      "block_0_attn_proj, \"레이어에서\", 100.00, 100.00\n",
      "block_0_attn_qact3, \"레이어에서\", 100.00, 100.00\n",
      "block_0_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_norm2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_mlp_fc2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_mlp_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_qact4, \"레이어에서\", 100.00, 100.00\n",
      "block_1_norm1, \"레이어에서\", 100.00, 100.00\n",
      "block_1_attn_proj, \"레이어에서\", 100.00, 100.00\n",
      "block_1_attn_qact3, \"레이어에서\", 100.00, 100.00\n",
      "block_1_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_1_norm2, \"레이어에서\", 100.00, 100.00\n",
      "block_1_mlp_fc2, \"레이어에서\", 100.00, 99.99\n",
      "block_1_mlp_qact2, \"레이어에서\", 100.00, 99.99\n",
      "block_1_qact4, \"레이어에서\", 100.00, 100.00\n",
      "block_2_norm1, \"레이어에서\", 100.00, 99.99\n",
      "block_2_attn_proj, \"레이어에서\", 100.00, 99.99\n",
      "block_2_attn_qact3, \"레이어에서\", 100.00, 99.99\n",
      "block_2_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_2_norm2, \"레이어에서\", 100.00, 99.98\n",
      "block_2_mlp_fc2, \"레이어에서\", 100.00, 99.97\n",
      "block_2_mlp_qact2, \"레이어에서\", 100.00, 99.97\n",
      "block_2_qact4, \"레이어에서\", 100.00, 99.99\n",
      "block_3_norm1, \"레이어에서\", 100.00, 99.98\n",
      "block_3_attn_proj, \"레이어에서\", 100.00, 99.98\n",
      "block_3_attn_qact3, \"레이어에서\", 100.00, 99.98\n",
      "block_3_qact2, \"레이어에서\", 100.00, 99.99\n",
      "block_3_norm2, \"레이어에서\", 100.00, 99.97\n",
      "block_3_mlp_fc2, \"레이어에서\", 99.99, 99.96\n",
      "block_3_mlp_qact2, \"레이어에서\", 99.99, 99.96\n",
      "block_3_qact4, \"레이어에서\", 100.00, 99.98\n",
      "block_4_norm1, \"레이어에서\", 100.00, 99.97\n",
      "block_4_attn_proj, \"레이어에서\", 99.98, 99.93\n",
      "block_4_attn_qact3, \"레이어에서\", 99.98, 99.93\n",
      "block_4_qact2, \"레이어에서\", 99.99, 99.97\n",
      "block_4_norm2, \"레이어에서\", 99.99, 99.96\n",
      "block_4_mlp_fc2, \"레이어에서\", 99.99, 99.92\n",
      "block_4_mlp_qact2, \"레이어에서\", 99.99, 99.92\n",
      "block_4_qact4, \"레이어에서\", 99.99, 99.96\n",
      "block_5_norm1, \"레이어에서\", 99.99, 99.95\n",
      "block_5_attn_proj, \"레이어에서\", 99.98, 99.86\n",
      "block_5_attn_qact3, \"레이어에서\", 99.98, 99.85\n",
      "block_5_qact2, \"레이어에서\", 99.99, 99.95\n",
      "block_5_norm2, \"레이어에서\", 99.99, 99.95\n",
      "block_5_mlp_fc2, \"레이어에서\", 99.95, 99.74\n",
      "block_5_mlp_qact2, \"레이어에서\", 99.95, 99.74\n",
      "block_5_qact4, \"레이어에서\", 99.99, 99.93\n",
      "block_6_norm1, \"레이어에서\", 99.99, 99.92\n",
      "block_6_attn_proj, \"레이어에서\", 99.98, 99.82\n",
      "block_6_attn_qact3, \"레이어에서\", 99.98, 99.82\n",
      "block_6_qact2, \"레이어에서\", 99.99, 99.90\n",
      "block_6_norm2, \"레이어에서\", 99.99, 99.92\n",
      "block_6_mlp_fc2, \"레이어에서\", 99.96, 99.64\n",
      "block_6_mlp_qact2, \"레이어에서\", 99.96, 99.64\n",
      "block_6_qact4, \"레이어에서\", 99.98, 99.85\n",
      "block_7_norm1, \"레이어에서\", 99.98, 99.83\n",
      "block_7_attn_proj, \"레이어에서\", 99.96, 99.72\n",
      "block_7_attn_qact3, \"레이어에서\", 99.96, 99.72\n",
      "block_7_qact2, \"레이어에서\", 99.98, 99.84\n",
      "block_7_norm2, \"레이어에서\", 99.99, 99.87\n",
      "block_7_mlp_fc2, \"레이어에서\", 99.94, 99.40\n",
      "block_7_mlp_qact2, \"레이어에서\", 99.94, 99.40\n",
      "block_7_qact4, \"레이어에서\", 99.97, 99.74\n",
      "block_8_norm1, \"레이어에서\", 99.97, 99.68\n",
      "block_8_attn_proj, \"레이어에서\", 99.90, 99.29\n",
      "block_8_attn_qact3, \"레이어에서\", 99.90, 99.29\n",
      "block_8_qact2, \"레이어에서\", 99.97, 99.77\n",
      "block_8_norm2, \"레이어에서\", 99.98, 99.84\n",
      "block_8_mlp_fc2, \"레이어에서\", 99.86, 99.14\n",
      "block_8_mlp_qact2, \"레이어에서\", 99.86, 99.14\n",
      "block_8_qact4, \"레이어에서\", 99.95, 99.69\n",
      "block_9_norm1, \"레이어에서\", 99.94, 99.55\n",
      "block_9_attn_proj, \"레이어에서\", 99.86, 99.06\n",
      "block_9_attn_qact3, \"레이어에서\", 99.86, 99.06\n",
      "block_9_qact2, \"레이어에서\", 99.96, 99.75\n",
      "block_9_norm2, \"레이어에서\", 99.97, 99.80\n",
      "block_9_mlp_fc2, \"레이어에서\", 99.94, 99.54\n",
      "block_9_mlp_qact2, \"레이어에서\", 99.94, 99.54\n",
      "block_9_qact4, \"레이어에서\", 99.96, 99.73\n",
      "block_10_norm1, \"레이어에서\", 99.95, 99.55\n",
      "block_10_attn_proj, \"레이어에서\", 99.74, 98.45\n",
      "block_10_attn_qact3, \"레이어에서\", 99.74, 98.45\n",
      "block_10_qact2, \"레이어에서\", 99.97, 99.78\n",
      "block_10_norm2, \"레이어에서\", 99.97, 99.80\n",
      "block_10_mlp_fc2, \"레이어에서\", 99.82, 99.14\n",
      "block_10_mlp_qact2, \"레이어에서\", 99.81, 99.14\n",
      "block_10_qact4, \"레이어에서\", 99.96, 99.76\n",
      "block_11_norm1, \"레이어에서\", 99.95, 99.60\n",
      "block_11_attn_proj, \"레이어에서\", 97.35, 94.55\n",
      "block_11_attn_qact3, \"레이어에서\", 97.33, 94.56\n",
      "block_11_qact2, \"레이어에서\", 99.95, 99.71\n",
      "block_11_norm2, \"레이어에서\", 99.97, 99.78\n",
      "block_11_mlp_fc2, \"레이어에서\", 99.97, 99.88\n",
      "block_11_mlp_qact2, \"레이어에서\", 99.97, 99.88\n",
      "block_11_qact4, \"레이어에서\", 99.94, 99.68\n",
      "final_norm, \"레이어에서\", 99.95, 99.64\n",
      "final_qact2, \"레이어에서\", 88.77, 41.27\n",
      "head, \"레이어에서\", 88.42, 46.48\n",
      "act_out, \"레이어에서\", 88.44, 46.49\n"
     ]
    }
   ],
   "source": [
    "#int8div와 int4div의 value를 각각 조회해 array의 차이를 출력한다.\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)  # 두 벡터의 내적\n",
    "    norm_vec1 = np.linalg.norm(vec1)  # 첫 번째 벡터의 크기 (norm)\n",
    "    norm_vec2 = np.linalg.norm(vec2)  # 두 번째 벡터의 크기 (norm)\n",
    "    \n",
    "    return dot_product / (norm_vec1 * norm_vec2) \n",
    "\n",
    "\n",
    "for key in int8_ddv.keys():\n",
    "    if  (key in not_quantized_ddv.keys()):\n",
    "        int8_similarity = cosine_similarity(int8_ddv[key], not_quantized_ddv[key])\n",
    "        int4_similarity = cosine_similarity(int4_ddv[key], not_quantized_ddv[key])\n",
    "        int8_similarity = (int8_similarity + 1) /2 * 100\n",
    "        int4_similarity = (int4_similarity + 1) /2 * 100\n",
    "        print(f'{key}, \"레이어에서\", {int8_similarity:.2f}, {int4_similarity:.2f}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 절대적 변화율 계산\n",
    "        # print(np.abs(diff).mean() * 100)\n",
    "        #difference를 통해 mse를 구한다.\n",
    "        # mse = np.square(difference).mean()\n",
    "        # print(mse)\n",
    "        \n",
    "# print(\"int4_ddv - not_quantized_ddv\")\n",
    "#         print(np.array(int4_ddv[key] - not_quantized_ddv[key]).mean()/2 * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [8]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = int8_model(inputs, bit_config, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int8_model(inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [4]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int4_model(adv_inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newerFQ2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
