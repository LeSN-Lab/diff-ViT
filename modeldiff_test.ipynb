{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jieungkim/.conda/envs/ptq4vit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from config import Config\n",
    "from models import *\n",
    "from generate_data import generate_data\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "parser = argparse.ArgumentParser(description='FQ-ViT')\n",
    "\n",
    "parser.add_argument('--model',\n",
    "                    choices=[\n",
    "                        'deit_tiny', 'deit_small', 'deit_base', 'vit_base',\n",
    "                        'vit_large', 'swin_tiny', 'swin_small', 'swin_base'\n",
    "                    ],\n",
    "                    default='deit_tiny',\n",
    "                    help='model')\n",
    "parser.add_argument('--data', metavar='DIR',\n",
    "                    default='/home/jieungkim/quantctr/imagenet',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--quant', default=True, action='store_true')\n",
    "parser.add_argument('--ptf', default=False)\n",
    "parser.add_argument('--lis', default=False)\n",
    "parser.add_argument('--quant-method',\n",
    "                    default='minmax',\n",
    "                    choices=['minmax', 'ema', 'omse', 'percentile'])\n",
    "parser.add_argument('--mixed', default=False, action='store_true')\n",
    "# TODO: 100 --> 32\n",
    "parser.add_argument('--calib-batchsize',\n",
    "                    default=5,\n",
    "                    type=int,\n",
    "                    help='batchsize of calibration set')\n",
    "parser.add_argument(\"--mode\", default=0,\n",
    "                        type=int, \n",
    "                        help=\"mode of calibration data, 0: PSAQ-ViT, 1: Gaussian noise, 2: Real data\")\n",
    "# TODO: 10 --> 1\n",
    "parser.add_argument('--calib-iter', default=10, type=int)\n",
    "# TODO: 100 --> 200\n",
    "parser.add_argument('--val-batchsize',\n",
    "                    default=5,\n",
    "                    type=int,\n",
    "                    help='batchsize of validation set')\n",
    "parser.add_argument('--num-workers',\n",
    "                    default=16,\n",
    "                    type=int,\n",
    "                    help='number of data loading workers (default: 16)')\n",
    "parser.add_argument('--device', default='cuda', type=str, help='device')\n",
    "parser.add_argument('--print-freq',\n",
    "                    default=100,\n",
    "                    type=int,\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--seed', default=0, type=int, help='seed')\n",
    "\n",
    "\n",
    "def str2model(name):\n",
    "    d = {\n",
    "        'deit_tiny': deit_tiny_patch16_224,\n",
    "        'deit_small': deit_small_patch16_224,\n",
    "        'deit_base': deit_base_patch16_224,\n",
    "        'vit_base': vit_base_patch16_224,\n",
    "        'vit_large': vit_large_patch16_224,\n",
    "        'swin_tiny': swin_tiny_patch4_window7_224,\n",
    "        'swin_small': swin_small_patch4_window7_224,\n",
    "        'swin_base': swin_base_patch4_window7_224,\n",
    "    }\n",
    "    print('Model: %s' % d[name].__name__)\n",
    "    return d[name]\n",
    "\n",
    "\n",
    "def seed(seed=0):\n",
    "    import os\n",
    "    import random\n",
    "    import sys\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    sys.setrecursionlimit(100000)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def build_transform(input_size=224,\n",
    "                    interpolation='bicubic',\n",
    "                    mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225),\n",
    "                    crop_pct=0.875):\n",
    "\n",
    "    def _pil_interp(method):\n",
    "        if method == 'bicubic':\n",
    "            return Image.BICUBIC\n",
    "        elif method == 'lanczos':\n",
    "            return Image.LANCZOS\n",
    "        elif method == 'hamming':\n",
    "            return Image.HAMMING\n",
    "        else:\n",
    "            return Image.BILINEAR\n",
    "\n",
    "    resize_im = input_size > 32\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        size = int(math.floor(input_size / crop_pct))\n",
    "        ip = _pil_interp(interpolation)\n",
    "        t.append(\n",
    "            transforms.Resize(\n",
    "                size,\n",
    "                interpolation=ip),  # to maintain same ratio w.r.t. 224 images\n",
    "        )\n",
    "        t.append(transforms.CenterCrop(input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args, val_loader, model, criterion, device, bit_config=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    val_start_time = end = time.time()\n",
    "    for i, (data, target) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if i == 0:\n",
    "            plot_flag = False\n",
    "        else:\n",
    "            plot_flag = False\n",
    "        with torch.no_grad():\n",
    "            output, FLOPs, distance = model(data, bit_config, plot_flag)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data.item(), data.size(0))\n",
    "        top1.update(prec1.data.item(), data.size(0))\n",
    "        top5.update(prec5.data.item(), data.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      i,\n",
    "                      len(val_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      loss=losses,\n",
    "                      top1=top1,\n",
    "                      top5=top5,\n",
    "                  ))\n",
    "    val_end_time = time.time()\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "          format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_make(model_name, ptf, lis, quant_method, device):\n",
    "    device = torch.device(device)\n",
    "    cfg = Config(ptf, lis, quant_method)\n",
    "    model = str2model(model_name)(pretrained=True, cfg=cfg)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "    \n",
    "def calibrate_model(mode = 0, args = None, model = None, train_loader = None, device = None):\n",
    "    if mode == 2:\n",
    "        print(\"Generating data...\")\n",
    "        calibrate_data = generate_data(args)\n",
    "        print(\"Calibrating with generated data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 1: Gaussian noise\n",
    "    elif args.mode == 1:\n",
    "        calibrate_data = torch.randn((args.calib_batchsize, 3, 224, 224)).to(device)\n",
    "        print(\"Calibrating with Gaussian noise...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            model.model_open_last_calibrate()\n",
    "            output = model(calibrate_data)\n",
    "        return model\n",
    "    # Case 2: Real data (Standard)\n",
    "    elif args.mode == 0:\n",
    "        # Get calibration set.\n",
    "        image_list = []\n",
    "        # output_list = []\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            if i == args.calib_iter:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            # target = target.to(device)\n",
    "            image_list.append(data)\n",
    "            # output_list.append(target)\n",
    "\n",
    "        print(\"Calibrating with real data...\")\n",
    "        model.model_open_calibrate()\n",
    "        with torch.no_grad():\n",
    "            # TODO:\n",
    "            # for i, image in enumerate(image_list):\n",
    "            #     if i == len(image_list) - 1:\n",
    "            #         # This is used for OMSE method to\n",
    "            #         # calculate minimum quantization error\n",
    "            #         model.model_open_last_calibrate()\n",
    "            #     output, FLOPs, global_distance = model(image, plot=False)\n",
    "            # model.model_quant(flag='off')\n",
    "            model.model_open_last_calibrate()\n",
    "            output, FLOPs, global_distance = model(image_list[0], plot=False)\n",
    "\n",
    "    model.model_close_calibrate()\n",
    "    model.model_quant()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "seed(args.seed)\n",
    "\n",
    "device = torch.device(args.device)\n",
    "cfg = Config(args.ptf, args.lis, args.quant_method)\n",
    "# model = str2model(args.model)(pretrained=True, cfg=cfg)\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Note: Different models have different strategies of data preprocessing.\n",
    "model_type = args.model.split('_')[0]\n",
    "if model_type == 'deit':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.875\n",
    "elif model_type == 'vit':\n",
    "    mean = (0.5, 0.5, 0.5)\n",
    "    std = (0.5, 0.5, 0.5)\n",
    "    crop_pct = 0.9\n",
    "elif model_type == 'swin':\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    crop_pct = 0.9\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "val_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "# Data\n",
    "traindir = os.path.join(args.data, 'train')\n",
    "valdir = os.path.join(args.data, 'val')\n",
    "\n",
    "val_dataset = datasets.ImageFolder(valdir, val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.val_batchsize,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# switch to evaluate mode\n",
    "# model.eval()\n",
    "\n",
    "# define loss function (criterion)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss(yhat, y):\n",
    "\treturn -((yhat[:,0]-y[:,0])**2 + 0.1*((yhat[:,1:]-y[:,1:])**2).mean(1)).mean()\n",
    "\n",
    "class AttackPGD(nn.Module):\n",
    "    def __init__(self, basic_net, epsilon, step_size, num_steps, bit_config):\n",
    "        super(AttackPGD, self).__init__()\n",
    "        self.basic_net = basic_net\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.bit_config = bit_config\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        self.basic_net.zero_grad()\n",
    "        x = inputs.clone().detach()\n",
    "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
    "        for i in range(self.num_steps):\n",
    "            x = x.clone().detach().requires_grad_(True)\n",
    "            # with torch.enable_grad():\n",
    "            outputs, Flops, distance = self.basic_net(x, self.bit_config, False)\n",
    "            loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "            # loss = myloss(outputs, targets)\n",
    "            loss.backward()\n",
    "            # grad = torch.autograd.grad(loss, [x], create_graph=False)[0]\n",
    "            grad = x.grad.clone()\n",
    "            x = x + self.step_size*torch.sign(grad)\n",
    "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
    "            x = torch.clamp(x, inputs.min().item(), inputs.max().item())\n",
    "            # x = torch.clamp(x, 0, 1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.basic_net.eval()\n",
    "                adv_output, Flops, distance= self.basic_net(x, self.bit_config, False)\n",
    "            \n",
    "        return adv_output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_inputs(n, rand=False, input_shape = (3, 224, 224)):\n",
    "    if rand:\n",
    "        batch_input_size = (n, input_shape[0], input_shape[1], input_shape[2])\n",
    "        images = np.random.normal(size = batch_input_size).astype(np.float32)\n",
    "    else:\n",
    "        model_type = args.model.split('_')[0]\n",
    "        if model_type == 'deit':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.875\n",
    "        elif model_type == 'vit':\n",
    "            mean = (0.5, 0.5, 0.5)\n",
    "            std = (0.5, 0.5, 0.5)\n",
    "            crop_pct = 0.9\n",
    "        elif model_type == 'swin':\n",
    "            mean = (0.485, 0.456, 0.406)\n",
    "            std = (0.229, 0.224, 0.225)\n",
    "            crop_pct = 0.9\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "        # Data\n",
    "        traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=n,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        images, labels = next(iter(train_loader))\n",
    "    return images.cuda(), labels.cuda()\n",
    "        \n",
    "def get_dataset(n, input_shape = (3, 224, 224)):\n",
    "    \n",
    "    \n",
    "    model_type = args.model.split('_')[0]\n",
    "    if model_type == 'deit':\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        crop_pct = 0.875\n",
    "    elif model_type == 'vit':\n",
    "        mean = (0.5, 0.5, 0.5)\n",
    "        std = (0.5, 0.5, 0.5)\n",
    "        crop_pct = 0.9\n",
    "    elif model_type == 'swin':\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        crop_pct = 0.9\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    train_transform = build_transform(mean=mean, std=std, crop_pct=crop_pct)\n",
    "\n",
    "    # Data\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(traindir, train_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=n,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return train_loader\n",
    "        \n",
    "\n",
    "def gen_adv_inputs(model, inputs, labels, bit_config, attack_net):\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # bit_config = [8]*50\n",
    "    with torch.no_grad():\n",
    "        clean_output, FLOPs, distance = model(inputs, bit_config, plot=False)\n",
    "    # output_shape = clean_output.shape\n",
    "    # batch_size = output_shape[0]\n",
    "    # num_classes = output_shape[1]\n",
    "    \n",
    "    \"\"\"\n",
    "    다양성 최대화:\n",
    "    ModelDiff 논문의 4.2절에서는 생성된 입력의 다양성(diversity)을 강조합니다.\n",
    "    target_outputs = output_mean - clean_output를 사용함으로써, 각 입력이 평균 출력과 다르게 되도록 유도하고 있습니다.\n",
    "    이는 생성된 입력들이 서로 다른 특성을 가지도록 하는 데 도움이 됩니다.\n",
    "    \"\"\"\n",
    "    output_mean = clean_output.mean(axis = 0)\n",
    "    target_outputs = output_mean - clean_output\n",
    "    \"\"\"\n",
    "    결정 경계 탐색:\n",
    "    y = target_outputs * 1000에서 큰 스케일 팩터(1000)를 사용하는 것은,\n",
    "    모델의 결정 경계를 더 잘 탐색하기 위한 것으로 보입니다.\n",
    "    이는 논문의 Figure 3에서 설명하는 \"decision boundary\" 개념과 연관됩니다.\n",
    "    \"\"\"\n",
    "    y = target_outputs * 1000 \n",
    "    \n",
    "    adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "    # adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "    torch.cuda.empty_cache()\n",
    "    return adv_inputs.detach()\n",
    "\n",
    "def metrics_output_diversity(model, bit_config, inputs, use_torch=False):\n",
    "    # 논문의 4.2절에서 설명한 출력 다양성 메트릭 계산\n",
    "    outputs = model(inputs, bit_config, False)[0].detach().to('cpu').numpy()\n",
    "#         output_dists = []\n",
    "#         for i in range(0, len(outputs) - 1):\n",
    "#             for j in range(i + 1, len(outputs)):\n",
    "#                 output_dist = spatial.distance.euclidean(outputs[i], outputs[j])\n",
    "#                 output_dists.append(output_dist)\n",
    "#         diversity = sum(output_dists) / len(output_dists)\n",
    "    # cdist 함수는 두 집합 모든 쌍 사이의 거리를 유클리드 거리를 이용해서 계산함.\n",
    "    #outputs_dists는 모든 출력 쌍 사이의 거리를 담은 행렬.\n",
    "    output_dists = spatial.distance.cdist(list(outputs), list(outputs), metric='euclidean')\n",
    "    #계산된 모든 거리의 평균을 구함.\n",
    "    diversity = np.mean(output_dists)\n",
    "    return diversity\n",
    "\n",
    "def gen_profiling_inputs_in_blackbox(model1, model1_bit_config, model2, model2_bit_config, seed_inputs, use_torch=False, epsilon=0.2):\n",
    "    #논문의 4.2절에서 설명한 테스트 입력 생성 알고리즘 구현\n",
    "    input_shape = seed_inputs[0].shape\n",
    "    n_inputs = seed_inputs.shape[0]\n",
    "    max_iterations = 1000 #최대 반복 횟수 설정\n",
    "    # max_steps = 10 \n",
    "    \n",
    "    \n",
    "    ndims = np.prod(input_shape) #입력의 총 차원 수 계산\n",
    "#         mutate_positions = torch.randperm(ndims)\n",
    "\n",
    "        # Move seed_inputs to GPU if not already\n",
    "    if not seed_inputs.is_cuda:\n",
    "        seed_inputs = seed_inputs.cuda()\n",
    "        \n",
    "    # 초기 모델의 출력 계산\n",
    "    with torch.no_grad():\n",
    "        initial_outputs1 = model1(seed_inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        initial_outputs2 = model2(seed_inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "    \n",
    "    def evaluate_inputs(inputs):\n",
    "        #논문의 Equantion 1에서 설명한 score 함수 구현\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).cuda()\n",
    "        elif inputs.device.type != 'cuda':\n",
    "            inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "            outputs2 = model2(inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        \n",
    "        metrics1 = metrics_output_diversity(model1, model1_bit_config, inputs) #diversity 계산\n",
    "        metrics2 = metrics_output_diversity(model2, model2_bit_config, inputs) # diversity 계산\n",
    "\n",
    "\n",
    "        # divergence 계산 (초기 출력과의 거리)\n",
    "        output_dist1 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs1),\n",
    "            list(initial_outputs1),\n",
    "            metric='euclidean').diagonal())\n",
    "        output_dist2 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs2),\n",
    "            list(initial_outputs2),\n",
    "            metric='euclidean').diagonal())\n",
    "        print(f'  output distance: {output_dist1},{output_dist2}')\n",
    "        print(f'  metrics: {metrics1},{metrics2}')\n",
    "        # if mutated_metrics <= metrics:\n",
    "        #     break\n",
    "        \n",
    "        #score 계산 : divergence와 diversity의 곱\n",
    "        return output_dist1 * output_dist2 * metrics1 * metrics2\n",
    "    \n",
    "    inputs = seed_inputs\n",
    "    score = evaluate_inputs(inputs)\n",
    "    print(f'score={score}')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        #Alogrithm 1: Search-based input generation \n",
    "        # comparator._compute_distance(inputs)\n",
    "        print(f'mutation {i}-th iteration')\n",
    "        # mutation_idx = random.randint(0, len(inputs))\n",
    "        # mutation = np.random.random_sample(size=input_shape).astype(np.float32)\n",
    "        \n",
    "        #무작위 위치 선택하여 mutation 생성\n",
    "        mutation_pos = np.random.randint(0, ndims)\n",
    "        mutation = np.zeros(ndims).astype(np.float32)\n",
    "        mutation[mutation_pos] = epsilon\n",
    "        mutation = np.reshape(mutation, input_shape) #이 코드는 1차원 mutation 벡터를 원래 입력 데이터의 shape으로 재구성합니다.\n",
    "        \n",
    "        \n",
    "        \n",
    "        mutation_batch = torch.zeros_like(inputs)\n",
    "        mutation_idx = np.random.randint(0, n_inputs)\n",
    "        mutation_batch[mutation_idx] = torch.from_numpy(mutation).cuda()\n",
    "        \n",
    "        # print(f'{inputs.shape} {mutation_perturbation.shape}')\n",
    "        # for j in range(max_steps):\n",
    "            # mutated_inputs = np.clip(inputs + mutation, 0, 1)\n",
    "            # print(f'{list(inputs)[0].shape}')\n",
    "        mutate_right_inputs = inputs + mutation_batch\n",
    "        mutate_right_score = evaluate_inputs(mutate_right_inputs)\n",
    "        mutate_left_inputs = inputs - mutation_batch\n",
    "        mutate_left_score = evaluate_inputs(mutate_left_inputs)\n",
    "        \n",
    "        if mutate_right_score <= score and mutate_left_score <= score:\n",
    "            continue\n",
    "        if mutate_right_score > mutate_left_score:\n",
    "            print(f'mutate right: {score}->{mutate_right_score}')\n",
    "            inputs = mutate_right_inputs\n",
    "            score = mutate_right_score\n",
    "        else:\n",
    "            print(f'mutate left: {score}->{mutate_left_score}')\n",
    "            inputs = mutate_left_inputs\n",
    "            score = mutate_left_score\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def gen_profiling_inputs_in_whitebox(model1, model1_bit_config, model2, model2_bit_config, seed_inputs, seed_labels, use_torch=False, epsilon=0.2, whitebox_attack_net=None):\n",
    "    #논문의 4.2절에서 설명한 테스트 입력 생성 알고리즘 구현\n",
    "    input_shape = seed_inputs[0].shape\n",
    "    n_inputs = seed_inputs.shape[0]\n",
    "    max_iterations = 20 #최대 반복 횟수 설정\n",
    "    # max_steps = 10 \n",
    "    \n",
    "    \n",
    "    ndims = np.prod(input_shape) #입력의 총 차원 수 계산\n",
    "#         mutate_positions = torch.randperm(ndims)\n",
    "\n",
    "        # Move seed_inputs to GPU if not already\n",
    "    if not seed_inputs.is_cuda:\n",
    "        seed_inputs = seed_inputs.cuda()\n",
    "        \n",
    "    # 초기 모델의 출력 계산\n",
    "    with torch.no_grad():\n",
    "        initial_outputs1 = model1(seed_inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        initial_outputs2 = model2(seed_inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "    \n",
    "    def evaluate_inputs(inputs):\n",
    "        #논문의 Equantion 1에서 설명한 score 함수 구현\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = torch.from_numpy(inputs).cuda()     \n",
    "        elif inputs.device.type != 'cuda':\n",
    "            inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs1 = model1(inputs, bit_config = model1_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "            outputs2 = model2(inputs, bit_config = model2_bit_config, plot=False)[0].detach().to('cpu').numpy()\n",
    "        \n",
    "        metrics1 = metrics_output_diversity(model1, model1_bit_config, inputs) #diversity 계산\n",
    "        metrics2 = metrics_output_diversity(model2, model2_bit_config, inputs) # diversity 계산\n",
    "\n",
    "\n",
    "        # divergence 계산 (초기 출력과의 거리)\n",
    "        output_dist1 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs1),\n",
    "            list(initial_outputs1),\n",
    "            metric='euclidean').diagonal())\n",
    "        output_dist2 = np.mean(spatial.distance.cdist(\n",
    "            list(outputs2),\n",
    "            list(initial_outputs2),\n",
    "            metric='euclidean').diagonal())\n",
    "        print(f'  output distance: {output_dist1},{output_dist2}')\n",
    "        print(f'  metrics: {metrics1},{metrics2}')\n",
    "        # if mutated_metrics <= metrics:\n",
    "        #     break\n",
    "        \n",
    "        #score 계산 : divergence와 diversity의 곱\n",
    "        return output_dist1 * output_dist2 * metrics1 * metrics2\n",
    "    \n",
    "    inputs = seed_inputs\n",
    "    max_inputs = None\n",
    "    labels = seed_labels\n",
    "    score = evaluate_inputs(inputs)\n",
    "    print(f'score={score}')\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        #Alogrithm 1: Search-based input generation \n",
    "        # comparator._compute_distance(inputs)\n",
    "        print(f'mutation {i}-th iteration')\n",
    "        # mutation_idx = random.randint(0, len(inputs))\n",
    "        # mutation = np.random.random_sample(size=input_shape).astype(np.float32)\n",
    "        \n",
    "        #무작위 위치 선택하여 mutation 생성\n",
    "        current_adv_inputs = gen_adv_inputs(model1, inputs, labels, model1_bit_config, attack_net=whitebox_attack_net)\n",
    "        \n",
    "        current_score = evaluate_inputs(current_adv_inputs)\n",
    "        \n",
    "        \n",
    "        if current_score > score:\n",
    "            print(f'current score update: {score}->{current_score}')\n",
    "            max_inputs = current_adv_inputs\n",
    "            score = current_score\n",
    "            \n",
    "    \n",
    "    return max_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n",
      "Model: deit_tiny_patch16_224\n"
     ]
    }
   ],
   "source": [
    "int8_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "int4_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "not_quantized_model = model_make(args.model, args.ptf, args.lis, args.quant_method, args.device)\n",
    "\n",
    "eight_bit_config = [8]*50\n",
    "not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.06, step_size=0.01, num_steps=50, bit_config=None)\n",
    "four_bit_config = [4]*50\n",
    "seed_images, seed_labels = get_seed_inputs(50, rand=False)\n",
    "adv_inputs = gen_adv_inputs(not_quantized_model, seed_images, seed_labels, bit_config=None, attack_net=not_quantized_attack_net)\n",
    "# mutation_inputs = gen_profiling_inputs_in_blackbox(not_quantized_model, None,  int4_model, four_bit_config, seed_images, epsilon=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutation_adv_inputs = gen_profiling_inputs_in_whitebox(not_quantized_model, None,  int4_model, four_bit_config, seed_images, seed_labels, epsilon=0.02, whitebox_attack_net=not_quantized_attack_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# int8_model = calibrate_model(args.mode, args, int8_model, train_loader, device)\n",
    "# int4_model = calibrate_model(args.mode, args, int4_model, train_loader, device)\n",
    "\n",
    "\n",
    "int8_model.eval()\n",
    "int4_model.eval()\n",
    "not_quantized_model.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cka 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_activations(act):\n",
    "    # 입력 텐서를 2D로 재구성합니다. 첫 번째 차원은 유지하고 나머지는 평탄화합니다.\n",
    "    act = act.view(act.size(0), -1)\n",
    "\n",
    "    # 각 샘플(행)에 대해 L2 norm을 계산합니다.\n",
    "    act_norm = torch.norm(act, p=2, dim=1, keepdim=True)\n",
    "\n",
    "    # 0으로 나누는 것을 방지하기 위해 작은 값을 더합니다.\n",
    "    act_norm = act_norm + 1e-8\n",
    "\n",
    "    # 각 샘플을 해당 norm으로 나누어 정규화합니다.\n",
    "    act = act / act_norm\n",
    "\n",
    "    return act\n",
    "#torch model의 layers의 수를 확인한다.\n",
    "from efficient_CKA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(images, model, normalize_act=False):\n",
    "    model = not_quantized_model\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def get_module_path(module):\n",
    "        return f\"{module.__class__.__module__}.{module.__class__.__name__}\"\n",
    "\n",
    "    activations = []\n",
    "    layer_info = []\n",
    "    from models.vit_fquant import Attention, Mlp\n",
    "    def hook_return(index):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(module, Attention):\n",
    "                activations.append(module.qkv_output)\n",
    "                layer_info.append({\n",
    "                'relative_index': len(activations) - 1,\n",
    "                'absolute_index': index,\n",
    "                'name': module.__class__.__name__,\n",
    "                'layer_type': type(module),\n",
    "                'path': get_module_path(module)\n",
    "\n",
    "                })\n",
    "            elif isinstance(module, Mlp):\n",
    "                activations.append(module.fc1_output)\n",
    "                layer_info.append({\n",
    "                'relative_index': len(activations) - 1,\n",
    "                'absolute_index': index,\n",
    "                'name': module.__class__.__name__,\n",
    "                'layer_type': type(module),\n",
    "                'path': get_module_path(module)\n",
    "\n",
    "                })\n",
    "            else:    \n",
    "                activations.append(output)\n",
    "                layer_info.append({\n",
    "                    'relative_index': len(activations) - 1,\n",
    "                    'absolute_index': index,\n",
    "                    'name': module.__class__.__name__,\n",
    "                    'layer_type': type(module),\n",
    "                    'path': get_module_path(module)\n",
    "\n",
    "                })\n",
    "            \n",
    "\n",
    "        return hook\n",
    "\n",
    "    hooks = []\n",
    "\n",
    "\n",
    "    for index, layer in enumerate(model.modules()):\n",
    "        if type(layer) in [QConv2d, QLinear, Attention, Mlp]:\n",
    "            hooks.append(layer.register_forward_hook(hook_return(index)))\n",
    "\n",
    "    # 모델을 통해 이미지를 전달합니다.\n",
    "    images = images.cuda()\n",
    "    _ = model(images)\n",
    "\n",
    "    # 등록된 후크를 제거합니다.\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # layer_info와 activations를 절대 인덱스를 기준으로 정렬\n",
    "    sorted_indices = sorted(range(len(layer_info)), key=lambda k: layer_info[k]['absolute_index'])\n",
    "    layer_info = [layer_info[i] for i in sorted_indices]\n",
    "    activations = [activations[i] for i in sorted_indices]\n",
    "\n",
    "    # 상대 인덱스 재할당\n",
    "    for i, info in enumerate(layer_info):\n",
    "        info['relative_index'] = i\n",
    "\n",
    "\n",
    "    if normalize_act:\n",
    "        activations = [normalize_activations(act) for act in activations]\n",
    "    return activations\n",
    "    # 정렬된 레이어 정보 출력\n",
    "    # for info in layer_info:\n",
    "    #     print(f\"Layer {info['relative_index']}(absolute: {info['absolute_index']}): {info['name']} (Type: {info['layer_type']}, Path: {info['path']})\")\n",
    "\n",
    "    # print(f\"\\nTotal number of activations: {len(activations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def compute_cka_internal(model, use_batch = True,\n",
    "                         use_train_mode = False,\n",
    "                         normalize_act = False,\n",
    "                         cka_batch = 50,\n",
    "                         cka_batch_iter = 10,\n",
    "                         cka_iter = 10):\n",
    "    model.eval()\n",
    "\n",
    "    sample_cka_dataset = get_dataset(cka_batch)\n",
    "\n",
    "    sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "    sample_images, _ = sample_cka_dataset\n",
    "    # n_layers = len(list(not_quantized_model.children()))\n",
    "    # n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "    sample_activations = get_activations(sample_images, model)\n",
    "    n_layers = len(sample_activations)\n",
    "\n",
    "    cka = MinibatchCKA(n_layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 사용 예시:\n",
    "    # model = YourModel()  # PyTorch 모델을 정의하세요\n",
    "    # images = torch.randn(10, 3, 224, 224)  # 예시 입력 이미지\n",
    "    # activations = get_activations(images, model, normalize_act=True)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    if use_batch:\n",
    "        for index in range(cka_iter):\n",
    "            #cka_batch만큼, shuffle해서, 데이터셋을 가져온다.\n",
    "            cka_dataset = get_dataset(cka_batch)\n",
    "            current_iter = 0\n",
    "            for images, _ in cka_dataset:\n",
    "                model_get_activation = get_activations(images, model, normalize_act) #각 모델의 레이어별 활성화를 가져온다.\n",
    "\n",
    "                cka.update_state(model_get_activation) #레이어 마다의 activation을 다 가져옴. 예를 들어 24 * 50 * feature^2. \n",
    "                \n",
    "                if current_iter > cka_batch_iter:\n",
    "                    break\n",
    "                current_iter += 1\n",
    "            print(\"현재 반복:\", index)\n",
    "    else:\n",
    "        cka_dataset = get_dataset(cka_batch)\n",
    "        all_images = []\n",
    "        for images, _ in cka_dataset:\n",
    "            all_images.append(images)\n",
    "        cka.update_state(get_activations(all_images, model, normalize_act))\n",
    "    heatmap = cka.result().cpu().numpy()\n",
    "    with open('cka_result.pkl', 'wb') as f:\n",
    "        #pickle로 heatmap을 저장한다.\n",
    "        pickle.dump(heatmap, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "sample_cka_dataset = get_dataset(50)\n",
    "\n",
    "sample_cka_dataset = next(iter(sample_cka_dataset))\n",
    "\n",
    "sample_images, _ = sample_cka_dataset\n",
    "# n_layers = len(list(not_quantized_model.children()))\n",
    "# n_layers = len([layer for layer in model.modules() if isinstance(layer, (nn.Conv2d, nn.Linear))])\n",
    "\n",
    "sample_activations = get_activations(sample_images, not_quantized_model)\n",
    "n_layers = len(sample_activations)\n",
    "\n",
    "print(n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 반복: 0\n",
      "현재 반복: 1\n",
      "현재 반복: 2\n",
      "현재 반복: 3\n",
      "현재 반복: 4\n",
      "현재 반복: 5\n",
      "현재 반복: 6\n",
      "현재 반복: 7\n",
      "현재 반복: 8\n",
      "현재 반복: 9\n",
      "현재 반복: 10\n",
      "현재 반복: 11\n",
      "현재 반복: 12\n",
      "현재 반복: 13\n",
      "현재 반복: 14\n",
      "현재 반복: 15\n",
      "현재 반복: 16\n",
      "현재 반복: 17\n",
      "현재 반복: 18\n",
      "현재 반복: 19\n"
     ]
    }
   ],
   "source": [
    "compute_cka_internal(not_quantized_model, use_batch = True, normalize_act = False, cka_batch = 10, cka_iter = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKA Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers [4, 21, 29, 46, 51, 61, 69, 86, 91, 101, 109, 126, 131, 141, 149, 166, 171, 181, 189, 206, 211, 221, 229, 246, 251, 261, 269, 286, 291, 301, 309, 326, 331, 341, 349, 366, 371, 381, 389, 406, 411, 421, 429, 446, 451, 461, 469, 486, 491, 502]\n",
      "x | y : 1 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from plot import *\n",
    "\n",
    "\n",
    "base_dir = '/home/jieungkim/quantctr/diff-ViT'\n",
    "\n",
    "\n",
    "\n",
    "# GPU 설정\n",
    "\n",
    "\n",
    "# CKA 결과 파일 경로 설정\n",
    "cka_dir = os.path.join(base_dir, 'cka_result.pkl')\n",
    "best_model = not_quantized_model\n",
    "# 저장된 모델 불러오기\n",
    "# best_model = torch.load(os.path.join(base_dir, 'model.pth'))\n",
    "\n",
    "# 'add' 연산이 포함된 레이어의 인덱스 찾기\n",
    "layers = []\n",
    "best_model.eval()\n",
    "# print(\"model.modules.length\", len([module for module in best_model.modules()])) // 506\n",
    "with torch.no_grad():\n",
    "    for i, module in enumerate(best_model.modules()):\n",
    "        # print(module.__class__.__name__.lower())\n",
    "        # if isinstance(module, torch.nn.Module) and 'mlp' in module.__class__.__name__.lower():\n",
    "        if type(module) in [QConv2d, QLinear]:\n",
    "            layers.append(i)\n",
    "print(\"layers\", layers)\n",
    "\n",
    "# CKA 결과 불러오기\n",
    "with open(cka_dir, 'rb') as f:\n",
    "    cka = pickle.load(f)\n",
    "\n",
    "# 특정 레이어에 대한 CKA 결과 추출\n",
    "# cka1 = cka[layers][:, layers]\n",
    "# print(cka.shape, cka1.shape)\n",
    "\n",
    "# 추출된 CKA 결과 저장\n",
    "out_dir = os.path.join(base_dir, 'cka_within_unquantized_model.pkl')\n",
    "# with open(out_dir, 'wb') as f:\n",
    "#     pickle.dump(cka1, f)\n",
    "\n",
    "# 전체 레이어에 대한 CKA 결과 플롯 생성\n",
    "plot_dir = os.path.join(base_dir, 'layer')\n",
    "plot_ckalist_resume([cka], plot_dir)\n",
    "\n",
    "# 특정 블록에 대한 CKA 결과 플롯 생성\n",
    "# plot_dir = os.path.join(base_dir, 'block')\n",
    "# plot_ckalist_resume([cka1], plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=None)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     eight_bit_config = [8]*50\n",
    "#     four_bit_config = [4] * 50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     # adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, labels)\n",
    "\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int4_model(adv_inputs, four_bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(name, model_outputs):\n",
    "    def hook(module, input, output):\n",
    "        model_outputs[name] = output\n",
    "    return hook\n",
    "\n",
    "int8_outputs = {}\n",
    "int4_outputs = {}\n",
    "not_quantized_outputs = {}\n",
    "def add_hooks(model, model_outputs):\n",
    "    # Input quantization\n",
    "    model.qact_input.register_forward_hook(hook_fn(\"qact_input\", model_outputs))\n",
    "    \n",
    "    # Patch Embedding\n",
    "    model.patch_embed.register_forward_hook(hook_fn(\"patch_embed\", model_outputs))\n",
    "    model.patch_embed.qact.register_forward_hook(hook_fn(\"patch_embed_qact\", model_outputs))\n",
    "    \n",
    "    # Position Embedding\n",
    "    model.pos_drop.register_forward_hook(hook_fn(\"pos_drop\", model_outputs))\n",
    "    model.qact_embed.register_forward_hook(hook_fn(\"qact_embed\", model_outputs))\n",
    "    model.qact_pos.register_forward_hook(hook_fn(\"qact_pos\", model_outputs))\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        block.norm1.register_forward_hook(hook_fn(f\"block_{i}_norm1\", model_outputs))\n",
    "        block.attn.qkv.register_forward_hook(hook_fn(f\"block_{i}_attn_qkv\", model_outputs))\n",
    "        block.attn.proj.register_forward_hook(hook_fn(f\"block_{i}_attn_proj\", model_outputs))\n",
    "        block.attn.qact3.register_forward_hook(hook_fn(f\"block_{i}_attn_qact3\", model_outputs))\n",
    "        block.qact2.register_forward_hook(hook_fn(f\"block_{i}_qact2\", model_outputs))\n",
    "        block.norm2.register_forward_hook(hook_fn(f\"block_{i}_norm2\", model_outputs))\n",
    "        block.mlp.fc1.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc1\", model_outputs))\n",
    "        block.mlp.fc2.register_forward_hook(hook_fn(f\"block_{i}_mlp_fc2\", model_outputs))\n",
    "        block.mlp.qact2.register_forward_hook(hook_fn(f\"block_{i}_mlp_qact2\", model_outputs))\n",
    "        block.qact4.register_forward_hook(hook_fn(f\"block_{i}_qact4\", model_outputs))\n",
    "    \n",
    "    # Final Norm Layer\n",
    "    model.norm.register_forward_hook(hook_fn(\"final_norm\", model_outputs))\n",
    "    model.qact2.register_forward_hook(hook_fn(\"final_qact2\", model_outputs))\n",
    "    \n",
    "    # Classifier Head\n",
    "    model.head.register_forward_hook(hook_fn(\"head\", model_outputs))\n",
    "    model.act_out.register_forward_hook(hook_fn(\"act_out\", model_outputs))\n",
    "    \n",
    "add_hooks(int8_model, int8_outputs)\n",
    "add_hooks(int4_model, int4_outputs)\n",
    "add_hooks(not_quantized_model, not_quantized_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ddv(model, normal_inputs, adv_inputs, outputs, bit_config = None):\n",
    "    if bit_config is not None:\n",
    "        model(normal_inputs, bit_config=bit_config, plot=False)    \n",
    "    else:\n",
    "        model(normal_inputs, plot=False)\n",
    "    normal_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    \n",
    "    # 적대적 입력에 대한 출력 저장\n",
    "    if bit_config is not None:\n",
    "        model(adv_inputs, bit_config=bit_config, plot=False)\n",
    "    else:\n",
    "        model(adv_inputs, plot=False)\n",
    "    adv_outputs = {k: v.clone() for k, v in outputs.items()}\n",
    "    \n",
    "    # print(normal_outputs.keys())\n",
    "    # print(adv_outputs.keys())\n",
    "\n",
    "    model_ddv_dict = {}\n",
    "    #dictionary int8_outputs을 모두 출력한다.\n",
    "    for key in normal_outputs.keys():\n",
    "    \n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        specific_layers_output_pairs = zip(normal_outputs[key], adv_outputs[key])\n",
    "    \n",
    "        ddv = []\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya.detach().cpu().numpy().flatten()\n",
    "            yb = yb.detach().cpu().numpy().flatten()\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb)\n",
    "            ddv.append(cos_similarity)\n",
    "        ddv = np.array(ddv)\n",
    "        norm = np.linalg.norm(ddv)\n",
    "        if norm != 0:\n",
    "            ddv = ddv/ norm\n",
    "        model_ddv_dict[key] = ddv\n",
    "        # print(key, \"레이어에서\", ddv.shape)\n",
    "    return model_ddv_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.66 GiB of which 5.69 MiB is free. Process 224057 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 6.32 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 859.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m int8_bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      2\u001b[0m int4_bit_config \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 3\u001b[0m int8_ddv \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ddv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint8_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint8_bit_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m int4_ddv \u001b[38;5;241m=\u001b[39m compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n\u001b[1;32m      5\u001b[0m not_quantized_ddv \u001b[38;5;241m=\u001b[39m compute_ddv(not_quantized_model, seed_images, adv_inputs, not_quantized_outputs)\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mcompute_ddv\u001b[0;34m(model, normal_inputs, adv_inputs, outputs, bit_config)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     model(adv_inputs, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m adv_outputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(normal_outputs.keys())\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(adv_outputs.keys())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_ddv_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     model(adv_inputs, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m adv_outputs \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(normal_outputs.keys())\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(adv_outputs.keys())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_ddv_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.66 GiB of which 5.69 MiB is free. Process 224057 has 1.30 GiB memory in use. Including non-PyTorch memory, this process has 6.32 GiB memory in use. Of the allocated memory 5.31 GiB is allocated by PyTorch, and 859.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, adv_inputs, int4_outputs, int4_bit_config)\n",
    "not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        if not (key in target_ddv.keys()):\n",
    "            continue\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "            # norm = np.linalg.norm(cos_similarity)\n",
    "            # if norm != 0:\n",
    "                # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            # print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_bit_config = [8]*50\n",
    "int4_bit_config = [4]*50\n",
    "int8_ddv = compute_ddv(int8_model, seed_images, mutation_adv_inputs, int8_outputs, int8_bit_config)\n",
    "int4_ddv = compute_ddv(int4_model, seed_images, mutation_adv_inputs, int4_outputs, int4_bit_config)\n",
    "not_quantized_ddv = compute_ddv(not_quantized_model, seed_images, mutation_adv_inputs, not_quantized_outputs)\n",
    "\n",
    "    \n",
    "def calculate_and_print_similarities(source_ddv, target_ddv):\n",
    "    for key in source_ddv.keys():\n",
    "    \n",
    "\n",
    "        #각 레이어에 대해 normal_outputs와 adv_outputs을 묶은 batch size만큼의 output을 저장한다.\n",
    "        if not (key in target_ddv.keys()):\n",
    "            continue\n",
    "        specific_layers_output_pairs = zip(source_ddv[key], target_ddv[key])\n",
    "\n",
    "\n",
    "        # 각 레이어에서 각각의 이미지에 대해 normal output과 adv output의 cosine similarity를 계산한다.\n",
    "        for i, (ya, yb) in enumerate(specific_layers_output_pairs):\n",
    "            #ya와 yb의 cosiene similarity를 계산\n",
    "            # dist = spatial.distance.cosine(ya, yb) -> 대체\n",
    "            ya = ya / np.linalg.norm(ya)\n",
    "            yb = yb / np.linalg.norm(yb)\n",
    "            cos_similarity = np.dot(ya, yb) * 100\n",
    "            \n",
    "        \n",
    "            # norm = np.linalg.norm(cos_similarity)\n",
    "            # if norm != 0:\n",
    "                # cos_similarity = cos_similarity/ norm\n",
    "            \n",
    "            # print(key, \"레이어에서\", cos_similarity)\n",
    "\n",
    "# calculate_and_print_similarities(int8_ddv, int4_ddv)\n",
    "calculate_and_print_similarities(int8_ddv, not_quantized_ddv)\n",
    "calculate_and_print_similarities(int4_ddv, not_quantized_ddv)\n",
    "# print(int8_ddv.keys())\n",
    "# print(int4_outputs.keys())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qact_input, \"레이어에서\", 100.00, 100.00\n",
      "patch_embed_qact, \"레이어에서\", 100.00, 100.00\n",
      "patch_embed, \"레이어에서\", 100.00, 100.00\n",
      "qact_embed, \"레이어에서\", 100.00, 100.00\n",
      "qact_pos, \"레이어에서\", 100.00, 100.00\n",
      "pos_drop, \"레이어에서\", 100.00, 100.00\n",
      "block_0_norm1, \"레이어에서\", 100.00, 100.00\n",
      "block_0_attn_proj, \"레이어에서\", 100.00, 100.00\n",
      "block_0_attn_qact3, \"레이어에서\", 100.00, 100.00\n",
      "block_0_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_norm2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_mlp_fc2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_mlp_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_0_qact4, \"레이어에서\", 100.00, 100.00\n",
      "block_1_norm1, \"레이어에서\", 100.00, 100.00\n",
      "block_1_attn_proj, \"레이어에서\", 100.00, 100.00\n",
      "block_1_attn_qact3, \"레이어에서\", 100.00, 100.00\n",
      "block_1_qact2, \"레이어에서\", 100.00, 100.00\n",
      "block_1_norm2, \"레이어에서\", 100.00, 100.00\n",
      "block_1_mlp_fc2, \"레이어에서\", 99.99, 99.99\n",
      "block_1_mlp_qact2, \"레이어에서\", 99.99, 99.99\n",
      "block_1_qact4, \"레이어에서\", 100.00, 100.00\n",
      "block_2_norm1, \"레이어에서\", 100.00, 99.99\n",
      "block_2_attn_proj, \"레이어에서\", 99.99, 99.99\n",
      "block_2_attn_qact3, \"레이어에서\", 99.99, 99.99\n",
      "block_2_qact2, \"레이어에서\", 100.00, 99.99\n",
      "block_2_norm2, \"레이어에서\", 100.00, 99.98\n",
      "block_2_mlp_fc2, \"레이어에서\", 99.99, 99.96\n",
      "block_2_mlp_qact2, \"레이어에서\", 99.99, 99.96\n",
      "block_2_qact4, \"레이어에서\", 100.00, 99.99\n",
      "block_3_norm1, \"레이어에서\", 99.99, 99.98\n",
      "block_3_attn_proj, \"레이어에서\", 99.98, 99.97\n",
      "block_3_attn_qact3, \"레이어에서\", 99.98, 99.97\n",
      "block_3_qact2, \"레이어에서\", 99.99, 99.98\n",
      "block_3_norm2, \"레이어에서\", 99.99, 99.97\n",
      "block_3_mlp_fc2, \"레이어에서\", 99.98, 99.96\n",
      "block_3_mlp_qact2, \"레이어에서\", 99.98, 99.96\n",
      "block_3_qact4, \"레이어에서\", 99.99, 99.98\n",
      "block_4_norm1, \"레이어에서\", 99.99, 99.97\n",
      "block_4_attn_proj, \"레이어에서\", 99.92, 99.89\n",
      "block_4_attn_qact3, \"레이어에서\", 99.92, 99.89\n",
      "block_4_qact2, \"레이어에서\", 99.97, 99.95\n",
      "block_4_norm2, \"레이어에서\", 99.96, 99.93\n",
      "block_4_mlp_fc2, \"레이어에서\", 99.93, 99.88\n",
      "block_4_mlp_qact2, \"레이어에서\", 99.93, 99.88\n",
      "block_4_qact4, \"레이어에서\", 99.97, 99.93\n",
      "block_5_norm1, \"레이어에서\", 99.97, 99.92\n",
      "block_5_attn_proj, \"레이어에서\", 99.89, 99.81\n",
      "block_5_attn_qact3, \"레이어에서\", 99.89, 99.81\n",
      "block_5_qact2, \"레이어에서\", 99.96, 99.92\n",
      "block_5_norm2, \"레이어에서\", 99.96, 99.92\n",
      "block_5_mlp_fc2, \"레이어에서\", 99.81, 99.68\n",
      "block_5_mlp_qact2, \"레이어에서\", 99.81, 99.68\n",
      "block_5_qact4, \"레이어에서\", 99.95, 99.90\n",
      "block_6_norm1, \"레이어에서\", 99.95, 99.89\n",
      "block_6_attn_proj, \"레이어에서\", 99.87, 99.77\n",
      "block_6_attn_qact3, \"레이어에서\", 99.87, 99.77\n",
      "block_6_qact2, \"레이어에서\", 99.93, 99.87\n",
      "block_6_norm2, \"레이어에서\", 99.94, 99.88\n",
      "block_6_mlp_fc2, \"레이어에서\", 99.78, 99.51\n",
      "block_6_mlp_qact2, \"레이어에서\", 99.78, 99.51\n",
      "block_6_qact4, \"레이어에서\", 99.91, 99.82\n",
      "block_7_norm1, \"레이어에서\", 99.91, 99.78\n",
      "block_7_attn_proj, \"레이어에서\", 99.83, 99.65\n",
      "block_7_attn_qact3, \"레이어에서\", 99.83, 99.65\n",
      "block_7_qact2, \"레이어에서\", 99.90, 99.80\n",
      "block_7_norm2, \"레이어에서\", 99.92, 99.82\n",
      "block_7_mlp_fc2, \"레이어에서\", 99.67, 99.26\n",
      "block_7_mlp_qact2, \"레이어에서\", 99.67, 99.26\n",
      "block_7_qact4, \"레이어에서\", 99.84, 99.69\n",
      "block_8_norm1, \"레이어에서\", 99.82, 99.60\n",
      "block_8_attn_proj, \"레이어에서\", 99.63, 99.16\n",
      "block_8_attn_qact3, \"레이어에서\", 99.63, 99.16\n",
      "block_8_qact2, \"레이어에서\", 99.84, 99.70\n",
      "block_8_norm2, \"레이어에서\", 99.90, 99.79\n",
      "block_8_mlp_fc2, \"레이어에서\", 99.48, 98.93\n",
      "block_8_mlp_qact2, \"레이어에서\", 99.48, 98.94\n",
      "block_8_qact4, \"레이어에서\", 99.78, 99.60\n",
      "block_9_norm1, \"레이어에서\", 99.71, 99.42\n",
      "block_9_attn_proj, \"레이어에서\", 99.51, 98.77\n",
      "block_9_attn_qact3, \"레이어에서\", 99.51, 98.77\n",
      "block_9_qact2, \"레이어에서\", 99.81, 99.65\n",
      "block_9_norm2, \"레이어에서\", 99.85, 99.71\n",
      "block_9_mlp_fc2, \"레이어에서\", 99.65, 99.33\n",
      "block_9_mlp_qact2, \"레이어에서\", 99.65, 99.33\n",
      "block_9_qact4, \"레이어에서\", 99.79, 99.62\n",
      "block_10_norm1, \"레이어에서\", 99.69, 99.36\n",
      "block_10_attn_proj, \"레이어에서\", 99.16, 98.15\n",
      "block_10_attn_qact3, \"레이어에서\", 99.16, 98.15\n",
      "block_10_qact2, \"레이어에서\", 99.82, 99.68\n",
      "block_10_norm2, \"레이어에서\", 99.85, 99.71\n",
      "block_10_mlp_fc2, \"레이어에서\", 99.40, 98.95\n",
      "block_10_mlp_qact2, \"레이어에서\", 99.41, 98.95\n",
      "block_10_qact4, \"레이어에서\", 99.79, 99.65\n",
      "block_11_norm1, \"레이어에서\", 99.70, 99.43\n",
      "block_11_attn_proj, \"레이어에서\", 96.04, 93.47\n",
      "block_11_attn_qact3, \"레이어에서\", 96.06, 93.47\n",
      "block_11_qact2, \"레이어에서\", 99.72, 99.57\n",
      "block_11_norm2, \"레이어에서\", 99.83, 99.68\n",
      "block_11_mlp_fc2, \"레이어에서\", 99.91, 99.83\n",
      "block_11_mlp_qact2, \"레이어에서\", 99.91, 99.83\n",
      "block_11_qact4, \"레이어에서\", 99.69, 99.51\n",
      "final_norm, \"레이어에서\", 99.75, 99.51\n",
      "final_qact2, \"레이어에서\", 45.06, 35.89\n",
      "head, \"레이어에서\", 51.45, 43.24\n",
      "act_out, \"레이어에서\", 51.43, 43.23\n"
     ]
    }
   ],
   "source": [
    "#int8div와 int4div의 value를 각각 조회해 array의 차이를 출력한다.\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)  # 두 벡터의 내적\n",
    "    norm_vec1 = np.linalg.norm(vec1)  # 첫 번째 벡터의 크기 (norm)\n",
    "    norm_vec2 = np.linalg.norm(vec2)  # 두 번째 벡터의 크기 (norm)\n",
    "    \n",
    "    return dot_product / (norm_vec1 * norm_vec2) \n",
    "\n",
    "\n",
    "for key in int8_ddv.keys():\n",
    "    if  (key in not_quantized_ddv.keys()):\n",
    "        int8_similarity = cosine_similarity(int8_ddv[key], not_quantized_ddv[key])\n",
    "        int4_similarity = cosine_similarity(int4_ddv[key], not_quantized_ddv[key])\n",
    "        int8_similarity = (int8_similarity + 1) /2 * 100\n",
    "        int4_similarity = (int4_similarity + 1) /2 * 100\n",
    "        print(f'{key}, \"레이어에서\", {int8_similarity:.2f}, {int4_similarity:.2f}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 절대적 변화율 계산\n",
    "        # print(np.abs(diff).mean() * 100)\n",
    "        #difference를 통해 mse를 구한다.\n",
    "        # mse = np.square(difference).mean()\n",
    "        # print(mse)\n",
    "        \n",
    "# print(\"int4_ddv - not_quantized_ddv\")\n",
    "#         print(np.array(int4_ddv[key] - not_quantized_ddv[key]).mean()/2 * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [8]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = int8_model(inputs, bit_config, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int8_model(inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_time = AverageMeter()\n",
    "# losses = AverageMeter()\n",
    "# top1 = AverageMeter()\n",
    "# top5 = AverageMeter()\n",
    "# # not_quantized_attack_net = AttackPGD(not_quantized_model, epsilon=0.02, step_size=0.01, num_steps=50, bit_config=bit_config)\n",
    "\n",
    "#     # switch to evaluate mode\n",
    "    \n",
    "\n",
    "# val_start_time = end = time.time()\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     int8_model.eval()\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     bit_config = [4]*50\n",
    "#     with torch.no_grad():\n",
    "#         clean_output, FLOPs, distance = not_quantized_model(inputs, plot=False)\n",
    "#     # output_shape = clean_output.shape\n",
    "#     # batch_size = output_shape[0]\n",
    "#     # num_classes = output_shape[1]\n",
    "    \n",
    "    \n",
    "#     output_mean = clean_output.mean(axis = 0)\n",
    "#     target_outputs = output_mean - clean_output\n",
    "    \n",
    "#     y = target_outputs * 1000 \n",
    "    \n",
    "#     adv_outputs, adv_inputs = attack_net(inputs, y)\n",
    "#     adv_inputs = adv_inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "#     if i == 0:\n",
    "#         plot_flag = False\n",
    "#     else:\n",
    "#         plot_flag = False\n",
    "#     with torch.no_grad():\n",
    "#         output, FLOPs, distance = int4_model(adv_inputs, bit_config, plot_flag)\n",
    "#     loss = criterion(output, labels)\n",
    "\n",
    "#     # measure accuracy and record loss\n",
    "#     prec1, prec5 = accuracy(output.data, labels, topk=(1, 5))\n",
    "#     losses.update(loss.data.item(), adv_inputs.size(0))\n",
    "#     top1.update(prec1.data.item(), adv_inputs.size(0))\n",
    "#     top5.update(prec5.data.item(), adv_inputs.size(0))\n",
    "\n",
    "#     # measure elapsed time\n",
    "#     batch_time.update(time.time() - end)\n",
    "#     end = time.time()\n",
    "\n",
    "#     if i % 10 == 0:\n",
    "#         print('Test: [{0}/{1}]\\t'\n",
    "#                 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "#                 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "#                 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "#                 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                     i,\n",
    "#                     len(val_loader),\n",
    "#                     batch_time=batch_time,\n",
    "#                     loss=losses,\n",
    "#                     top1=top1,\n",
    "#                     top5=top5,\n",
    "#                 ))\n",
    "# val_end_time = time.time()\n",
    "# print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Time {time:.3f}'.\n",
    "#         format(top1=top1, top5=top5, time=val_end_time - val_start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newerFQ2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
